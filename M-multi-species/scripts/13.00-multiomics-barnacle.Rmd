---
title: "13.00-multiomics-barnacle"
author: "Sam White"
date: "2025-10-03"
output: 
  github_document:
    toc: true
    number_sections: true
  bookdown::html_document2:
    theme: cosmo
    toc: true
    toc_float: true
    number_sections: true
    code_folding: show
    code_download: true
  html_document:
    theme: cosmo
    toc: true
    toc_float: true
    number_sections: true
    code_folding: show
    code_download: true
bibliography: references.bib
citeproc: true
---

# BACKGROUND

This analysis prepares three-species ortholog expression matrices, normalizes the data, constructs a multi‑omics tensor (genes × combined species-samples × timepoints), and runs a sparse tensor decomposition using the [barnacle](https://github.com/blasks/barnacle) [@blaskowski2024] workflow to discover shared gene/time/species patterns.

## Workflow overview

1. **Data Preparation** - Extract and normalize ortholog expression data
2. **Tensor Construction** - Build 3D tensor combining species, samples, and timepoints
3. **Parameter Selection** - Use split-half bootstrap grid search to jointly optimize rank and lambda parameters
4. **Final Decomposition** - Fit model with optimal parameters on full dataset
5. **Interpretation** - Examine factors, visualizations, and biological meaning

## Parameter selection approach

**This analysis uses a split-half bootstrap grid search approach** recommended by the dissertation author [@blaskowski2024] for datasets with limited biological replicates.

### Split-Half Bootstrap Grid Search

**Key features:**
- **Split-half cross-validation** (50/50 train/test split) instead of leave-one-out
- **Bootstrap resampling** with repeated random splits to assess stability
- **Stratified splitting** ensures balanced species representation in train/test sets
- **Joint rank × lambda grid search** tests all parameter combinations simultaneously
- **Incremental checkpointing** saves results after each bootstrap iteration
- **Resume capability** allows continuing from last checkpoint if interrupted
- **Parallel execution** using joblib for multi-core speedup

**Two-stage parameter selection using 1SE rule:**

**Stage 1 - Rank Selection:**
- Evaluate all ranks at λ=0.0 (no regularization) across bootstrap iterations
- Calculate mean cross-validation SSE and standard error for each rank
- Find minimum mean SSE and compute 1SE threshold: min_SSE + 1×SE
- **Select smallest rank with mean SSE ≤ threshold** (parsimony principle)
- Rationale: Avoids overfitting by preferring simpler models when performance is statistically comparable

**Stage 2 - Lambda Selection:**
- At the selected rank, evaluate all lambda values across bootstrap iterations
- Calculate mean Factor Match Score (FMS) and standard error for each lambda
- Find maximum mean FMS and compute 1SE threshold: max_FMS - 1×SE
- **Select maximum lambda with mean FMS ≥ threshold** (maximum sparsity)
- Rationale: Maximizes sparsity while maintaining factor stability and identifiability

**Key metrics:**
- **Cross-validation SSE**: Prediction error on held-out test data (lower = better generalization)
- **Factor Match Score (FMS)**: Stability of factor recovery across random initializations (higher = better identifiability)
- **Variance explained**: Proportion of data variation captured by the model
- **Sparsity**: Number of genes/samples with strong loadings per component
- **1SE rule**: Conservative selection criterion balancing model complexity and performance

## Key steps performed by this script:

- Load annotated ortholog groups and per-species count matrices.
- Filter ortholog groups to retain complete three‑way matches with available expression data.
- Extract per‑species expression matrices mapped to ortholog group IDs.
- Normalize counts using `sctransform`.
- Build a 3D tensor combining species/sample and timepoint dimensions.
- Run [barnacle](https://github.com/blasks/barnacle) [@blaskowski2024] sparse CP decomposition and save factor matrices, metadata and figures.
- Assess rank appropriateness through multiple metrics and visualizations.

## Input files

- `../output/12-ortho-annot/ortholog_groups_annotated.csv` : annotated ortholog groups with columns for `group_id`, `apul`, `peve`, `ptua`, `type`, etc.
- `../../D-Apul/output/02.20-D-Apul-RNAseq-alignment-HiSat2/apul-gene_count_matrix.csv` : Apul gene count matrix (contains `gene_id` plus sample columns).
- `../../E-Peve/output/02.20-E-Peve-RNAseq-alignment-HiSat2/peve-gene_count_matrix.csv` : Peve gene count matrix.
- `../../F-Ptua/output/02.20-F-Ptua-RNAseq-alignment-HiSat2/ptua-gene_count_matrix.csv` : Ptua gene count matrix.
- (Also referenced) transcript-level matrices: `apul-transcript_count_matrix.csv`, `peve-transcript_count_matrix.csv`, `ptua-transcript_count_matrix.csv` when needed.

## Output files

- `apul_ortholog_expression.csv`, `peve_ortholog_expression.csv`, `ptua_ortholog_expression.csv` : per‑species expression matrices aligned to ortholog `group_id`.
- `apul_normalized_expression.csv`, `peve_normalized_expression.csv`, `ptua_normalized_expression.csv` : normalized expression matrices (sctransform or log2(CPM+1)).
- `multiomics_tensor.npy` : saved NumPy array of the 3D tensor used for decomposition (genes × combined_samples × timepoints).
- `barnacle_factors/` directory containing:
    - `gene_factors.csv` : gene loadings per component (genes × components).
    - `sample_factors.csv` : combined sample (species_sample) loadings per component with `Species` and `Sample_ID` metadata.
    - `time_factors.csv` : timepoint loadings per component.
    - `component_weights.csv` : component weights / importance.
    - `sample_mapping.csv` : mapping of combined sample indices to species and sample IDs.
    - `metadata.json` : analysis parameters and tensor metadata (shape, rank, lambdas, convergence, etc.).
    - `figures/` : generated visualizations (component weights, time loadings, sample heatmap, PCA, top ortholog plots).

## Notes / assumptions:

- Sample column names are parsed expecting a dot-separated format with a `TP#` timepoint token (e.g., `ACR.139.TP1`).
- Apul ortholog IDs in the ortholog table include transcript suffixes (e.g., `-T1`) which are removed for matching to the gene count matrix.
- Missing values in the tensor are handled by substitution (current workflow fills NaNs with zeros before decomposition).
- Samples must have all four timepoints (TP1, TP2, TP3, TP4) to be included in the analysis.


# SETUP

## Libraries

```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(reticulate)
library(matrixStats)  # Required for sctransform internal functions
library(sctransform)
library(glmGamPoi)

knitr::opts_chunk$set(
  echo = TRUE,         # Display code chunks
    eval = FALSE,        # Evaluate code chunks
    results = "hold",   # Hold outputs and show them after the full code chunk
  warning = FALSE,     # Hide warnings
        collapse = FALSE,    # Keep code and output in separate blocks
        warning = FALSE,     # Hide warnings
        message = FALSE,     # Hide messages
        comment = "##"      # Prefix output lines with '##' so output is visually distinct
)
```

## Set R variables

```{r R-variables, eval=TRUE}
# OUTPUT DIRECTORY
output_dir <- "../output/13.00-multiomics-barnacle"

#INPUT FILE(S)
ortholog_groups_file <- "../output/12-ortho-annot/ortholog_groups_annotated.csv"

# Transcript count matrices for each species
apul_transcript_matrix_file <- "../../D-Apul/output/02.20-D-Apul-RNAseq-alignment-HiSat2/apul-transcript_count_matrix.csv"
peve_transcript_matrix_file <- "../../E-Peve/output/02.20-E-Peve-RNAseq-alignment-HiSat2/peve-transcript_count_matrix.csv"
ptua_transcript_matrix_file <- "../../F-Ptua/output/02.20-F-Ptua-RNAseq-alignment-HiSat2/ptua-transcript_count_matrix.csv"

# CONDA
conda_env_name <- c("/home/sam/programs/mambaforge/envs/barnacle_py311_env")
conda_path <- c("/opt/anaconda/anaconda3/bin/conda")
```

## Load [barnacle](https://github.com/blasks/barnacle) conda environment

If this is successful, the first line of output should show that the Python being used is the one in your [barnacle](https://github.com/blasks/barnacle) [@blaskowski2024] conda environment path.

E.g.

`python:         /home/sam/programs/mambaforge/envs/barnacle_py311_env/bin/python`

```{r load-barnacle-conda-env, eval=TRUE}
use_condaenv(condaenv = conda_env_name, conda = conda_path)
py_config()
```

# DATA PREP

## Load ortholog groups data

```{r load-ortholog-data, eval=TRUE}
# Read in the ortholog groups data
ortholog_groups <- read.csv(ortholog_groups_file)



# Display basic info about the data
cat("Dimensions of ortholog groups data:", dim(ortholog_groups), "\n\n")
cat("Column names:", colnames(ortholog_groups), "\n\n")
head(ortholog_groups)
str(ortholog_groups)
```

## Extract ortholog expression data

Now let's extract expression data for genes that are present in the ortholog groups. We'll use the gene count matrices with gene IDs to properly map the data.

### Load gene count matrices

```{r load-gene-matrices, eval=TRUE}
# Define file paths for gene count matrices
apul_gene_matrix_file <- "../../D-Apul/output/02.20-D-Apul-RNAseq-alignment-HiSat2/apul-gene_count_matrix.csv"
peve_gene_matrix_file <- "../../E-Peve/output/02.20-E-Peve-RNAseq-alignment-HiSat2/peve-gene_count_matrix.csv"
ptua_gene_matrix_file <- "../../F-Ptua/output/02.20-F-Ptua-RNAseq-alignment-HiSat2/ptua-gene_count_matrix.csv"

# Load gene count matrices for each species
cat("Loading gene count matrices...\n\n")

apul_gene_matrix <- read.csv(apul_gene_matrix_file)
cat("Apul gene matrix dimensions:", dim(apul_gene_matrix), "\n")

peve_gene_matrix <- read.csv(peve_gene_matrix_file)
cat("Peve gene matrix dimensions:", dim(peve_gene_matrix), "\n")

ptua_gene_matrix <- read.csv(ptua_gene_matrix_file)
cat("Ptua gene matrix dimensions:", dim(ptua_gene_matrix), "\n\n")
```

### Filter ortholog groups for complete three-way matches

```{r filter-ortholog-groups, eval=TRUE}
cat("Filtering for complete three-way ortholog groups...\n")

# Keep only rows where all three species have entries (no NA values or empty strings)
complete_ortholog_groups <- ortholog_groups[nzchar(ortholog_groups$apul) & 
                                          nzchar(ortholog_groups$peve) & 
                                          nzchar(ortholog_groups$ptua), ]

cat("Total ortholog groups:", nrow(ortholog_groups), "\n")
cat("Complete three-way ortholog groups:", nrow(complete_ortholog_groups), "\n")

```

### Filter for expression data availability

```{r filter-expression-availability, eval=TRUE}
cat("Filtering ortholog groups to ensure all genes have expression data...\n")

# Clean gene IDs to check against expression data
# For Apul: remove -T[n] suffix from ortholog groups to match gene matrix format
apul_ortholog_genes_check <- gsub("-T[0-9]+$", "", complete_ortholog_groups$apul)

# For Peve and Ptua: use as-is (will clean gene matrix IDs later)
peve_ortholog_genes_check <- complete_ortholog_groups$peve
ptua_ortholog_genes_check <- complete_ortholog_groups$ptua

# Check which genes are present in expression data
# (Note: We need to clean gene matrix IDs to match)
apul_gene_matrix_ids <- gsub("^gene-", "", apul_gene_matrix$gene_id)  # Remove gene- prefix if present
peve_gene_matrix_ids <- gsub("^gene-", "", peve_gene_matrix$gene_id)  # Remove gene- prefix
ptua_gene_matrix_ids <- gsub("^gene-", "", ptua_gene_matrix$gene_id)  # Remove gene- prefix

# Find which ortholog genes are present in each species' expression data
apul_present <- apul_ortholog_genes_check %in% apul_gene_matrix_ids
peve_present <- peve_ortholog_genes_check %in% peve_gene_matrix_ids
ptua_present <- ptua_ortholog_genes_check %in% ptua_gene_matrix_ids

# Keep only ortholog groups where all three species have expression data
expression_complete_mask <- apul_present & peve_present & ptua_present
complete_ortholog_groups <- complete_ortholog_groups[expression_complete_mask, ]

cat("Ortholog groups after filtering for expression data availability:", nrow(complete_ortholog_groups), "\n")

```

### Gene ID cleaning examples

```{r gene-id-examples-testing, eval=TRUE}
cat("\n=== GENE ID CLEANING EXAMPLES ===\n")
cat("Apul (clean ortholog groups to match gene matrix):\n")
cat("Ortholog groups original:", head(complete_ortholog_groups$apul, 3), "\n")
cat("Ortholog groups cleaned:", head(gsub("-T[0-9]+$", "", complete_ortholog_groups$apul), 3), "\n")
cat("Gene matrix (target format):", head(apul_gene_matrix$gene_id, 3), "\n\n")

cat("Peve (clean gene matrix to match ortholog groups):\n")
cat("Ortholog groups (target format):", head(complete_ortholog_groups$peve, 3), "\n") 
cat("Gene matrix original:", head(peve_gene_matrix$gene_id, 3), "\n")
cat("Gene matrix cleaned:", head(peve_gene_matrix$gene_id_clean, 3), "\n\n")

cat("Ptua (clean gene matrix to match ortholog groups):\n")
cat("Ortholog groups (target format):", head(complete_ortholog_groups$ptua, 3), "\n")
cat("Gene matrix original:", head(ptua_gene_matrix$gene_id, 3), "\n")
cat("Gene matrix cleaned:", head(ptua_gene_matrix$gene_id_clean, 3), "\n\n")

```

### Clean gene IDs for matching

```{r clean-gene-ids, eval=TRUE}
cat("Cleaning gene matrix IDs to match ortholog group format...\n")

# For Apul: ortholog groups have "FUN_000185-T1", gene matrix has "FUN_002326"
# We need to remove "-T1" from ortholog groups to match gene matrix
apul_ortholog_genes <- unique(gsub("-T[0-9]+$", "", complete_ortholog_groups$apul))

# For Peve and Ptua: keep ortholog groups as-is and clean gene matrix
peve_ortholog_genes <- unique(complete_ortholog_groups$peve)
ptua_ortholog_genes <- unique(complete_ortholog_groups$ptua)

# Clean gene matrix IDs accordingly
# Apul: gene matrix already in correct format (no cleaning needed)
apul_gene_matrix$gene_id_clean <- apul_gene_matrix$gene_id

# Peve: gene matrix has "gene-Peve_00000032", ortholog groups have "Peve_00037402"  
# So we need to remove "gene-" prefix from gene matrix
peve_gene_matrix$gene_id_clean <- gsub("^gene-", "", peve_gene_matrix$gene_id)

# Ptua: gene matrix has "gene-Pocillopora_meandrina_HIv1___RNAseq.g20905.t1", 
# ortholog groups have "Pocillopora_meandrina_HIv1___RNAseq.g28886.t1"
# So we just need to remove "gene-" prefix from gene matrix
ptua_gene_matrix$gene_id_clean <- gsub("^gene-", "", ptua_gene_matrix$gene_id)

cat("Apul ortholog genes (complete groups only):", length(apul_ortholog_genes), "\n")
cat("Peve ortholog genes (complete groups only):", length(peve_ortholog_genes), "\n")
cat("Ptua ortholog genes (complete groups only):", length(ptua_ortholog_genes), "\n\n")
```

### Define sample filtering function

```{r define-sample-filter-function, eval=TRUE}
# Function to filter samples that have all four timepoints (TP1, TP2, TP3, TP4)
filter_complete_samples <- function(gene_matrix, species_name) {
  cat("  Filtering samples with complete timepoints for", species_name, "...\n")
  
  # Get expression columns (exclude gene_id column, gene_id_clean may not exist yet)
  all_cols <- colnames(gene_matrix)
  id_cols <- c("gene_id", "gene_id_clean")
  expr_cols <- setdiff(all_cols, id_cols)
  
  if(length(expr_cols) == 0) {
    cat("  ERROR: No expression columns found! Returning original matrix.\n")
    return(gene_matrix)
  }
  
  # Parse sample column names to identify sample groups and timepoints
  sample_timepoint_map <- list()
  
  for(col in expr_cols) {
    # Expected formats: ACR.139.TP1, POR.216.TP1, POC.201.TP1, etc.
    # Note: Using dots (.) as separators, not hyphens (-)
    parts <- strsplit(col, "\\.")[[1]]  # Split on dots, not hyphens
    
    if(length(parts) >= 3) {
      # Extract timepoint from last part (e.g., "TP1", "TP2", etc.)
      tp_part <- parts[length(parts)]
      
      if(grepl("^TP[0-9]+$", tp_part)) {
        timepoint <- as.numeric(gsub("TP", "", tp_part))
        
        # Sample ID is everything except the timepoint part, joined with dots
        sample_id <- paste(parts[1:(length(parts)-1)], collapse = ".")
        
        if(!sample_id %in% names(sample_timepoint_map)) {
          sample_timepoint_map[[sample_id]] <- list()
        }
        sample_timepoint_map[[sample_id]][[as.character(timepoint)]] <- col
      }
    }
  }
  
  # Identify samples with all four timepoints (1, 2, 3, 4)
  complete_samples <- c()
  incomplete_samples <- c()
  required_timepoints <- c("1", "2", "3", "4")
  
  for(sample_id in names(sample_timepoint_map)) {
    available_timepoints <- names(sample_timepoint_map[[sample_id]])
    
    if(all(required_timepoints %in% available_timepoints)) {
      complete_samples <- c(complete_samples, sample_id)
    } else {
      incomplete_samples <- c(incomplete_samples, sample_id)
      missing_tps <- setdiff(required_timepoints, available_timepoints)
      cat("    Sample", sample_id, "missing timepoints:", paste(paste0("TP", missing_tps), collapse = ", "), "\n")
    }
  }
  
  cat("  Complete samples (all 4 timepoints):", length(complete_samples), "\n")
  cat("  Incomplete samples:", length(incomplete_samples), "\n")
  
  if(length(incomplete_samples) > 0) {
    cat("  Removing incomplete samples:", paste(incomplete_samples, collapse = ", "), "\n")
  }
  
  # Build list of columns to keep (include gene_id columns plus ONLY complete sample columns)
  if(length(complete_samples) == 0) {
    cat("  WARNING: No complete samples found! Keeping all expression columns.\n")
    # Fallback: keep all expression columns
    keep_cols <- c("gene_id")
    if("gene_id_clean" %in% colnames(gene_matrix)) {
      keep_cols <- c(keep_cols, "gene_id_clean")
    }
    keep_cols <- c(keep_cols, expr_cols)
  } else {
    keep_cols <- c("gene_id")
    if("gene_id_clean" %in% colnames(gene_matrix)) {
      keep_cols <- c(keep_cols, "gene_id_clean")
    }
    
    # ONLY add columns from complete samples (this excludes incomplete sample columns)
    for(sample_id in complete_samples) {
      for(tp in required_timepoints) {
        if(tp %in% names(sample_timepoint_map[[sample_id]])) {
          keep_cols <- c(keep_cols, sample_timepoint_map[[sample_id]][[tp]])
        }
      }
    }
    
    # Double-check: ensure we're not accidentally including incomplete sample columns
    incomplete_cols <- c()
    for(sample_id in incomplete_samples) {
      for(tp in names(sample_timepoint_map[[sample_id]])) {
        incomplete_cols <- c(incomplete_cols, sample_timepoint_map[[sample_id]][[tp]])
      }
    }
    
    if(length(incomplete_cols) > 0) {
      # Remove any incomplete sample columns that might have been included
      keep_cols <- setdiff(keep_cols, incomplete_cols)
    }
  }
  
  # Filter the gene matrix to keep only complete samples
  keep_cols <- intersect(keep_cols, colnames(gene_matrix))  # Ensure columns exist
  filtered_matrix <- gene_matrix[, keep_cols, drop = FALSE]
  
  # Count ID vs expression columns in final result
  final_expr_cols <- setdiff(colnames(filtered_matrix), c("gene_id", "gene_id_clean"))
  cat("  Final expression columns:", length(final_expr_cols), "\n")
  
  return(filtered_matrix)
}
```

### Define expression extraction function

```{r define-expression-extraction-function, eval=TRUE}
# Function to extract expression data for a species using ortholog group mapping
extract_species_expression <- function(ortholog_groups_df, gene_matrix, species_col, species_name) {
  cat("Processing", species_name, "...\n")
  
  # First, filter the gene matrix to keep only samples with complete timepoints
  filtered_gene_matrix <- filter_complete_samples(gene_matrix, species_name)
  
  # Remove duplicate group_ids, keeping first occurrence of each
  unique_groups <- ortholog_groups_df[!duplicated(ortholog_groups_df$group_id), ]
  cat("  Unique ortholog groups:", nrow(unique_groups), "\n")
  
  # Create results data frame starting with group_id
  result_df <- data.frame(group_id = unique_groups$group_id, stringsAsFactors = FALSE)
  
  # Get expression columns (exclude gene_id and gene_id_clean columns)
  expr_cols <- setdiff(colnames(filtered_gene_matrix), c("gene_id", "gene_id_clean"))
  
  # Initialize expression columns with NA
  for(col in expr_cols) {
    result_df[[col]] <- NA
  }
  
  # For each ortholog group, find the corresponding gene and extract expression
  for(i in seq_len(nrow(unique_groups))) {
    target_gene <- unique_groups[[species_col]][i]
    
    # Clean target gene for matching
    if(species_name == "Apul") {
      # Remove transcript suffix for Apul
      target_gene_clean <- gsub("-T[0-9]+$", "", target_gene)
      matching_rows <- which(filtered_gene_matrix$gene_id_clean == target_gene_clean)
    } else {
      # For Peve and Ptua, match cleaned gene_id
      matching_rows <- which(filtered_gene_matrix$gene_id_clean == target_gene)
    }
    
    if(length(matching_rows) == 1) {
      # Single match - copy expression data
      for(col in expr_cols) {
        result_df[i, col] <- filtered_gene_matrix[matching_rows, col]
      }
    } else if(length(matching_rows) > 1) {
      # Multiple matches - take first gene (no averaging)
      first_match <- matching_rows[1]
      for(col in expr_cols) {
        result_df[i, col] <- filtered_gene_matrix[first_match, col]
      }
    } else {
      # No match - leave as NA (will be removed later)
    }
  }
  
  # Remove rows with all NA expression values
  expr_na_mask <- apply(result_df[, expr_cols, drop = FALSE], 1, function(x) all(is.na(x)))
  result_df <- result_df[!expr_na_mask, ]
  
  cat("  Final dimensions:", nrow(result_df), "ortholog groups x", ncol(result_df)-1, "samples\n")
  cat("  Removed", sum(expr_na_mask), "groups with no expression data\n\n")
  
  return(result_df)
}
```

### Extract ortholog expression data

```{r extract-ortholog-expression-data, eval=TRUE}
cat("Creating ortholog expression data with proper group_id mapping...\n")

# Extract expression data for each species using the ortholog group mapping
apul_ortholog_expression <- extract_species_expression(complete_ortholog_groups, apul_gene_matrix, "apul", "Apul")
peve_ortholog_expression <- extract_species_expression(complete_ortholog_groups, peve_gene_matrix, "peve", "Peve")
ptua_ortholog_expression <- extract_species_expression(complete_ortholog_groups, ptua_gene_matrix, "ptua", "Ptua")

cat("=== FINAL ORTHOLOG EXPRESSION DATA DIMENSIONS ===\n")
cat("Apul:", nrow(apul_ortholog_expression), "ortholog groups x", ncol(apul_ortholog_expression)-1, "samples\n")
cat("Peve:", nrow(peve_ortholog_expression), "ortholog groups x", ncol(peve_ortholog_expression)-1, "samples\n")
cat("Ptua:", nrow(ptua_ortholog_expression), "ortholog groups x", ncol(ptua_ortholog_expression)-1, "samples\n\n")
```

### Write ortholog expression data

```{r write-expression-data, eval=TRUE}
cat("Exporting ortholog expression data to CSV files...\n")

# Define output file paths
apul_output_file <- file.path(output_dir, "apul_ortholog_expression.csv")
peve_output_file <- file.path(output_dir, "peve_ortholog_expression.csv")
ptua_output_file <- file.path(output_dir, "ptua_ortholog_expression.csv")

# Write CSV files without quotes
write.csv(apul_ortholog_expression, file = apul_output_file, quote = FALSE, row.names = FALSE)
cat("Exported Apul ortholog expression data to:", apul_output_file, "\n")

write.csv(peve_ortholog_expression, file = peve_output_file, quote = FALSE, row.names = FALSE)
cat("Exported Peve ortholog expression data to:", peve_output_file, "\n")

write.csv(ptua_ortholog_expression, file = ptua_output_file, quote = FALSE, row.names = FALSE)
cat("Exported Ptua ortholog expression data to:", ptua_output_file, "\n")

cat("\nAll ortholog expression data exported successfully!\n\n")
```

### Column structure analysis

```{r column-structure-analysis, eval=TRUE}
cat("=== COLUMN STRUCTURE ANALYSIS ===\n")
cat("Apul columns:", ncol(apul_ortholog_expression), "\n")
cat("Apul column names (first 10):", paste(head(colnames(apul_ortholog_expression), 10), collapse = ", "), "\n")
cat("Apul column names (last 10):", paste(tail(colnames(apul_ortholog_expression), 10), collapse = ", "), "\n\n")

cat("Peve columns:", ncol(peve_ortholog_expression), "\n")
cat("Peve column names (first 10):", paste(head(colnames(peve_ortholog_expression), 10), collapse = ", "), "\n")
cat("Peve column names (last 10):", paste(tail(colnames(peve_ortholog_expression), 10), collapse = ", "), "\n\n")

cat("Ptua columns:", ncol(ptua_ortholog_expression), "\n")
cat("Ptua column names (first 10):", paste(head(colnames(ptua_ortholog_expression), 10), collapse = ", "), "\n")
cat("Ptua column names (last 10):", paste(tail(colnames(ptua_ortholog_expression), 10), collapse = ", "), "\n\n")

```

### Summary statistics

```{r summary-statistics, eval=TRUE}
cat("=== LINE COUNTS FOR ORTHOLOG EXPRESSION DATA ===\n")
cat("Apul ortholog expression with info: ", nrow(apul_ortholog_expression), " rows\n")
cat("Peve ortholog expression with info: ", nrow(peve_ortholog_expression), " rows\n")
cat("Ptua ortholog expression with info: ", nrow(ptua_ortholog_expression), " rows\n\n")
```

### Sample filtering verification

```{r sample-filtering-verification, eval=TRUE}
cat("=== SAMPLE FILTERING VERIFICATION ===\n")
cat("This analysis filters samples to include ONLY those with all four timepoints (TP1, TP2, TP3, TP4)\n\n")

# Function to analyze sample completeness in the filtered data
analyze_sample_completeness <- function(data_df, species_name) {
  cat("Analyzing", species_name, "sample completeness:\n")
  
  # Get expression columns (exclude group_id)
  expr_cols <- setdiff(colnames(data_df), "group_id")
  
  # Parse sample column names to identify sample groups and timepoints
  sample_timepoint_map <- list()
  
  for(col in expr_cols) {
    parts <- strsplit(col, "\\.")[[1]]  # Split on dots to match actual format
    
    if(length(parts) >= 3) {
      tp_part <- parts[length(parts)]
      
      if(grepl("^TP[0-9]+$", tp_part)) {
        timepoint <- as.numeric(gsub("TP", "", tp_part))
        sample_id <- paste(parts[1:(length(parts)-1)], collapse = ".")
        
        if(!sample_id %in% names(sample_timepoint_map)) {
          sample_timepoint_map[[sample_id]] <- c()
        }
        sample_timepoint_map[[sample_id]] <- c(sample_timepoint_map[[sample_id]], timepoint)
      }
    }
  }
  
  # Check completeness
  complete_samples <- 0
  incomplete_samples <- 0
  required_timepoints <- c(1, 2, 3, 4)
  
  for(sample_id in names(sample_timepoint_map)) {
    available_timepoints <- sort(sample_timepoint_map[[sample_id]])
    
    if(all(required_timepoints %in% available_timepoints)) {
      complete_samples <- complete_samples + 1
    } else {
      incomplete_samples <- incomplete_samples + 1
      missing_tps <- setdiff(required_timepoints, available_timepoints)
      cat("  WARNING: Sample", sample_id, "missing timepoints:", paste(paste0("TP", missing_tps), collapse = ", "), "\n")
    }
  }
  
  cat("  Total samples:", length(sample_timepoint_map), "\n")
  cat("  Complete samples (all 4 timepoints):", complete_samples, "\n")
  cat("  Incomplete samples:", incomplete_samples, "\n")
  
  if(incomplete_samples > 0) {
    cat("  ERROR: Filtering did not work correctly!\n")
  } else {
    cat("  SUCCESS: All samples have complete timepoints\n")
  }
  
  cat("\n")
  
  return(list(
    total = length(sample_timepoint_map),
    complete = complete_samples,
    incomplete = incomplete_samples
  ))
}

# Verify each species
apul_stats <- analyze_sample_completeness(apul_ortholog_expression, "Apul")
peve_stats <- analyze_sample_completeness(peve_ortholog_expression, "Peve")
ptua_stats <- analyze_sample_completeness(ptua_ortholog_expression, "Ptua")

cat("=== FILTERING SUMMARY ===\n")
cat("All samples in the filtered dataset should have exactly 4 timepoints (TP1, TP2, TP3, TP4)\n")
cat("Apul: ", apul_stats$complete, "/", apul_stats$total, " complete samples\n")
cat("Peve: ", peve_stats$complete, "/", peve_stats$total, " complete samples\n")
cat("Ptua: ", ptua_stats$complete, "/", ptua_stats$total, " complete samples\n")

total_incomplete <- apul_stats$incomplete + peve_stats$incomplete + ptua_stats$incomplete
if(total_incomplete == 0) {
  cat("SUCCESS: All samples across all species have complete timepoints!\n")
} else {
  cat("ERROR: Found", total_incomplete, "incomplete samples - filtering needs to be fixed!\n")
}
cat("\n")
```

### Verify three-way ortholog filtering and gene coverage

This section verifies that:
1. All ortholog groups are complete three-way orthologs
2. Gene counts are identical across all three species
3. Expression data contains 100% of the expected ortholog genes

```{r verify-ortho-filtering, eval=TRUE}
cat("\n=== VERIFICATION: THREE-WAY ORTHOLOG COUNTS ===\n")
cat("After filtering, all species should have identical three_way ortholog counts.\n\n")

# Check how many genes we have for each species
cat("Genes found in expression data by species:\n")
cat("Apul ortholog expression (before adding info):", nrow(apul_ortholog_expression), "\n")
cat("Peve ortholog expression (before adding info):", nrow(peve_ortholog_expression), "\n")
cat("Ptua ortholog expression (before adding info):", nrow(ptua_ortholog_expression), "\n\n")

# Verify all ortholog groups are three-way
cat("Complete three-way ortholog groups available:", nrow(complete_ortholog_groups), "\n")
cat("All should be type 'three_way'? Check:", table(complete_ortholog_groups$type), "\n\n")

# Verify perfect gene coverage (should be 100% for all species now)
apul_coverage <- length(intersect(apul_ortholog_genes, apul_gene_matrix$gene_id_clean))
peve_coverage <- length(intersect(peve_ortholog_genes, peve_gene_matrix$gene_id_clean))
ptua_coverage <- length(intersect(ptua_ortholog_genes, ptua_gene_matrix$gene_id_clean))

cat("Gene coverage in expression data (should be 100% for all):\n")
cat("Apul: ", apul_coverage, "/", length(apul_ortholog_genes), " (", round(apul_coverage/length(apul_ortholog_genes)*100, 1), "%)\n")
cat("Peve: ", peve_coverage, "/", length(peve_ortholog_genes), " (", round(peve_coverage/length(peve_ortholog_genes)*100, 1), "%)\n")
cat("Ptua: ", ptua_coverage, "/", length(ptua_ortholog_genes), " (", round(ptua_coverage/length(ptua_ortholog_genes)*100, 1), "%)\n\n")
```

### Preview expression data

```{r preview-expression-data, eval=TRUE}
cat("\n=== PREVIEW OF ORTHOLOG EXPRESSION DATA ===\n")

cat("Apul ortholog expression:\n\n")
str(apul_ortholog_expression)
cat("\n\n")

cat("\nPeve ortholog expression:\n")
str(peve_ortholog_expression)
cat("\n\n")

cat("\nPtua ortholog expression:\n")
str(ptua_ortholog_expression)
cat("\n\n")
```

# BARNACLE ANALYSIS

This workflow focuses on comparing multiple ranks to find the optimal number of components.

Based on the barnacle workflow, we:

1. Normalize count data with `sctransform`

2. Create tensors for multiomics analysis

3. Compare multiple ranks systematically

4. Choose optimal rank based on metrics

## Load expression data

```{r load-ortholog-expression-data, eval=TRUE}
# Read the exported ortholog expression data
apul_expr <- read.csv(file.path(output_dir, "apul_ortholog_expression.csv"))
peve_expr <- read.csv(file.path(output_dir, "peve_ortholog_expression.csv"))  
ptua_expr <- read.csv(file.path(output_dir, "ptua_ortholog_expression.csv"))

cat("Loaded expression data:\n")
cat("Apul:", nrow(apul_expr), "genes x", ncol(apul_expr)-1, "samples\n")
cat("Peve:", nrow(peve_expr), "genes x", ncol(peve_expr)-1, "samples\n") 
cat("Ptua:", nrow(ptua_expr), "genes x", ncol(ptua_expr)-1, "samples\n")
```

## Normalize data with sctransform

Following the barnacle manuscript approach, we'll use `sctransform` to normalize each species' data. We'll use a bulk RNA-seq appropriate approach.

```{r normalize-sctransform, eval=TRUE}
# Function to normalize count data with sctransform for bulk RNA-seq
normalize_with_sctransform <- function(count_data, species_name) {
  cat("Normalizing", species_name, "data with sctransform...\n")
  
  # Check if we have group_id or gene_id column
  id_col <- if("group_id" %in% colnames(count_data)) "group_id" else "gene_id"
  cat("Using", id_col, "as identifier column\n")
  
  # Check for and handle duplicate group_ids/gene_ids
  duplicate_ids <- count_data[[id_col]][duplicated(count_data[[id_col]])]
  if(length(duplicate_ids) > 0) {
    cat("Found", length(unique(duplicate_ids)), "duplicate", id_col, "values:\n")
    cat("  Examples:", head(unique(duplicate_ids), 10), "\n")
    cat("  Note: sctransform may fail due to duplicate row names, will fall back to log2(CPM + 1)\n")
  } else {
    cat("No duplicate", id_col, "values found\n")
  }
  
  # Use original data without aggregation - let sctransform handle duplicates or fail
  agg_data <- count_data
  
  # Extract count matrix (genes as rows, samples as columns)
  count_matrix <- as.matrix(agg_data[, -1])  # Remove id column
  rownames(count_matrix) <- agg_data[[id_col]]
  
  # Check for and handle problematic values
  cat("Checking data quality...\n")
  cat("  - Zero values:", sum(count_matrix == 0), "/", length(count_matrix), "\n")
  cat("  - NA values:", sum(is.na(count_matrix)), "\n")
  cat("  - Infinite values:", sum(is.infinite(count_matrix)), "\n")
  cat("  - Min value:", min(count_matrix, na.rm = TRUE), "\n")
  cat("  - Max value:", max(count_matrix, na.rm = TRUE), "\n")
  cat("  - Total ortholog groups:", nrow(count_matrix), "\n")
  
  # Ensure data is numeric and handle any potential issues (NA/infinite values)
  # Note: We do NOT filter low-count genes here - sctransform handles them appropriately
  # Filtering would remove species-specific expression patterns that are biologically interesting
  count_matrix_cleaned <- apply(count_matrix, c(1,2), function(x) {
    if(is.na(x) || !is.finite(x)) return(0)
    return(as.numeric(x))
  })
  
  # Ensure row and column names are character strings (not factors)
  rownames(count_matrix_cleaned) <- as.character(rownames(count_matrix))
  colnames(count_matrix_cleaned) <- as.character(colnames(count_matrix))
  
  # sctransform expects genes as rows and cells (samples) as columns —
  # our matrix is already genes x samples (rows=genes, cols=samples), so do NOT transpose.
  count_matrix_for_vst <- count_matrix_cleaned
  
  # Apply sctransform normalization with bulk RNA-seq appropriate parameters
  normalized_df <- tryCatch({
    # First try with minimal parameters to isolate the issue
    cat("  Attempting sctransform with minimal parameters...\n")
    # Pass genes x samples matrix directly
    normalized <- sctransform::vst(
      count_matrix_for_vst,
      verbosity = 1
    )

    # Extract normalized data (should be genes x samples)
    normalized_data <- normalized$y
    
    # Create output data frame
    result_df <- data.frame(
      group_id = rownames(normalized_data),
      normalized_data,
      stringsAsFactors = FALSE
    )
    
    cat("sctransform normalization successful for", species_name, "\n")
    return(result_df)
    
  }, error = function(e) {
    cat("sctransform with minimal parameters failed for", species_name, ":", e$message, "\n")
    cat("Trying sctransform with glmGamPoi method...\n")
    
    # Try again with glmGamPoi method
    tryCatch({
      normalized <- sctransform::vst(
        count_matrix_for_vst, 
        method = "glmGamPoi",
        n_genes = min(2000, nrow(count_matrix_for_vst)),
        return_cell_attr = TRUE,
        verbosity = 1
      )

      # Extract normalized data (genes x samples)
      normalized_data <- normalized$y
      
      # Create output data frame with original gene set (fill missing with zeros)
      full_normalized_data <- matrix(0, nrow = nrow(count_matrix), ncol = ncol(count_matrix))
      rownames(full_normalized_data) <- rownames(count_matrix)
      colnames(full_normalized_data) <- colnames(count_matrix)
      
      # Fill in normalized values for kept genes
      full_normalized_data[rownames(normalized_data), ] <- normalized_data
      
      result_df <- data.frame(
        group_id = rownames(full_normalized_data),
        full_normalized_data,
        stringsAsFactors = FALSE
      )
      
      cat("sctransform normalization with glmGamPoi successful for", species_name, "\n")
      return(result_df)
      
    }, error = function(e2) {
      stop("sctransform failed for ", species_name, ": ", e2$message)
    })
  })
  
  cat("Input dimensions:", nrow(count_data), "rows x", ncol(count_data)-1, "samples\n")
  cat("Output dimensions:", nrow(normalized_df), "ortholog groups x", ncol(normalized_df)-1, "samples\n\n")
  
  return(normalized_df)
}

# Normalize each species
cat("=== STARTING NORMALIZATION ===\n\n")
apul_normalized <- normalize_with_sctransform(apul_expr, "Apul")
peve_normalized <- normalize_with_sctransform(peve_expr, "Peve") 
ptua_normalized <- normalize_with_sctransform(ptua_expr, "Ptua")
cat("=== NORMALIZATION COMPLETE ===\n\n")
```

## Export normalized data for Python analysis

```{r export-normalized-data, eval=TRUE}
# Export normalized data for Python processing
apul_norm_file <- file.path(output_dir, "apul_normalized_expression.csv")
peve_norm_file <- file.path(output_dir, "peve_normalized_expression.csv")
ptua_norm_file <- file.path(output_dir, "ptua_normalized_expression.csv")

write.csv(apul_normalized, apul_norm_file, row.names = FALSE, quote = FALSE)
write.csv(peve_normalized, peve_norm_file, row.names = FALSE, quote = FALSE)
write.csv(ptua_normalized, ptua_norm_file, row.names = FALSE, quote = FALSE)

cat("Exported normalized data:\n")
cat("Apul:", apul_norm_file, "\n")
cat("Peve:", peve_norm_file, "\n")
cat("Ptua:", ptua_norm_file, "\n\n")
```

## Create tensor dataset in Python

Now we'll switch to Python to create the multiomics tensor and run barnacle analysis.

### Setup logging
```{python setup-logging, eval=TRUE}
import sys
import os
from datetime import datetime

# Set up logging to both console and file
class Logger:
    def __init__(self, log_file):
        self.terminal = sys.stdout
        self.log = open(log_file, 'w')
        
    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
        self.log.flush()
        
    def flush(self):
        self.terminal.flush()
        self.log.flush()

# Create log file path
output_dir = r.output_dir
log_file = os.path.join(output_dir, f'barnacle_analysis_log_{datetime.now().strftime("%Y%m%d_%H%M%S")}.txt')

# Redirect stdout to both console and log file
sys.stdout = Logger(log_file)
sys.stderr = sys.stdout  # Also capture error messages

print("="*60)
print(f"BARNACLE ANALYSIS LOG")
print(f"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"Log file: {log_file}")
print("="*60)
print()
```


### Create multiomics tensor
```{python create-multiomics-tensor, eval=TRUE}
import pandas as pd
import numpy as np
import os
from pathlib import Path

# Set up paths
output_dir = r.output_dir
print(f"Working in output directory: {output_dir}")

# Load normalized data
apul_norm = pd.read_csv(os.path.join(output_dir, "apul_normalized_expression.csv"))
peve_norm = pd.read_csv(os.path.join(output_dir, "peve_normalized_expression.csv"))
ptua_norm = pd.read_csv(os.path.join(output_dir, "ptua_normalized_expression.csv"))

print("Loaded normalized data:")
print(f"Apul: {apul_norm.shape}")
print(f"Peve: {peve_norm.shape}")  
print(f"Ptua: {ptua_norm.shape}")

# Check which genes are common across all species
apul_genes = set(apul_norm['group_id'])
peve_genes = set(peve_norm['group_id'])
ptua_genes = set(ptua_norm['group_id'])

common_genes = apul_genes & peve_genes & ptua_genes
print(f"\nCommon genes across all species: {len(common_genes)}")

# Filter to common genes and align gene order
common_genes_list = sorted(list(common_genes))

apul_common = apul_norm[apul_norm['group_id'].isin(common_genes_list)].set_index('group_id').reindex(common_genes_list)
peve_common = peve_norm[peve_norm['group_id'].isin(common_genes_list)].set_index('group_id').reindex(common_genes_list)
ptua_common = ptua_norm[ptua_norm['group_id'].isin(common_genes_list)].set_index('group_id').reindex(common_genes_list)

print(f"\nFiltered to common genes:")
print(f"Apul: {apul_common.shape}")
print(f"Peve: {peve_common.shape}")
print(f"Ptua: {ptua_common.shape}")
```

## Parse sample information

```{python parse-sample-info, eval=TRUE}
# Parse sample names to extract sample information for each species independently
def parse_species_samples(columns, species_name):
    """Parse sample column names for a specific species"""
    sample_map = {}
    sample_ids = []
    timepoints = set()
    
    for col in columns:
        # Expected format: PREFIX-NUMBER-TP# (e.g., ACR-139-TP1, POR-216-TP1, POC-201-TP1)
        # Also support legacy format: PREFIX.NUMBER.TP# for backwards compatibility
        parts = col.split('-') if '-' in col else col.split('.')
        
        if len(parts) >= 3 and parts[-1].startswith('TP'):
            # Extract timepoint from last part (e.g., "TP1", "TP2", etc.)
            tp_part = parts[-1]
            timepoint = int(tp_part[2:])  # e.g., 1 from TP1
            
            # Sample ID is everything except the timepoint part
            sample_id_parts = parts[:-1]
            sample_id = '-'.join(sample_id_parts) if '-' in col else '.'.join(sample_id_parts)
            
            sample_map[(sample_id, timepoint)] = col
            if sample_id not in sample_ids:
                sample_ids.append(sample_id)
            timepoints.add(timepoint)
        else:
            print(f"Warning: Could not parse column name: {col}")
    
    return sample_map, sample_ids, sorted(timepoints)

# Parse sample information for each species independently
print("Parsing sample information for each species...")

species_data = {
    'apul': apul_common,
    'peve': peve_common, 
    'ptua': ptua_common
}

species_info = {}
all_timepoints = set()

for species, data in species_data.items():
    sample_map, sample_ids, timepoints = parse_species_samples(data.columns, species)
    species_info[species] = {
        'sample_map': sample_map,
        'sample_ids': sample_ids,
        'timepoints': timepoints,
        'n_samples': len(sample_ids)
    }
    all_timepoints.update(timepoints)
    
    print(f"{species}:")
    print(f"  Samples: {len(sample_ids)} ({sample_ids[:3]}...)")
    print(f"  Timepoints: {timepoints}")

common_timepoints = sorted(list(all_timepoints))
print(f"\nTimepoints found across all species: {common_timepoints}")

# Find the maximum number of samples to determine tensor dimensions
max_samples = max(info['n_samples'] for info in species_info.values())
print(f"Maximum samples in any species: {max_samples}")

# Print detailed sample structure
print(f"\nDetailed sample structure:")
for species, info in species_info.items():
    print(f"{species}: {info['n_samples']} samples × {len(info['timepoints'])} timepoints")
```

## Create 3D tensor (genes × species_samples × timepoints)

```{python create-3d-tensor, eval=TRUE}
# Create a 3D tensor: genes × samples × timepoints
# Samples are identified by their names which include species information (e.g., apul-A1, peve-P1)

print("Creating 3D tensor with samples as the second dimension...")
print("Note: R-level filtering has already removed samples without all 4 timepoints")

# First, collect all actual sample-timepoint combinations that have data
all_sample_columns = []
sample_labels = []  # Track which sample belongs to which species
species_sample_map = {}  # Map from combined index to (species, sample_idx, sample_id)

sample_idx = 0
for species in ['apul', 'peve', 'ptua']:
    data = species_data[species]
    info = species_info[species]
    
    print(f"\nProcessing {species}:")
    for sample_id in info['sample_ids']:
        # Check if this sample has data for ALL required timepoints
        available_timepoints = []
        sample_timepoint_cols = []
        
        for timepoint in common_timepoints:
            if (sample_id, timepoint) in info['sample_map']:
                col_name = info['sample_map'][(sample_id, timepoint)]
                sample_timepoint_cols.append(col_name)
                available_timepoints.append(timepoint)
        
        # Only include samples that have ALL timepoints (should be all samples due to R filtering)
        if len(available_timepoints) == len(common_timepoints):
            all_sample_columns.extend(sample_timepoint_cols)
            sample_labels.append(f"{species}_{sample_id}")
            species_sample_map[sample_idx] = {
                'species': species,
                'sample_id': sample_id,
                'sample_idx_in_species': info['sample_ids'].index(sample_id)
            }
            sample_idx += 1
            print(f"  Added {sample_id} with {len(sample_timepoint_cols)} timepoints: {sorted(available_timepoints)}")
        else:
            print(f"  Skipped {sample_id} - incomplete timepoints: {sorted(available_timepoints)} (expected: {sorted(common_timepoints)})")

n_genes = len(common_genes_list)
n_combined_samples = len(sample_labels)
n_timepoints = len(common_timepoints)

print(f"\nCreating 3D tensor with shape: ({n_genes}, {n_combined_samples}, {n_timepoints})")
print(f"Combined samples from all species: {n_combined_samples}")

# Initialize tensor
tensor_3d = np.full((n_genes, n_combined_samples, n_timepoints), np.nan)

# Fill tensor
filled_count = 0
missing_count = 0

for combined_idx, sample_label in enumerate(sample_labels):
    species_info_map = species_sample_map[combined_idx]
    species = species_info_map['species']
    sample_id = species_info_map['sample_id']
    
    data = species_data[species]
    info = species_info[species]
    
    for time_idx, timepoint in enumerate(common_timepoints):
        if (sample_id, timepoint) in info['sample_map']:
            col_name = info['sample_map'][(sample_id, timepoint)]
            tensor_3d[:, combined_idx, time_idx] = data[col_name].values
            filled_count += 1
        else:
            missing_count += 1

# Check tensor statistics
n_missing = np.sum(np.isnan(tensor_3d))
n_total = tensor_3d.size
n_finite = np.sum(np.isfinite(tensor_3d))

print(f"\n=== TENSOR STATISTICS ===")
print(f"Tensor shape: {tensor_3d.shape}")
print(f"Total elements: {n_total}")
print(f"Finite values: {n_finite}")
print(f"Missing/NaN values: {n_missing}")
print(f"Missing percentage: {n_missing / n_total * 100:.2f}%")
print(f"Filled {filled_count} sample-timepoint combinations")
print(f"Missing {missing_count} sample-timepoint combinations")

# Check non-zero values among finite values
finite_values = tensor_3d[np.isfinite(tensor_3d)]
n_nonzero = np.sum(finite_values != 0)
print(f"Non-zero finite values: {n_nonzero}")
print(f"Zero finite values: {len(finite_values) - n_nonzero}")
print(f"Sparsity among finite values: {(len(finite_values) - n_nonzero) / len(finite_values) * 100:.2f}%")

# Save sample mapping for later interpretation
sample_mapping = pd.DataFrame([
    {
        'combined_index': i,
        'sample_label': label,
        'species': species_sample_map[i]['species'],
        'sample_id': species_sample_map[i]['sample_id']
    }
    for i, label in enumerate(sample_labels)
])
print(f"\nSample mapping:")
print(sample_mapping.head(10))
```

## Dissertation-Validated Parameter Selection

**RECOMMENDED APPROACH**: Split-Half Bootstrap Grid Search

This analysis uses the split-half bootstrap grid search approach recommended by the dissertation author [@blaskowski2024] for datasets with limited biological replicates.

**Key features:**
- **Split-half cross-validation** (50/50 train/test split)
- **Bootstrap resampling** with repeated random splits
- **Joint rank × lambda grid search** tests all parameter combinations
- **Two-stage selection** using 1SE rule for both rank and lambda

**Dissertation Reference:** Blaskowski (2024), Section 1.2.3

```{python dissertation-grid-search-functions, eval=TRUE}
# ===========================================
# SPLIT-HALF BOOTSTRAP GRID SEARCH
# Implementation of Blaskowski (2024) Section 1.2.3
# Modified with dissertation author's recommendation
# ===========================================
# Original: Leave-one-dataset-out CV with 3 datasets
# Modified: Split-half CV with bootstrap resampling
# 
# Rationale: With limited replicates (27 colonies, 3 species),
# split-half provides more stable train/test sets than LOOCV.
# Bootstrap iterations assess model training consistency.
# ===========================================

from barnacle.decomposition import SparseCP
from tlviz.factor_tools import factor_match_score
import numpy as np
import pandas as pd
from itertools import combinations


def create_stratified_split(species_sample_map, test_fraction=0.5, random_state=None):
    """
    Create stratified train/test split ensuring species representation.
    
    Parameters:
    -----------
    species_sample_map : dict
        Mapping of sample indices to species and sample_id
    test_fraction : float
        Fraction of samples to hold out for testing (default: 0.5 for split-half)
    random_state : int or None
        Random seed for reproducibility
    
    Returns:
    --------
    train_indices : list
        Sample indices for training
    test_indices : list
        Sample indices for testing
    """
    if random_state is not None:
        np.random.seed(random_state)
    
    # Group samples by species
    species_samples = {}
    for idx, info in species_sample_map.items():
        species = info['species']
        if species not in species_samples:
            species_samples[species] = []
        species_samples[species].append(idx)
    
    train_indices = []
    test_indices = []
    
    # Split each species proportionally
    for species, indices in species_samples.items():
        indices = list(indices)
        np.random.shuffle(indices)
        
        n_test = max(1, int(len(indices) * test_fraction))  # At least 1 per species
        test_indices.extend(indices[:n_test])
        train_indices.extend(indices[n_test:])
    
    return sorted(train_indices), sorted(test_indices)

def calculate_cv_sse(tensor, decomposition, train_indices, test_indices):
    """
    Calculate Sum of Squared Errors on test data using tensorly's built-in functions.
    
    This reconstructs the tensor using factors learned from training data
    and evaluates SSE on held-out test data.
    
    Approach:
    - Use gene and time factors from training decomposition (fixed)
    - Fit sample factors for test set using least-squares (optimal projection)
    - Reconstruct test tensor using tensorly's cp_to_tensor
    - Calculate SSE using standard formula
    
    Parameters:
    -----------
    tensor : 3D array
        Full tensor (genes × samples × timepoints)
    decomposition : barnacle decomposition object
        Fitted decomposition from training data
    train_indices : list
        Sample indices used for training
    test_indices : list
        Sample indices for test set
    
    Returns:
    --------
    sse : float
        Sum of squared errors on test data
    """
    import tensorly as tl
    from tensorly.tenalg import multi_mode_dot
    
    # Extract factors from training
    gene_factors = decomposition.factors[0]  # genes × rank
    sample_factors_train = decomposition.factors[1]  # train_samples × rank  
    time_factors = decomposition.factors[2]  # timepoints × rank
    weights = decomposition.weights  # rank
    
    # Get test tensor
    test_tensor = tensor[:, test_indices, :]
    n_genes, n_test_samples, n_timepoints = test_tensor.shape
    rank = len(weights)
    
    # Fit sample factors for test set using least-squares
    # This finds optimal sample factors given fixed gene and time factors
    # Solve: test_tensor ≈ gene_factors × sample_factors_test^T × time_factors
    # Using tensorly's contraction: unfold along sample mode and solve
    
    # Unfold test tensor along sample mode (mode 1)
    test_unfolded = tl.unfold(test_tensor, mode=1)  # n_test_samples × (n_genes * n_timepoints)
    
    # Compute Khatri-Rao product of gene and time factors
    # This represents the "design matrix" for sample factors
    kr_product = tl.tenalg.khatri_rao([gene_factors, time_factors])  # (n_genes * n_timepoints) × rank
    
    # Apply weights to Khatri-Rao product
    kr_weighted = kr_product * weights[np.newaxis, :]  # Broadcast weights
    
    # Solve least-squares: sample_factors_test = test_unfolded @ kr_weighted @ (kr_weighted^T @ kr_weighted)^-1
    # Simplified: use lstsq for numerical stability
    sample_factors_test, residuals, rank_kr, s = np.linalg.lstsq(
        kr_weighted, 
        test_unfolded.T,  # Transpose to solve for each sample
        rcond=None
    )
    sample_factors_test = sample_factors_test.T  # n_test_samples × rank
    
    # Reconstruct test tensor using tensorly's cp_to_tensor
    # This is the standard, validated reconstruction method
    cp_tensor_test = tl.cp_tensor.CPTensor(
        (weights, [gene_factors, sample_factors_test, time_factors])
    )
    reconstructed = tl.cp_to_tensor(cp_tensor_test)
    
    # Calculate SSE using standard formula
    # SSE = sum of squared differences between observed and reconstructed
    sse = tl.norm(test_tensor - reconstructed, 2) ** 2
    
    return sse


def _fit_single_model_combination(
    R, lambda_val, train_tensor, test_tensor, tensor_filled,
    train_indices, test_indices, boot_idx, boot_seed, n_iter_max
):
    """
    Helper function to fit a single rank×lambda combination.
    Used for parallel execution within bootstrap iterations.
    
    Parameters are intentionally simplified to avoid pickling issues.
    """
    try:
        # Fit model on TRAINING data
        model_train = SparseCP(
            rank=R,
            lambdas=[lambda_val, 0.0, lambda_val],
            nonneg_modes=[0],
            norm_constraint=True,
            init='random',
            tol=1e-5,
            n_iter_max=n_iter_max,
            random_state=boot_seed,
            n_initializations=1
        )
        decomp_train = model_train.fit_transform(train_tensor, verbose=0)
        
        # Calculate SSE on TEST data
        sse_test = calculate_cv_sse(tensor_filled, decomp_train, 
                                   train_indices, test_indices)
        
        # Fit model on TEST data (for FMS calculation)
        model_test = SparseCP(
            rank=R,
            lambdas=[lambda_val, 0.0, lambda_val],
            nonneg_modes=[0],
            norm_constraint=True,
            init='random',
            tol=1e-5,
            n_iter_max=n_iter_max,
            random_state=boot_seed + 1000,  # Different seed
            n_initializations=1
        )
        decomp_test = model_test.fit_transform(test_tensor, verbose=0)
        
        # Calculate FMS between train and test models
        try:
            cp_train = (
                decomp_train.weights,
                [decomp_train.factors[0],  # genes
                 decomp_train.factors[2]]  # time
            )
            cp_test = (
                decomp_test.weights,
                [decomp_test.factors[0],  # genes
                 decomp_test.factors[2]]  # time
            )
            
            fms = factor_match_score(cp_train, cp_test, return_permutation=False)
        except Exception as e:
            fms = np.nan
        
        return {
            'bootstrap_iter': boot_idx,
            'boot_seed': boot_seed,
            'rank': R,
            'lambda': lambda_val,
            'sse_test': sse_test,
            'fms': fms,
            'n_train': len(train_indices),
            'n_test': len(test_indices),
            'converged': True,
            'error': None
        }
        
    except Exception as e:
        return {
            'bootstrap_iter': boot_idx,
            'boot_seed': boot_seed,
            'rank': R,
            'lambda': lambda_val,
            'sse_test': np.inf,
            'fms': np.nan,
            'n_train': len(train_indices),
            'n_test': len(test_indices),
            'converged': False,
            'error': str(e)
        }


def split_half_bootstrap_grid_search(
    tensor,
    species_sample_map,
    rank_range=[5, 10, 15, 20, 25, 30, 35, 40],
    lambda_values=[0.0, 0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0],
    n_bootstrap=10,
    test_fraction=0.5,
    n_iter_max=10000,
    random_state=42,
    output_dir=None,
    resume=True,
    n_jobs=-1
):
    """
    Split-half bootstrap grid search with repeated random splits.
    
    INCREMENTAL SAVING: Results saved after each bootstrap iteration.
    RESUME CAPABILITY: Can load existing results and continue from last iteration.
    PARALLEL EXECUTION: Uses joblib to parallelize grid search within each bootstrap iteration.
    
    Recommended by dissertation author when you don't have enough replicates
    for separate training/test tensors.
    
    Approach:
    1. For each bootstrap iteration:
       - Create stratified 50/50 train/test split (species-balanced)
       - Fit models at all rank × lambda combinations ON PARALLEL (using joblib)
       - Evaluate on test set
       - SAVE RESULTS IMMEDIATELY to checkpoint file
    2. Aggregate results across bootstrap iterations
    3. Two-stage selection:
       - Stage 1: Select rank at minimum mean CV-SSE (λ=0.0 only)
       - Stage 2: Select lambda using FMS at optimal rank
    
    Parameters:
    -----------
    tensor : 3D array
        Full tensor (genes × samples × timepoints)
    species_sample_map : dict
        Mapping of sample indices to species/sample metadata
    rank_range : list
        Ranks to test
    lambda_values : list
        Lambda values to test
    n_bootstrap : int
        Number of random train/test splits (default: 10)
    test_fraction : float
        Fraction of samples for testing (default: 0.5 for split-half)
    n_iter_max : int
        Maximum iterations for optimization
    random_state : int
        Base random seed (each bootstrap uses seed + iteration)
    output_dir : str or None
        Directory to save incremental checkpoint files.
        If None, results stored in memory only (no saving/resume).
    resume : bool
        If True and output_dir exists, load completed iterations and continue.
        If False, delete existing checkpoints and start fresh.
    n_jobs : int
        Number of parallel jobs for grid search within each bootstrap iteration.
        -1 uses all available cores. 1 disables parallelization.
    
    Returns:
    --------
    optimal_rank : int
        Selected rank (minimum mean CV-SSE at λ=0.0)
    optimal_lambda : float
        Selected lambda (maximum λ with FMS ≥ max_FMS - 1SE)
    results_df : DataFrame
        Full results with bootstrap statistics
    bootstrap_results : list
        Raw results from each bootstrap iteration
    """
    import os
    from joblib import Parallel, delayed
    
    print("="*80)
    print("SPLIT-HALF BOOTSTRAP GRID SEARCH")
    print("="*80)
    print(f"Bootstrap iterations: {n_bootstrap}")
    print(f"Train/test split: {int((1-test_fraction)*100)}/{int(test_fraction*100)}")
    print(f"Ranks to test: {rank_range}")
    print(f"Lambdas to test: {lambda_values}")
    print(f"Total combinations: {len(rank_range)} × {len(lambda_values)} = {len(rank_range) * len(lambda_values)}")
    print(f"Total models to fit: {len(rank_range) * len(lambda_values) * n_bootstrap * 2}")
    print(f"  ({n_bootstrap} bootstrap × 2 models per split × {len(rank_range) * len(lambda_values)} combinations)")
    
    # Parallel execution setup
    if n_jobs == -1:
        import multiprocessing
        n_jobs_actual = multiprocessing.cpu_count()
        print(f"Parallel execution: ENABLED ({n_jobs_actual} cores)")
    elif n_jobs == 1:
        n_jobs_actual = 1
        print(f"Parallel execution: DISABLED (sequential)")
    else:
        n_jobs_actual = n_jobs
        print(f"Parallel execution: ENABLED ({n_jobs_actual} cores)")
    
    # Setup incremental saving
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        print(f"Output directory: {output_dir}")
        print(f"Incremental saving: ENABLED")
    else:
        print(f"Incremental saving: DISABLED (results in memory only)")
    
    print("="*80)
    
    # Handle missing values
    tensor_filled = np.nan_to_num(tensor, nan=0.0)
    
    # Storage for all bootstrap iterations
    all_bootstrap_results = []
    
    # Check for existing checkpoint files (resume capability)
    start_iter = 0
    if output_dir and resume:
        print("\nChecking for existing checkpoint files...")
        completed_iters = []
        for boot_idx in range(n_bootstrap):
            checkpoint_file = os.path.join(output_dir, f'bootstrap_checkpoint_iter{boot_idx:03d}.csv')
            if os.path.exists(checkpoint_file):
                completed_iters.append(boot_idx)
                # Load completed results
                iter_df = pd.read_csv(checkpoint_file)
                all_bootstrap_results.extend(iter_df.to_dict('records'))
        
        if completed_iters:
            start_iter = max(completed_iters) + 1
            print(f"  ✓ Found {len(completed_iters)} completed iterations: {completed_iters}")
            print(f"  → Resuming from iteration {start_iter}")
        else:
            print(f"  No existing checkpoints found. Starting from beginning.")
    elif output_dir and not resume:
        print("\nResume disabled. Deleting existing checkpoints...")
        for boot_idx in range(n_bootstrap):
            checkpoint_file = os.path.join(output_dir, f'bootstrap_checkpoint_iter{boot_idx:03d}.csv')
            if os.path.exists(checkpoint_file):
                os.remove(checkpoint_file)
                print(f"  ✗ Deleted: {checkpoint_file}")
    
    # Check if all iterations already complete
    if start_iter >= n_bootstrap:
        print(f"\n✓ All {n_bootstrap} bootstrap iterations already completed!")
        print(f"  Loaded {len(all_bootstrap_results)} results from checkpoints.")
        print(f"  Proceeding to aggregation...")
    else:
        print(f"\nRunning bootstrap iterations {start_iter} to {n_bootstrap-1}...")
    
    # Run bootstrap iterations
    for boot_idx in range(start_iter, n_bootstrap):
        boot_seed = random_state + boot_idx
        print(f"\n{'='*80}")
        print(f"BOOTSTRAP ITERATION {boot_idx + 1}/{n_bootstrap} (seed={boot_seed})")
        print("="*80)
        
        # Store results for this iteration
        iter_results = []
        
        # Create stratified train/test split
        train_indices, test_indices = create_stratified_split(
            species_sample_map, 
            test_fraction=test_fraction,
            random_state=boot_seed
        )
        
        print(f"Train samples: {len(train_indices)}, Test samples: {len(test_indices)}")
        
        # Count species in each split
        train_species = {}
        test_species = {}
        for idx in train_indices:
            sp = species_sample_map[idx]['species']
            train_species[sp] = train_species.get(sp, 0) + 1
        for idx in test_indices:
            sp = species_sample_map[idx]['species']
            test_species[sp] = test_species.get(sp, 0) + 1
        
        print(f"Train species: {train_species}")
        print(f"Test species: {test_species}")
        
        train_tensor = tensor_filled[:, train_indices, :]
        test_tensor = tensor_filled[:, test_indices, :]
        
        # Grid search over all rank × lambda combinations (PARALLEL)
        print(f"\nRunning grid search with {n_jobs_actual} parallel job(s)...")
        
        # Create list of all combinations to process
        combinations_to_process = [
            (R, lambda_val)
            for R in rank_range
            for lambda_val in lambda_values
        ]
        
        # Parallel execution of all rank×lambda combinations
        iter_results = Parallel(n_jobs=n_jobs_actual, verbose=10)(
            delayed(_fit_single_model_combination)(
                R, lambda_val, train_tensor, test_tensor, tensor_filled,
                train_indices, test_indices, boot_idx, boot_seed, n_iter_max
            )
            for R, lambda_val in combinations_to_process
        )
        
        # Print summary of results
        n_converged = sum(1 for r in iter_results if r['converged'])
        n_failed = len(iter_results) - n_converged
        print(f"\nCompleted {len(iter_results)} model fits:")
        print(f"  ✓ Converged: {n_converged}")
        if n_failed > 0:
            print(f"  ✗ Failed: {n_failed}")
        
        # SAVE CHECKPOINT after completing this bootstrap iteration
        all_bootstrap_results.extend(iter_results)
        
        if output_dir:
            checkpoint_file = os.path.join(output_dir, f'bootstrap_checkpoint_iter{boot_idx:03d}.csv')
            iter_df = pd.DataFrame(iter_results)
            iter_df.to_csv(checkpoint_file, index=False)
            print(f"\n✓ Saved checkpoint: {checkpoint_file}")
            print(f"  ({len(iter_results)} results for iteration {boot_idx + 1})")
    
    # Convert to DataFrame
    bootstrap_df = pd.DataFrame(all_bootstrap_results)
    
    # Calculate aggregate statistics across bootstrap iterations
    print(f"\n{'='*80}")
    print("AGGREGATING BOOTSTRAP RESULTS")
    print("="*80)
    
    agg_results = []
    for (R, lambda_val), group in bootstrap_df.groupby(['rank', 'lambda']):
        valid_sse = group[group['converged']]['sse_test']
        valid_fms = group[group['converged']]['fms'].dropna()
        
        agg_results.append({
            'rank': R,
            'lambda': lambda_val,
            'mean_sse': valid_sse.mean() if len(valid_sse) > 0 else np.inf,
            'std_sse': valid_sse.std() if len(valid_sse) > 0 else np.inf,
            'se_sse': valid_sse.sem() if len(valid_sse) > 0 else np.inf,
            'mean_fms': valid_fms.mean() if len(valid_fms) > 0 else np.nan,
            'std_fms': valid_fms.std() if len(valid_fms) > 0 else np.nan,
            'se_fms': valid_fms.sem() if len(valid_fms) > 0 else np.nan,
            'n_converged': group['converged'].sum(),
            'n_total': len(group)
        })
    
    results_df = pd.DataFrame(agg_results)
    
    # ===========================================
    # STAGE 1: SELECT RANK (λ=0.0 models only)
    # ===========================================
    print(f"\n{'='*80}")
    print("STAGE 1: RANK SELECTION (1SE RULE)")
    print("Criterion: Smallest rank within 1SE of minimum SSE at λ=0.0")
    print("="*80)
    
    lambda_zero_df = results_df[results_df['lambda'] == 0.0].copy()
    
    if len(lambda_zero_df) == 0:
        print("ERROR: No λ=0.0 models found")
        return None, None, results_df, all_bootstrap_results
    
    # Find minimum SSE and its SE
    min_sse_idx = lambda_zero_df['mean_sse'].idxmin()
    min_sse = lambda_zero_df.loc[min_sse_idx, 'mean_sse']
    min_sse_se = lambda_zero_df.loc[min_sse_idx, 'se_sse']
    min_sse_rank = int(lambda_zero_df.loc[min_sse_idx, 'rank'])
    
    # Calculate 1SE threshold (minimum SSE + 1 standard error)
    sse_1se_threshold = min_sse + min_sse_se
    
    print(f"\nMinimum SSE: {min_sse:.2e} ± {min_sse_se:.2e} (at rank={min_sse_rank})")
    print(f"1SE Threshold: {sse_1se_threshold:.2e}")
    print(f"\nApplying 1SE rule (parsimony principle):")
    print(f"  Select smallest rank where SSE ≤ {sse_1se_threshold:.2e}")
    
    # Find all ranks within 1SE of minimum
    within_1se = lambda_zero_df[lambda_zero_df['mean_sse'] <= sse_1se_threshold].copy()
    within_1se = within_1se.sort_values('rank')
    
    if len(within_1se) > 0:
        # Select SMALLEST rank (most parsimonious) within 1SE
        optimal_rank = int(within_1se.iloc[0]['rank'])
        optimal_rank_sse = within_1se.iloc[0]['mean_sse']
        optimal_rank_se = within_1se.iloc[0]['se_sse']
        
        print(f"\n✓ OPTIMAL RANK (1SE rule): {optimal_rank}")
        print(f"  Mean SSE: {optimal_rank_sse:.2e} ± {optimal_rank_se:.2e}")
        print(f"  (smallest rank with SSE within 1SE of minimum)")
        
        if optimal_rank != min_sse_rank:
            print(f"\n  Note: Rank {min_sse_rank} had absolute minimum SSE,")
            print(f"        but rank {optimal_rank} selected by 1SE rule (more parsimonious)")
        
        print(f"\nRanks within 1SE of minimum:")
        display_df = within_1se[['rank', 'mean_sse', 'se_sse', 'n_converged']].copy()
        print(display_df.to_string(index=False))
    else:
        # Fallback: use minimum SSE rank
        optimal_rank = min_sse_rank
        optimal_rank_sse = min_sse
        optimal_rank_se = min_sse_se
        print(f"\n✓ OPTIMAL RANK (minimum SSE): {optimal_rank}")
        print(f"  Mean SSE: {optimal_rank_sse:.2e} ± {optimal_rank_se:.2e}")
        print(f"  (no other ranks within 1SE)")
    
    print(f"\nAll λ=0.0 results (for reference):")
    display_all = lambda_zero_df[['rank', 'mean_sse', 'se_sse', 'n_converged']].copy()
    display_all = display_all.sort_values('rank')
    display_all['within_1se'] = display_all['mean_sse'] <= sse_1se_threshold
    print(display_all.to_string(index=False))
    
    # ===========================================
    # STAGE 2: SELECT LAMBDA (fixed rank)
    # ===========================================
    print(f"\n{'='*80}")
    print("STAGE 2: LAMBDA SELECTION (1SE RULE)")
    print("Criterion: Maximum λ where FMS ≥ (max_FMS - 1SE)")
    print("="*80)
    
    fixed_rank_df = results_df[results_df['rank'] == optimal_rank].copy()
    fixed_rank_df = fixed_rank_df[~fixed_rank_df['mean_fms'].isna()]
    
    if len(fixed_rank_df) == 0:
        print("WARNING: No valid FMS scores at optimal rank")
        optimal_lambda = 0.0
    else:
        max_fms_idx = fixed_rank_df['mean_fms'].idxmax()
        max_fms = fixed_rank_df.loc[max_fms_idx, 'mean_fms']
        max_fms_se = fixed_rank_df.loc[max_fms_idx, 'se_fms']
        
        threshold = max_fms - max_fms_se
        
        print(f"\nMax FMS: {max_fms:.4f} ± {max_fms_se:.4f}")
        print(f"1SE Threshold: {threshold:.4f}")
        
        within_1se = fixed_rank_df[fixed_rank_df['mean_fms'] >= threshold]
        
        if len(within_1se) > 0:
            optimal_lambda_idx = within_1se['lambda'].idxmax()
            optimal_lambda = float(within_1se.loc[optimal_lambda_idx, 'lambda'])
            optimal_fms = within_1se.loc[optimal_lambda_idx, 'mean_fms']
            
            print(f"\n✓ OPTIMAL LAMBDA: {optimal_lambda}")
            print(f"  Mean FMS: {optimal_fms:.4f}")
        else:
            optimal_lambda = 0.0
            print(f"\nWARNING: No lambda within 1SE, defaulting to 0.0")
    
    print(f"\n{'='*80}")
    print("BOOTSTRAP STABILITY ASSESSMENT")
    print("="*80)
    
    # Check coefficient of variation for optimal parameters
    optimal_combo = bootstrap_df[
        (bootstrap_df['rank'] == optimal_rank) & 
        (bootstrap_df['lambda'] == optimal_lambda) &
        (bootstrap_df['converged'])
    ]
    
    if len(optimal_combo) > 0:
        cv_sse = optimal_combo['sse_test'].std() / optimal_combo['sse_test'].mean()
        cv_fms = optimal_combo['fms'].std() / optimal_combo['fms'].mean() if optimal_combo['fms'].notna().sum() > 0 else np.nan
        
        print(f"Optimal rank={optimal_rank}, lambda={optimal_lambda}:")
        print(f"  SSE coefficient of variation: {cv_sse:.3f}")
        print(f"  FMS coefficient of variation: {cv_fms:.3f}")
        print(f"  Convergence rate: {len(optimal_combo)}/{n_bootstrap} ({len(optimal_combo)/n_bootstrap*100:.1f}%)")
        
        if cv_sse < 0.1:
            print(f"  ✓ Excellent stability (CV < 0.1)")
        elif cv_sse < 0.2:
            print(f"  ✓ Good stability (CV < 0.2)")
        else:
            print(f"  ⚠ Variable results (CV > 0.2) - consider more bootstrap iterations")
    
    print(f"\n{'='*80}")
    print("FINAL SELECTED PARAMETERS")
    print("="*80)
    print(f"✓ Rank: {optimal_rank}")
    print(f"✓ Lambda: {optimal_lambda}")
    print(f"  Based on {n_bootstrap} bootstrap iterations")
    print("="*80)
    
    return optimal_rank, optimal_lambda, results_df, all_bootstrap_results


def dissertation_grid_search_cv(
    tensor,
    replicate_groups,
    rank_range=[5, 10, 15, 20, 25, 30, 35],
    lambda_values=[0.0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],
    n_iter_max=10000,
    random_state=42
):
    """
    [LEGACY] Full dissertation grid search: Test all rank × lambda combinations.
    
    ⚠️ THIS FUNCTION IS NO LONGER USED IN CURRENT ANALYSIS
    Replaced by split_half_bootstrap_grid_search() based on author consultation.
    Preserved for reference and comparison purposes only.
    
    Original implementation of Blaskowski (2024) Section 1.2.3, page 20-21:
    "We fit a series of models to each replicate subtensor using a grid search 
     of different R and λ parameter values."
    
    For each (rank, lambda) combination:
    1. Fit model to each CV fold (leave-one-group-out)
    2. Calculate cross-validated SSE scores (each model vs held-out data)
    3. Calculate cross-validated FMS scores (pairwise between models)
    
    Two-stage selection (Dissertation method):
    - Stage 1: Select R at minimum CV-SSE for λ=0.0 models
    - Stage 2: Select maximum λ where CV-FMS ≥ (max_FMS - 1SE)
    
    Parameters:
    -----------
    tensor : 3D array
        Full tensor (genes × samples × timepoints)
    replicate_groups : dict
        Keys: group names (e.g., species)
        Values: list of sample indices
    rank_range : list
        Ranks to test
    lambda_values : list
        Lambda values to test
    n_iter_max : int
        Maximum iterations for optimization
    random_state : int
        Random seed
    
    Returns:
    --------
    optimal_rank : int
        Selected rank (minimum CV-SSE at λ=0.0)
    optimal_lambda : float
        Selected lambda (maximum λ with FMS ≥ max_FMS - 1SE)
    grid_results_df : DataFrame
        Full grid search results with SSE and FMS for all combinations
    """
    
    print("="*80)
    print("FULL DISSERTATION GRID SEARCH")
    print("Blaskowski (2024) Section 1.2.3")
    print("="*80)
    print(f"Testing {len(rank_range)} ranks: {rank_range}")
    print(f"Testing {len(lambda_values)} lambdas: {lambda_values}")
    print(f"Total combinations: {len(rank_range) * len(lambda_values)}")
    print(f"Cross-validation: Leave-one-group-out ({len(replicate_groups)} folds)")
    print(f"Random state: {random_state}")
    print("="*80)
    print("\nFor each (R, λ) combination, calculating:")
    print(f"  • {len(replicate_groups)} × {len(replicate_groups)-1} = {len(replicate_groups) * (len(replicate_groups)-1)} SSE scores")
    print(f"    (each model evaluated on {len(replicate_groups)-1} held-out groups)")
    print(f"  • {len(list(combinations(range(len(replicate_groups)), 2)))} FMS scores")
    print(f"    (pairwise comparisons between {len(replicate_groups)} models)")
    print("="*80)
    
    # Handle missing values once
    tensor_filled = np.nan_to_num(tensor, nan=0.0)
    
    # Storage for all grid search results
    grid_results = []
    
    # Grid search over all rank × lambda combinations
    total_combinations = len(rank_range) * len(lambda_values)
    combination_idx = 0
    
    for R in rank_range:
        for lambda_val in lambda_values:
            combination_idx += 1
            
            print(f"\n{'='*80}")
            print(f"Combination {combination_idx}/{total_combinations}: Rank={R}, Lambda={lambda_val}")
            print(f"{'='*80}")
            
            # Storage for this combination
            fold_decompositions = []
            fold_metadata = []
            all_sse_scores = []
            
            # Fit model to each CV fold
            for fold_idx, (group_name, test_indices) in enumerate(replicate_groups.items()):
                print(f"  Fold {fold_idx+1}/{len(replicate_groups)}: Training without {group_name}")
                
                # Create training data (all groups except current)
                train_indices = []
                for other_name, other_indices in replicate_groups.items():
                    if other_name != group_name:
                        train_indices.extend(other_indices)
                
                train_tensor = tensor_filled[:, train_indices, :]
                
                try:
                    # Fit model
                    model = SparseCP(
                        rank=R,
                        lambdas=[lambda_val, 0.0, lambda_val],  # Apply to gene and time factors
                        nonneg_modes=[0],
                        norm_constraint=True,
                        init='random',
                        tol=1e-5,
                        n_iter_max=n_iter_max,
                        random_state=random_state,
                        n_initializations=1
                    )
                    
                    decomposition = model.fit_transform(train_tensor, verbose=0)
                    
                    # Calculate SSE on THIS fold's held-out data
                    sse_own = calculate_cv_sse(tensor_filled, decomposition, train_indices, test_indices)
                    
                    # Calculate SSE on ALL OTHER held-out groups
                    sse_others = []
                    for other_name, other_test_indices in replicate_groups.items():
                        if other_name != group_name:
                            sse_other = calculate_cv_sse(tensor_filled, decomposition, 
                                                        train_indices, other_test_indices)
                            sse_others.append(sse_other)
                            all_sse_scores.append(sse_other)
                    
                    print(f"    ✓ SSE on {group_name}: {sse_own:.2e}")
                    print(f"      SSE on others: {np.mean(sse_others):.2e} ± {np.std(sse_others):.2e}")
                    
                    fold_decompositions.append(decomposition)
                    fold_metadata.append({
                        'fold': fold_idx + 1,
                        'group': group_name,
                        'sse_own': sse_own,
                        'sse_others_mean': np.mean(sse_others),
                        'converged': True
                    })
                    
                except Exception as e:
                    print(f"    ✗ Error: {e}")
                    fold_decompositions.append(None)
                    fold_metadata.append({
                        'fold': fold_idx + 1,
                        'group': group_name,
                        'sse_own': np.nan,
                        'sse_others_mean': np.nan,
                        'converged': False
                    })
            
            # Calculate pairwise FMS between successful folds
            successful_decompositions = [d for d in fold_decompositions if d is not None]
            
            fms_scores = []
            if len(successful_decompositions) >= 2:
                print(f"\n  Calculating FMS between {len(successful_decompositions)} successful models...")
                
                for i, j in combinations(range(len(successful_decompositions)), 2):
                    try:
                        # Compare only gene and time factors
                        # (sample factors have different dimensions across folds)
                        cp_tensor_i = (
                            successful_decompositions[i].weights,
                            [successful_decompositions[i].factors[0],  # genes
                             successful_decompositions[i].factors[2]]  # time
                        )
                        cp_tensor_j = (
                            successful_decompositions[j].weights,
                            [successful_decompositions[j].factors[0],  # genes
                             successful_decompositions[j].factors[2]]  # time
                        )
                        
                        fms = factor_match_score(
                            cp_tensor_i, 
                            cp_tensor_j,
                            return_permutation=False
                        )
                        fms_scores.append(fms)
                        
                    except Exception as e:
                        print(f"    Warning: FMS calculation failed for pair ({i},{j}): {e}")
                        continue
            
            # Aggregate results for this combination
            valid_sse = [s for s in all_sse_scores if not np.isnan(s)]
            valid_fms = [f for f in fms_scores if not np.isnan(f)]
            
            if len(valid_sse) > 0:
                mean_cv_sse = np.mean(valid_sse)
                std_cv_sse = np.std(valid_sse)
                se_cv_sse = std_cv_sse / np.sqrt(len(valid_sse))
            else:
                mean_cv_sse = np.inf
                std_cv_sse = np.inf
                se_cv_sse = np.inf
            
            if len(valid_fms) > 0:
                mean_cv_fms = np.mean(valid_fms)
                std_cv_fms = np.std(valid_fms)
                se_cv_fms = std_cv_fms / np.sqrt(len(valid_fms))
            else:
                mean_cv_fms = np.nan
                std_cv_fms = np.nan
                se_cv_fms = np.nan
            
            grid_results.append({
                'rank': R,
                'lambda': lambda_val,
                'mean_cv_sse': mean_cv_sse,
                'std_cv_sse': std_cv_sse,
                'se_cv_sse': se_cv_sse,
                'n_sse_scores': len(valid_sse),
                'mean_cv_fms': mean_cv_fms,
                'std_cv_fms': std_cv_fms,
                'se_cv_fms': se_cv_fms,
                'n_fms_scores': len(valid_fms),
                'n_successful_folds': len(successful_decompositions),
                'fold_metadata': fold_metadata
            })
            
            print(f"\n  RESULTS:")
            print(f"    CV-SSE: {mean_cv_sse:.2e} ± {std_cv_sse:.2e} ({len(valid_sse)} scores)")
            print(f"    CV-FMS: {mean_cv_fms:.4f} ± {std_cv_fms:.4f} ({len(valid_fms)} scores)")
    
    # Convert to DataFrame
    grid_df = pd.DataFrame(grid_results)
    
    # ===========================================
    # STAGE 1: SELECT RANK (λ=0.0 models only)
    # ===========================================
    print(f"\n{'='*80}")
    print("STAGE 1: RANK SELECTION")
    print("Dissertation: 'Select R at minimum CV-SSE for λ=0.0 models'")
    print("="*80)
    
    lambda_zero_df = grid_df[grid_df['lambda'] == 0.0].copy()
    
    if len(lambda_zero_df) == 0 or lambda_zero_df['mean_cv_sse'].isna().all():
        print("ERROR: No valid λ=0.0 models. Cannot select rank.")
        return None, None, grid_df
    
    optimal_rank_idx = lambda_zero_df['mean_cv_sse'].idxmin()
    optimal_rank = int(lambda_zero_df.loc[optimal_rank_idx, 'rank'])
    optimal_rank_sse = lambda_zero_df.loc[optimal_rank_idx, 'mean_cv_sse']
    optimal_rank_std = lambda_zero_df.loc[optimal_rank_idx, 'std_cv_sse']
    
    print(f"\n✓ OPTIMAL RANK: {optimal_rank}")
    print(f"  CV-SSE: {optimal_rank_sse:.2e} ± {optimal_rank_std:.2e}")
    print(f"\nAll λ=0.0 results:")
    display_df = lambda_zero_df[['rank', 'mean_cv_sse', 'std_cv_sse', 'n_successful_folds']].copy()
    display_df = display_df.sort_values('mean_cv_sse')
    print(display_df.to_string(index=False, float_format=lambda x: f'{x:.2e}' if x > 1 else f'{x:.4f}'))
    
    # ===========================================
    # STAGE 2: SELECT LAMBDA (fixed rank)
    # ===========================================
    print(f"\n{'='*80}")
    print("STAGE 2: LAMBDA SELECTION")
    print("Dissertation: 'Select maximum λ where CV-FMS ≥ (max_FMS - 1SE)'")
    print("="*80)
    
    fixed_rank_df = grid_df[grid_df['rank'] == optimal_rank].copy()
    fixed_rank_df = fixed_rank_df[~fixed_rank_df['mean_cv_fms'].isna()]
    
    if len(fixed_rank_df) == 0:
        print("ERROR: No valid FMS scores at optimal rank. Cannot select lambda.")
        return optimal_rank, None, grid_df
    
    # Find maximum FMS
    max_fms_idx = fixed_rank_df['mean_cv_fms'].idxmax()
    max_fms = fixed_rank_df.loc[max_fms_idx, 'mean_cv_fms']
    max_fms_se = fixed_rank_df.loc[max_fms_idx, 'se_cv_fms']
    max_fms_lambda = fixed_rank_df.loc[max_fms_idx, 'lambda']
    
    # Calculate 1SE threshold
    fms_1se_threshold = max_fms - max_fms_se
    
    print(f"\nMaximum FMS: {max_fms:.4f} ± {max_fms_se:.4f} (at λ={max_fms_lambda})")
    print(f"1SE Threshold: {fms_1se_threshold:.4f}")
    
    # Find maximum lambda within 1SE
    within_1se = fixed_rank_df[fixed_rank_df['mean_cv_fms'] >= fms_1se_threshold]
    
    if len(within_1se) > 0:
        optimal_lambda_idx = within_1se['lambda'].idxmax()
        optimal_lambda = float(within_1se.loc[optimal_lambda_idx, 'lambda'])
        optimal_lambda_fms = within_1se.loc[optimal_lambda_idx, 'mean_cv_fms']
        optimal_lambda_sse = within_1se.loc[optimal_lambda_idx, 'mean_cv_sse']
        
        print(f"\n✓ OPTIMAL LAMBDA: {optimal_lambda}")
        print(f"  CV-FMS: {optimal_lambda_fms:.4f}")
        print(f"  CV-SSE: {optimal_lambda_sse:.2e}")
        print(f"  (Maximum λ where FMS ≥ {fms_1se_threshold:.4f})")
        
        print(f"\nAll results at rank={optimal_rank}:")
        display_df = fixed_rank_df[['lambda', 'mean_cv_fms', 'se_cv_fms', 'mean_cv_sse']].copy()
        display_df = display_df.sort_values('lambda')
        print(display_df.to_string(index=False, float_format=lambda x: f'{x:.4f}'))
    else:
        optimal_lambda = 0.0
        print(f"\nWARNING: No lambda values within 1SE of max FMS.")
        print(f"Defaulting to λ=0.0")
    
    print(f"\n{'='*80}")
    print("FINAL SELECTED PARAMETERS")
    print("="*80)
    print(f"✓ Rank: {optimal_rank}")
    print(f"✓ Lambda: {optimal_lambda}")
    print("="*80)
    
    return optimal_rank, optimal_lambda, grid_df


print("="*80)
print("PARAMETER SELECTION FUNCTIONS LOADED")
print("="*80)
print("✓ CURRENT APPROACH (RECOMMENDED):")
print("  - split_half_bootstrap_grid_search()")
print("    → Split-half CV with bootstrap resampling")
print("    → Incremental checkpointing & resume capability")
print("    → Recommended by dissertation author for limited replicates")
print("    → Assesses model training consistency")
print("")
print("⚠ LEGACY APPROACHES (NOT USED):")
print("  - dissertation_grid_search_cv()")
print("    → Leave-one-out cross-validation")
print("    → Original dissertation approach")
print("    → Preserved for reference only")
print("="*80)
```

### Run Split-Half Bootstrap Grid Search (RECOMMENDED)

**RECOMMENDED APPROACH** based on consultation with dissertation author.

**Why split-half with bootstrap?**
- You don't have enough replicates to create independent train/test tensors
- Split-half provides stable train/test sets (unlike leave-one-out)
- Bootstrap iterations assess model training consistency
- Validates that results are reproducible regardless of which samples are used

**Parallel Execution:**
- Grid search within each bootstrap iteration runs in parallel using `joblib`
- Set `n_jobs=-1` to use all available CPU cores (default)
- Set `n_jobs=1` to disable parallelization (sequential execution)
- Significantly reduces runtime on multi-core systems
- Example: 56 models with 8 cores = ~7x faster than sequential

**Incremental Saving:**
- Results saved after each bootstrap iteration completes
- If interrupted, can resume from last completed iteration
- Checkpoint files: `bootstrap_checkpoint_iter000.csv`, `bootstrap_checkpoint_iter001.csv`, etc.
- Set `resume=True` (default) to continue from existing checkpoints
- Set `resume=False` to delete checkpoints and start fresh

**Workflow:**
1. Run this chunk (runtime depends on cores available and n_bootstrap)
2. If interrupted, re-run to resume from last checkpoint
3. Final results automatically saved to aggregated files
4. Visualization chunk loads from files independently

```{python run-split-half-bootstrap, eval=FALSE}
# ===========================================
# RUN SPLIT-HALF BOOTSTRAP GRID SEARCH
# ===========================================
# Recommended by dissertation author for limited replicate scenarios
# ===========================================

print("="*80)
print("SPLIT-HALF BOOTSTRAP PARAMETER SELECTION")
print("="*80)

# ============================================
# CONFIGURE PARAMETERS
# ============================================

# Rank range to test
BOOTSTRAP_RANK_RANGE = [5, 10, 15, 20, 25, 30, 35, 40]

# Lambda values to test
BOOTSTRAP_LAMBDA_VALUES = [0.0, 0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0]

# Bootstrap parameters
BOOTSTRAP_N_ITERATIONS = 10  # Number of random train/test splits
BOOTSTRAP_TEST_FRACTION = 0.5  # 50/50 split
BOOTSTRAP_N_ITER_MAX = 10000
BOOTSTRAP_RANDOM_STATE = 42
BOOTSTRAP_N_JOBS = -1  # Use all available cores (-1), or set to specific number (1 = sequential)

print(f"\nConfiguration:")
print(f"  Ranks: {BOOTSTRAP_RANK_RANGE}")
print(f"  Lambdas: {BOOTSTRAP_LAMBDA_VALUES}")
print(f"  Bootstrap iterations: {BOOTSTRAP_N_ITERATIONS}")
print(f"  Train/test split: {int((1-BOOTSTRAP_TEST_FRACTION)*100)}/{int(BOOTSTRAP_TEST_FRACTION*100)}")
print(f"  Parallel jobs: {BOOTSTRAP_N_JOBS} ({'all cores' if BOOTSTRAP_N_JOBS == -1 else 'cores' if BOOTSTRAP_N_JOBS > 1 else 'sequential'})")
print("="*80)

# Estimate runtime
total_models = len(BOOTSTRAP_RANK_RANGE) * len(BOOTSTRAP_LAMBDA_VALUES) * BOOTSTRAP_N_ITERATIONS * 2
print(f"\nEstimated workload:")
print(f"  Total model fits: {total_models}")
print(f"  ({len(BOOTSTRAP_RANK_RANGE)} ranks × {len(BOOTSTRAP_LAMBDA_VALUES)} lambdas × {BOOTSTRAP_N_ITERATIONS} bootstrap × 2 models per split)")
if BOOTSTRAP_N_JOBS == -1 or BOOTSTRAP_N_JOBS > 1:
    import multiprocessing
    n_cores = multiprocessing.cpu_count() if BOOTSTRAP_N_JOBS == -1 else BOOTSTRAP_N_JOBS
    print(f"  Estimated time with {n_cores} cores: ~{total_models * 2 / 60 / n_cores:.1f}-{total_models * 5 / 60 / n_cores:.1f} minutes")
    print(f"    (assuming 2-5 min per model, parallelized)")
else:
    print(f"  Estimated time (sequential): ~{total_models * 2 / 60:.1f}-{total_models * 5 / 60:.1f} minutes")
    print(f"    (assuming 2-5 min per model)")
print("="*80)
print("\n⚠ NOTE: Set eval=TRUE above when ready to run")
print("="*80)

import time
start_time = time.time()

# Create output directory for checkpoints
bootstrap_dir = os.path.join(output_dir, 'bootstrap_grid_search')
os.makedirs(bootstrap_dir, exist_ok=True)

# Run split-half bootstrap grid search with incremental saving and parallel execution
optimal_rank_boot, optimal_lambda_boot, results_df_boot, bootstrap_raw = split_half_bootstrap_grid_search(
    tensor=tensor_3d,
    species_sample_map=species_sample_map,
    rank_range=BOOTSTRAP_RANK_RANGE,
    lambda_values=BOOTSTRAP_LAMBDA_VALUES,
    n_bootstrap=BOOTSTRAP_N_ITERATIONS,
    test_fraction=BOOTSTRAP_TEST_FRACTION,
    n_iter_max=BOOTSTRAP_N_ITER_MAX,
    random_state=BOOTSTRAP_RANDOM_STATE,
    output_dir=bootstrap_dir,  # Enable incremental saving
    resume=True,  # Resume from existing checkpoints if interrupted
    n_jobs=BOOTSTRAP_N_JOBS  # Parallel execution
)

elapsed_time = time.time() - start_time

print(f"\n{'='*80}")
print("BOOTSTRAP GRID SEARCH COMPLETE")
print("="*80)
print(f"Total time: {elapsed_time / 60:.1f} minutes")
print(f"Time per model: {elapsed_time / total_models:.1f} seconds")
print("="*80)

# Save results (final aggregated files)
# Note: Individual iteration checkpoints already saved incrementally

# Save aggregated results
results_df_boot.to_csv(
    os.path.join(bootstrap_dir, 'bootstrap_aggregated_results.csv'),
    index=False
)
print(f"\n✓ Aggregated results saved to: bootstrap_aggregated_results.csv")

# Save raw bootstrap data
pd.DataFrame(bootstrap_raw).to_csv(
    os.path.join(bootstrap_dir, 'bootstrap_raw_iterations.csv'),
    index=False
)
print(f"✓ Raw bootstrap data saved to: bootstrap_raw_iterations.csv")

# Save optimal parameters
import json
with open(os.path.join(bootstrap_dir, 'optimal_parameters.json'), 'w') as f:
    json.dump({
        'method': 'split_half_bootstrap',
        'reference': 'Blaskowski 2024 (modified with author consultation)',
        'optimal_rank': int(optimal_rank_boot) if optimal_rank_boot is not None else None,
        'optimal_lambda': float(optimal_lambda_boot) if optimal_lambda_boot is not None else None,
        'rank_range_tested': BOOTSTRAP_RANK_RANGE,
        'lambda_values_tested': BOOTSTRAP_LAMBDA_VALUES,
        'n_bootstrap_iterations': BOOTSTRAP_N_ITERATIONS,
        'test_fraction': BOOTSTRAP_TEST_FRACTION,
        'random_state': BOOTSTRAP_RANDOM_STATE,
        'timestamp': pd.Timestamp.now().isoformat(),
        'elapsed_minutes': elapsed_time / 60
    }, f, indent=2)

print(f"✓ Optimal parameters saved to: optimal_parameters.json")
print(f"\n✓ All results saved to: {bootstrap_dir}/")
print("="*80)
```

### Visualize Bootstrap Results

```{python visualize-bootstrap-results, eval=FALSE}
# ===========================================
# VISUALIZE BOOTSTRAP GRID SEARCH RESULTS
# ===========================================
# This chunk loads from files, so can run independently
# ===========================================

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import os
import json

bootstrap_dir = os.path.join(output_dir, 'bootstrap_grid_search')

# Check if results exist
results_file = os.path.join(bootstrap_dir, 'bootstrap_aggregated_results.csv')
if not os.path.exists(results_file):
    print("ERROR: Bootstrap results not found. Run the bootstrap grid search first.")
else:
    # Load results
    results_df = pd.read_csv(results_file)
    
    # Load optimal parameters (if exists, otherwise calculate with 1SE rule)
    params_file = os.path.join(bootstrap_dir, 'optimal_parameters.json')
    if os.path.exists(params_file):
        with open(params_file, 'r') as f:
            optimal_params = json.load(f)
        optimal_rank = optimal_params['optimal_rank']
        optimal_lambda = optimal_params['optimal_lambda']
        n_bootstrap = optimal_params['n_bootstrap_iterations']
    else:
        # Calculate optimal rank using 1SE rule
        lambda_zero = results_df[results_df['lambda'] == 0.0].copy()
        min_idx = lambda_zero['mean_sse'].idxmin()
        min_sse = lambda_zero.loc[min_idx, 'mean_sse']
        min_se = lambda_zero.loc[min_idx, 'se_sse']
        threshold = min_sse + min_se
        within_1se = lambda_zero[lambda_zero['mean_sse'] <= threshold].sort_values('rank')
        optimal_rank = int(within_1se.iloc[0]['rank'])
        
        # Calculate optimal lambda using 1SE rule
        fixed_rank = results_df[results_df['rank'] == optimal_rank].copy()
        fixed_rank = fixed_rank[~fixed_rank['mean_fms'].isna()]
        if len(fixed_rank) > 0:
            max_idx = fixed_rank['mean_fms'].idxmax()
            max_fms = fixed_rank.loc[max_idx, 'mean_fms']
            max_se = fixed_rank.loc[max_idx, 'se_fms']
            fms_threshold = max_fms - max_se
            within_1se_fms = fixed_rank[fixed_rank['mean_fms'] >= fms_threshold]
            optimal_lambda = float(within_1se_fms['lambda'].max())
        else:
            optimal_lambda = 0.0
        
        n_bootstrap = 10  # Default if not in file
    
    print(f"Loaded bootstrap results:")
    print(f"  Optimal rank (1SE rule): {optimal_rank_1se}")
    print(f"  Optimal lambda (1SE rule): {optimal_lambda}")
    print(f"  Bootstrap iterations: {n_bootstrap}")
    
    # Create visualization directory
    fig_dir = os.path.join(bootstrap_dir, 'figures')
    os.makedirs(fig_dir, exist_ok=True)
    
    # ========================================
    # FIGURE 1: SSE Heatmap
    # ========================================
    fig, ax = plt.subplots(figsize=(12, 8))
    
    plot_df = results_df.pivot(index='rank', columns='lambda', values='mean_sse')
    
    sns.heatmap(plot_df, annot=True, fmt='.2e', cmap='YlOrRd_r',
                cbar_kws={'label': 'Mean SSE'}, ax=ax)
    
    # Mark optimal
    if optimal_rank_1se is not None and optimal_lambda is not None:
        rank_pos = list(plot_df.index).index(optimal_rank_1se)
        lambda_pos = list(plot_df.columns).index(optimal_lambda)
        ax.plot(lambda_pos + 0.5, rank_pos + 0.5, 'b*', markersize=20, 
                markeredgewidth=2, markeredgecolor='white')
    
    ax.set_title('Bootstrap Grid Search: Mean SSE across Rank × Lambda', 
                fontsize=14, fontweight='bold')
    ax.set_xlabel('Lambda (Regularization)', fontsize=12)
    ax.set_ylabel('Rank', fontsize=12)
    
    plt.tight_layout()
    plt.savefig(os.path.join(fig_dir, 'bootstrap_sse_heatmap.png'),
                dpi=300, bbox_inches='tight')
    plt.show()
    plt.close()
    
    print("✓ Figure 1 saved: bootstrap_sse_heatmap.png")
    
    # ========================================
    # FIGURE 2: FMS Heatmap
    # ========================================
    fig, ax = plt.subplots(figsize=(12, 8))
    
    plot_df_fms = results_df.pivot(index='rank', columns='lambda', values='mean_fms')
    
    sns.heatmap(plot_df_fms, annot=True, fmt='.3f', cmap='RdYlGn',
                vmin=0, vmax=1, cbar_kws={'label': 'Mean FMS'}, ax=ax)
    
    # Mark optimal
    if optimal_rank_1se is not None and optimal_lambda is not None:
        if optimal_lambda in plot_df_fms.columns:
            rank_pos = list(plot_df_fms.index).index(optimal_rank_1se)
            lambda_pos = list(plot_df_fms.columns).index(optimal_lambda)
            ax.plot(lambda_pos + 0.5, rank_pos + 0.5, 'b*', markersize=20,
                   markeredgewidth=2, markeredgecolor='white')
    
    ax.set_title('Bootstrap Grid Search: Mean FMS across Rank × Lambda',
                fontsize=14, fontweight='bold')
    ax.set_xlabel('Lambda (Regularization)', fontsize=12)
    ax.set_ylabel('Rank', fontsize=12)
    
    plt.tight_layout()
    plt.savefig(os.path.join(fig_dir, 'bootstrap_fms_heatmap.png'),
                dpi=300, bbox_inches='tight')
    plt.show()
    plt.close()
    
    print("✓ Figure 2 saved: bootstrap_fms_heatmap.png")
    
    # ========================================
    # FIGURE 3: Rank Selection (λ=0.0) with 1SE Rule
    # ========================================
    print("\nGenerating Figure 3: Rank selection with 1SE rule...")
    
    lambda_zero = results_df[results_df['lambda'] == 0.0].copy()
    lambda_zero = lambda_zero.sort_values('rank')
    
    # Calculate 1SE threshold for rank selection
    min_sse_idx = lambda_zero['mean_sse'].idxmin()
    min_sse = lambda_zero.loc[min_sse_idx, 'mean_sse']
    min_sse_se = lambda_zero.loc[min_sse_idx, 'se_sse']
    min_sse_rank = int(lambda_zero.loc[min_sse_idx, 'rank'])
    sse_1se_threshold = min_sse + min_sse_se
    
    print(f"  Minimum SSE: {min_sse:.2e} ± {min_sse_se:.2e} (at rank={min_sse_rank})")
    print(f"  1SE Threshold: {sse_1se_threshold:.2e}")
    
    # Identify which ranks are within 1SE
    lambda_zero['within_1se'] = lambda_zero['mean_sse'] <= sse_1se_threshold
    
    # Find optimal rank by 1SE rule (smallest rank within 1SE)
    within_1se_df = lambda_zero[lambda_zero['within_1se']].sort_values('rank')
    if len(within_1se_df) > 0:
        optimal_rank_1se = int(within_1se_df.iloc[0]['rank'])
        print(f"  Optimal rank (1SE rule): {optimal_rank_1se}")
        print(f"  Ranks within 1SE: {list(within_1se_df['rank'])}")
    else:
        optimal_rank_1se = min_sse_rank
        print(f"  Optimal rank (minimum SSE): {optimal_rank_1se}")
    
    # Calculate coefficient of variation (relative variability)
    lambda_zero['cv'] = lambda_zero['std_sse'] / lambda_zero['mean_sse']
    
    fig, axes = plt.subplots(2, 1, figsize=(12, 10), height_ratios=[2, 1])
    
    # Panel A: Mean SSE with 1SE threshold
    ax1 = axes[0]
    
    # Plot all points
    ax1.errorbar(lambda_zero['rank'], lambda_zero['mean_sse'],
               yerr=lambda_zero['se_sse'],
               marker='o', markersize=10, linewidth=2.5,
               capsize=5, capthick=2,
               color='darkviolet', ecolor='plum',
               label='Mean SSE ± SE', zorder=2)
    
    # Highlight points within 1SE
    within_1se_df = lambda_zero[lambda_zero['within_1se']]
    ax1.scatter(within_1se_df['rank'], within_1se_df['mean_sse'],
               s=200, marker='o', facecolors='none', 
               edgecolors='green', linewidths=3,
               label='Within 1SE of minimum', zorder=3)
    
    # Draw 1SE threshold line
    ax1.axhline(y=sse_1se_threshold, color='orange', linestyle='--',
              linewidth=2.5, alpha=0.8,
              label=f'1SE Threshold = {sse_1se_threshold:.2e}')
    
    # Mark minimum SSE rank
    ax1.axvline(x=min_sse_rank, color='blue', linestyle=':',
              linewidth=2, alpha=0.6,
              label=f'Min SSE at rank={min_sse_rank}')
    
    # Mark optimal rank (1SE rule) - use calculated value, not loaded parameter
    ax1.axvline(x=optimal_rank_1se, color='red', linestyle='--',
              linewidth=2.5, alpha=0.8,
              label=f'Selected Rank = {optimal_rank_1se} (1SE rule)')
    
    ax1.set_xlabel('Rank', fontsize=12)
    ax1.set_ylabel('Mean SSE', fontsize=12)
    ax1.set_title('Stage 1: Rank Selection at λ=0.0 (1SE Rule)\n' +
                f'Based on {n_bootstrap} Bootstrap Iterations',
                fontsize=13, fontweight='bold')
    ax1.legend(fontsize=10, loc='upper right')
    ax1.grid(alpha=0.3)
    ax1.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))
    
    # Panel B: Coefficient of Variation
    ax2 = axes[1]
    
    # Color code by within 1SE
    colors = ['green' if w else 'teal' for w in lambda_zero['within_1se']]
    ax2.scatter(lambda_zero['rank'], lambda_zero['cv'] * 100,
               s=100, c=colors, marker='s', alpha=0.7, edgecolors='black', linewidths=1.5)
    ax2.plot(lambda_zero['rank'], lambda_zero['cv'] * 100,
            linewidth=2, color='gray', alpha=0.5, linestyle=':')
    
    ax2.axvline(x=optimal_rank_1se, color='red', linestyle='--',
              linewidth=2, alpha=0.7)
    
    # Add horizontal line for mean CV
    mean_cv = lambda_zero['cv'].mean() * 100
    ax2.axhline(y=mean_cv, color='gray', linestyle=':', 
               linewidth=1.5, alpha=0.7,
               label=f'Mean CV = {mean_cv:.2f}%')
    
    ax2.set_xlabel('Rank', fontsize=12)
    ax2.set_ylabel('Coefficient of Variation (%)', fontsize=12)
    ax2.set_title('Bootstrap Stability',
                fontsize=11, fontweight='bold')
    ax2.legend(fontsize=10, loc='upper right')
    ax2.grid(alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(fig_dir, 'bootstrap_rank_selection_1se.png'),
                dpi=300, bbox_inches='tight')
    plt.show()
    plt.close()
    
    print("✓ Figure 3 saved: bootstrap_rank_selection_1se.png")
    
    # ========================================
    # FIGURE 4: Lambda Selection at Optimal Rank
    # ========================================
    print(f"\nGenerating Figure 4: Lambda selection at rank={optimal_rank_1se}...")
    
    if optimal_rank_1se is not None:
        optimal_rank_data = results_df[results_df['rank'] == optimal_rank_1se].copy()
        optimal_rank_data = optimal_rank_data.sort_values('lambda')
        
        fig, axes = plt.subplots(2, 1, figsize=(12, 10))
        
        # SSE vs Lambda
        ax1 = axes[0]
        ax1.errorbar(optimal_rank_data['lambda'], optimal_rank_data['mean_sse'],
                    yerr=optimal_rank_data['se_sse'],
                    marker='o', markersize=10, linewidth=2.5,
                    capsize=5, capthick=2,
                    color='steelblue', ecolor='lightblue')
        
        if optimal_lambda is not None:
            ax1.axvline(x=optimal_lambda, color='red', linestyle='--',
                       linewidth=2, alpha=0.7,
                       label=f'Optimal λ = {optimal_lambda}')
        
        ax1.set_xlabel('Lambda', fontsize=12)
        ax1.set_ylabel('Mean SSE (± SE)', fontsize=12)
        ax1.set_title(f'SSE vs Lambda at Optimal Rank (R={optimal_rank_1se})',
                     fontsize=13, fontweight='bold')
        ax1.legend(fontsize=11)
        ax1.grid(alpha=0.3)
        
        # FMS vs Lambda
        ax2 = axes[1]
        fms_data = optimal_rank_data[~optimal_rank_data['mean_fms'].isna()]
        
        if len(fms_data) > 0:
            ax2.errorbar(fms_data['lambda'], fms_data['mean_fms'],
                        yerr=fms_data['se_fms'],
                        marker='o', markersize=10, linewidth=2.5,
                        capsize=5, capthick=2,
                        color='forestgreen', ecolor='lightgreen')
            
            # 1SE threshold
            max_fms = fms_data['mean_fms'].max()
            max_fms_row = fms_data[fms_data['mean_fms'] == max_fms].iloc[0]
            threshold = max_fms - max_fms_row['se_fms']
            
            ax2.axhline(y=threshold, color='orange', linestyle='--',
                       linewidth=2, alpha=0.7,
                       label=f'1SE Threshold = {threshold:.3f}')
            
            if optimal_lambda is not None:
                ax2.axvline(x=optimal_lambda, color='red', linestyle='--',
                           linewidth=2, alpha=0.7,
                           label=f'Optimal λ = {optimal_lambda}')
            
            ax2.set_xlabel('Lambda', fontsize=12)
            ax2.set_ylabel('Mean FMS (± SE)', fontsize=12)
            ax2.set_title(f'Stage 2: Lambda Selection at Optimal Rank (R={optimal_rank_1se})\n' +
                         'Criterion: Maximum λ where FMS ≥ (max_FMS - 1SE)',
                         fontsize=13, fontweight='bold')
            ax2.legend(fontsize=11)
            ax2.grid(alpha=0.3)
            ax2.set_ylim(0, 1.05)
        
        plt.tight_layout()
        plt.savefig(os.path.join(fig_dir, 'bootstrap_lambda_selection.png'),
                    dpi=300, bbox_inches='tight')
        plt.show()
        plt.close()
    
    print(f"\n✓ All figures saved to: {fig_dir}/")
```




## Close log file

```{python close-log, eval=TRUE}
from datetime import datetime

print()
print("="*60)
print(f"ANALYSIS COMPLETE")
print(f"Finished: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"All output saved to: {output_dir}")
print(f"Log file: {log_file}")
print("="*60)

# Restore stdout and close log file
if hasattr(sys.stdout, 'log'):
    sys.stdout.log.close()
    sys.stdout = sys.stdout.terminal
```



# SYSTEM INFO

```{r system-info, eval=TRUE}
sessionInfo()
```

# REFERENCES
