---
title: "13.00-multiomics-barnacle"
author: "Sam White"
date: "2025-10-03"
output: 
  github_document:
    toc: true
    number_sections: true
  bookdown::html_document2:
    theme: cosmo
    toc: true
    toc_float: true
    number_sections: true
    code_folding: show
    code_download: true
  html_document:
    theme: cosmo
    toc: true
    toc_float: true
    number_sections: true
    code_folding: show
    code_download: true
bibliography: references.bib
citeproc: true
---

# BACKGROUND

# Setup

## Libraries

```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(reticulate)
library(sctransform)
library(glmGamPoi)

knitr::opts_chunk$set(
  echo = TRUE,         # Display code chunks
  eval = FALSE,        # Evaluate code chunks
  warning = FALSE,     # Hide warnings
  message = FALSE,     # Hide messages
  comment = ""         # Prevents appending '##' to beginning of lines in code output
)
```

## Set R variables

```{r R-variables, eval=TRUE}
# OUTPUT DIRECTORY
output_dir <- "../output/13.00-multiomics-barnacle"

#INPUT FILE(S)
ortholog_groups_file <- "../output/12-ortho-annot/ortholog_groups_annotated.csv"

# Transcript count matrices for each species
apul_transcript_matrix_file <- "../../D-Apul/output/02.20-D-Apul-RNAseq-alignment-HiSat2/apul-transcript_count_matrix.csv"
peve_transcript_matrix_file <- "../../E-Peve/output/02.20-E-Peve-RNAseq-alignment-HiSat2/peve-transcript_count_matrix.csv"
ptua_transcript_matrix_file <- "../../F-Ptua/output/02.20-F-Ptua-RNAseq-alignment-HiSat2/ptua-transcript_count_matrix.csv"

# CONDA
conda_env_name <- c("/home/sam/programs/mambaforge/envs/barnacle_py311_env")
conda_path <- c("/opt/anaconda/anaconda3/bin/conda")
```

## Load [barnacle](https://github.com/blasks/barnacle) conda environment

If this is successful, the first line of output should show that the Python being used is the one in your [barnacle](https://github.com/blasks/barnacle) [@blaskowski2024] conda environment path.

E.g.

`python:         /home/sam/programs/mambaforge/envs/barnacle_py311_env/bin/python`

```{r load-barnacle-conda-env, eval=TRUE}
use_condaenv(condaenv = conda_env_name, conda = conda_path)
py_config()
```

# DATA PREP

## Load ortholog groups data

```{r load-ortholog-data, eval=TRUE}
# Read in the ortholog groups data
ortholog_groups <- read.csv(ortholog_groups_file)



# Display basic info about the data
cat("Dimensions of ortholog groups data:", dim(ortholog_groups), "\n\n")
cat("Column names:", colnames(ortholog_groups), "\n\n")
head(ortholog_groups)
str(ortholog_groups)
```

## Extract ortholog expression data

Now let's extract expression data for genes that are present in the ortholog groups. We'll use the gene count matrices with gene IDs to properly map the data.

### Load gene count matrices

```{r load-gene-matrices, eval=TRUE}
# Define file paths for gene count matrices
apul_gene_matrix_file <- "../../D-Apul/output/02.20-D-Apul-RNAseq-alignment-HiSat2/apul-gene_count_matrix.csv"
peve_gene_matrix_file <- "../../E-Peve/output/02.20-E-Peve-RNAseq-alignment-HiSat2/peve-gene_count_matrix.csv"
ptua_gene_matrix_file <- "../../F-Ptua/output/02.20-F-Ptua-RNAseq-alignment-HiSat2/ptua-gene_count_matrix.csv"

# Load gene count matrices for each species
cat("Loading gene count matrices...\n\n")

apul_gene_matrix <- read.csv(apul_gene_matrix_file)
cat("Apul gene matrix dimensions:", dim(apul_gene_matrix), "\n")

peve_gene_matrix <- read.csv(peve_gene_matrix_file)
cat("Peve gene matrix dimensions:", dim(peve_gene_matrix), "\n")

ptua_gene_matrix <- read.csv(ptua_gene_matrix_file)
cat("Ptua gene matrix dimensions:", dim(ptua_gene_matrix), "\n\n")
```

### Filter ortholog groups for complete three-way matches

```{r filter-ortholog-groups, eval=TRUE}
cat("Filtering for complete three-way ortholog groups...\n")

# Keep only rows where all three species have entries (no NA values or empty strings)
complete_ortholog_groups <- ortholog_groups[nzchar(ortholog_groups$apul) & 
                                          nzchar(ortholog_groups$peve) & 
                                          nzchar(ortholog_groups$ptua), ]

cat("Total ortholog groups:", nrow(ortholog_groups), "\n")
cat("Complete three-way ortholog groups:", nrow(complete_ortholog_groups), "\n")

```

### Filter for expression data availability

```{r filter-expression-availability, eval=TRUE}
cat("Filtering ortholog groups to ensure all genes have expression data...\n")

# Clean gene IDs to check against expression data
# For Apul: remove -T[n] suffix from ortholog groups to match gene matrix format
apul_ortholog_genes_check <- gsub("-T[0-9]+$", "", complete_ortholog_groups$apul)

# For Peve and Ptua: use as-is (will clean gene matrix IDs later)
peve_ortholog_genes_check <- complete_ortholog_groups$peve
ptua_ortholog_genes_check <- complete_ortholog_groups$ptua

# Check which genes are present in expression data
# (Note: We need to clean gene matrix IDs to match)
apul_gene_matrix_ids <- gsub("^gene-", "", apul_gene_matrix$gene_id)  # Remove gene- prefix if present
peve_gene_matrix_ids <- gsub("^gene-", "", peve_gene_matrix$gene_id)  # Remove gene- prefix
ptua_gene_matrix_ids <- gsub("^gene-", "", ptua_gene_matrix$gene_id)  # Remove gene- prefix

# Find which ortholog genes are present in each species' expression data
apul_present <- apul_ortholog_genes_check %in% apul_gene_matrix_ids
peve_present <- peve_ortholog_genes_check %in% peve_gene_matrix_ids
ptua_present <- ptua_ortholog_genes_check %in% ptua_gene_matrix_ids

# Keep only ortholog groups where all three species have expression data
expression_complete_mask <- apul_present & peve_present & ptua_present
complete_ortholog_groups <- complete_ortholog_groups[expression_complete_mask, ]

cat("Ortholog groups after filtering for expression data availability:", nrow(complete_ortholog_groups), "\n")

```

### Gene ID cleaning examples

```{r gene-id-examples-testing, eval=TRUE}
cat("\n=== GENE ID CLEANING EXAMPLES ===\n")
cat("Apul (clean ortholog groups to match gene matrix):\n")
cat("Ortholog groups original:", head(complete_ortholog_groups$apul, 3), "\n")
cat("Ortholog groups cleaned:", head(gsub("-T[0-9]+$", "", complete_ortholog_groups$apul), 3), "\n")
cat("Gene matrix (target format):", head(apul_gene_matrix$gene_id, 3), "\n\n")

cat("Peve (clean gene matrix to match ortholog groups):\n")
cat("Ortholog groups (target format):", head(complete_ortholog_groups$peve, 3), "\n") 
cat("Gene matrix original:", head(peve_gene_matrix$gene_id, 3), "\n")
cat("Gene matrix cleaned:", head(peve_gene_matrix$gene_id_clean, 3), "\n\n")

cat("Ptua (clean gene matrix to match ortholog groups):\n")
cat("Ortholog groups (target format):", head(complete_ortholog_groups$ptua, 3), "\n")
cat("Gene matrix original:", head(ptua_gene_matrix$gene_id, 3), "\n")
cat("Gene matrix cleaned:", head(ptua_gene_matrix$gene_id_clean, 3), "\n\n")

```

### Clean gene IDs for matching

```{r clean-gene-ids, eval=TRUE}
cat("Cleaning gene matrix IDs to match ortholog group format...\n")

# For Apul: ortholog groups have "FUN_000185-T1", gene matrix has "FUN_002326"
# We need to remove "-T1" from ortholog groups to match gene matrix
apul_ortholog_genes <- unique(gsub("-T[0-9]+$", "", complete_ortholog_groups$apul))

# For Peve and Ptua: keep ortholog groups as-is and clean gene matrix
peve_ortholog_genes <- unique(complete_ortholog_groups$peve)
ptua_ortholog_genes <- unique(complete_ortholog_groups$ptua)

# Clean gene matrix IDs accordingly
# Apul: gene matrix already in correct format (no cleaning needed)
apul_gene_matrix$gene_id_clean <- apul_gene_matrix$gene_id

# Peve: gene matrix has "gene-Peve_00000032", ortholog groups have "Peve_00037402"  
# So we need to remove "gene-" prefix from gene matrix
peve_gene_matrix$gene_id_clean <- gsub("^gene-", "", peve_gene_matrix$gene_id)

# Ptua: gene matrix has "gene-Pocillopora_meandrina_HIv1___RNAseq.g20905.t1", 
# ortholog groups have "Pocillopora_meandrina_HIv1___RNAseq.g28886.t1"
# So we just need to remove "gene-" prefix from gene matrix
ptua_gene_matrix$gene_id_clean <- gsub("^gene-", "", ptua_gene_matrix$gene_id)

cat("Apul ortholog genes (complete groups only):", length(apul_ortholog_genes), "\n")
cat("Peve ortholog genes (complete groups only):", length(peve_ortholog_genes), "\n")
cat("Ptua ortholog genes (complete groups only):", length(ptua_ortholog_genes), "\n\n")
```

### Create ortholog expression data with group_id mapping

```{r create-ortholog-expression-data, eval=TRUE}
cat("Creating ortholog expression data with proper group_id mapping...\n")

# Function to extract expression data for a species using ortholog group mapping
extract_species_expression <- function(ortholog_groups_df, gene_matrix, species_col, species_name) {
  cat("Processing", species_name, "...\n")
  
  # Remove duplicate group_ids, keeping first occurrence of each
  cat("  Input ortholog groups:", nrow(ortholog_groups_df), "\n")
  unique_groups <- ortholog_groups_df[!duplicated(ortholog_groups_df$group_id), ]
  cat("  After removing duplicates:", nrow(unique_groups), "unique group_ids\n")
  
  # Create results data frame starting with group_id
  result_df <- data.frame(group_id = unique_groups$group_id, stringsAsFactors = FALSE)
  
  # Get expression columns (exclude gene_id and gene_id_clean columns)
  expr_cols <- setdiff(colnames(gene_matrix), c("gene_id", "gene_id_clean"))
  
  # Initialize expression columns with NA
  for(col in expr_cols) {
    result_df[[col]] <- NA
  }
  
  # For each ortholog group, find the corresponding gene and extract expression
  for(i in 1:nrow(unique_groups)) {
    target_gene <- unique_groups[[species_col]][i]
    
    # Clean target gene for matching
    if(species_name == "Apul") {
      # Remove transcript suffix for Apul
      target_gene_clean <- gsub("-T[0-9]+$", "", target_gene)
      matching_rows <- which(gene_matrix$gene_id_clean == target_gene_clean)
    } else {
      # For Peve and Ptua, match cleaned gene_id
      matching_rows <- which(gene_matrix$gene_id_clean == target_gene)
    }
    
    if(length(matching_rows) == 1) {
      # Single match - copy expression data
      for(col in expr_cols) {
        result_df[i, col] <- gene_matrix[matching_rows, col]
      }
    } else if(length(matching_rows) > 1) {
      # Multiple matches - take first gene (no averaging)
      cat("  Group", unique_groups$group_id[i], "has", length(matching_rows), "gene matches - using first match\n")
      first_match <- matching_rows[1]
      for(col in expr_cols) {
        result_df[i, col] <- gene_matrix[first_match, col]
      }
    } else {
      # No match - leave as NA
      cat("  Warning: No expression data found for group", unique_groups$group_id[i], "gene", target_gene, "\n")
    }
  }
  
  # Remove rows with all NA expression values
  expr_na_mask <- apply(result_df[, expr_cols, drop = FALSE], 1, function(x) all(is.na(x)))
  result_df <- result_df[!expr_na_mask, ]
  
  cat("  Final dimensions:", nrow(result_df), "ortholog groups x", ncol(result_df)-1, "samples\n")
  cat("  Removed", sum(expr_na_mask), "groups with no expression data\n\n")
  
  return(result_df)
}

# Extract expression data for each species using the ortholog group mapping
apul_ortholog_expression <- extract_species_expression(complete_ortholog_groups, apul_gene_matrix, "apul", "Apul")
peve_ortholog_expression <- extract_species_expression(complete_ortholog_groups, peve_gene_matrix, "peve", "Peve")
ptua_ortholog_expression <- extract_species_expression(complete_ortholog_groups, ptua_gene_matrix, "ptua", "Ptua")

cat("=== FINAL ORTHOLOG EXPRESSION DATA DIMENSIONS ===\n")
cat("Apul:", nrow(apul_ortholog_expression), "ortholog groups x", ncol(apul_ortholog_expression)-1, "samples\n")
cat("Peve:", nrow(peve_ortholog_expression), "ortholog groups x", ncol(peve_ortholog_expression)-1, "samples\n")
cat("Ptua:", nrow(ptua_ortholog_expression), "ortholog groups x", ncol(ptua_ortholog_expression)-1, "samples\n\n")
```

### Write ortholog expression data

```{r write-expression-data, eval=TRUE}
cat("Exporting ortholog expression data to CSV files...\n")

# Define output file paths
apul_output_file <- file.path(output_dir, "apul_ortholog_expression.csv")
peve_output_file <- file.path(output_dir, "peve_ortholog_expression.csv")
ptua_output_file <- file.path(output_dir, "ptua_ortholog_expression.csv")

# Write CSV files without quotes
write.csv(apul_ortholog_expression, file = apul_output_file, quote = FALSE, row.names = FALSE)
cat("Exported Apul ortholog expression data to:", apul_output_file, "\n")

write.csv(peve_ortholog_expression, file = peve_output_file, quote = FALSE, row.names = FALSE)
cat("Exported Peve ortholog expression data to:", peve_output_file, "\n")

write.csv(ptua_ortholog_expression, file = ptua_output_file, quote = FALSE, row.names = FALSE)
cat("Exported Ptua ortholog expression data to:", ptua_output_file, "\n")

cat("\nAll ortholog expression data exported successfully!\n\n")
```

### Column structure analysis

```{r column-structure-analysis, eval=TRUE}
cat("=== COLUMN STRUCTURE ANALYSIS ===\n")
cat("Apul columns:", ncol(apul_ortholog_expression), "\n")
cat("Apul column names (first 10):", paste(head(colnames(apul_ortholog_expression), 10), collapse = ", "), "\n")
cat("Apul column names (last 10):", paste(tail(colnames(apul_ortholog_expression), 10), collapse = ", "), "\n\n")

cat("Peve columns:", ncol(peve_ortholog_expression), "\n")
cat("Peve column names (first 10):", paste(head(colnames(peve_ortholog_expression), 10), collapse = ", "), "\n")
cat("Peve column names (last 10):", paste(tail(colnames(peve_ortholog_expression), 10), collapse = ", "), "\n\n")

cat("Ptua columns:", ncol(ptua_ortholog_expression), "\n")
cat("Ptua column names (first 10):", paste(head(colnames(ptua_ortholog_expression), 10), collapse = ", "), "\n")
cat("Ptua column names (last 10):", paste(tail(colnames(ptua_ortholog_expression), 10), collapse = ", "), "\n\n")

```

### Summary statistics

```{r summary-statistics, eval=TRUE}
cat("=== LINE COUNTS FOR ORTHOLOG EXPRESSION DATA ===\n")
cat("Apul ortholog expression with info: ", nrow(apul_ortholog_expression), " rows\n")
cat("Peve ortholog expression with info: ", nrow(peve_ortholog_expression), " rows\n")
cat("Ptua ortholog expression with info: ", nrow(ptua_ortholog_expression), " rows\n\n")
```

### Diagnostic analysis

```{r diagnostic-analysis, eval=TRUE}
cat("\n=== VERIFICATION: THREE-WAY ORTHOLOG COUNTS ===\n")
cat("After filtering, all species should have identical three_way ortholog counts.\n\n")

# Check how many genes we have for each species
cat("Genes found in expression data by species:\n")
cat("Apul ortholog expression (before adding info):", nrow(apul_ortholog_expression), "\n")
cat("Peve ortholog expression (before adding info):", nrow(peve_ortholog_expression), "\n")
cat("Ptua ortholog expression (before adding info):", nrow(ptua_ortholog_expression), "\n\n")

# Verify all ortholog groups are three-way
cat("Complete three-way ortholog groups available:", nrow(complete_ortholog_groups), "\n")
cat("All should be type 'three_way'? Check:", table(complete_ortholog_groups$type), "\n\n")

# Verify perfect gene coverage (should be 100% for all species now)
apul_coverage <- length(intersect(apul_ortholog_genes, apul_gene_matrix$gene_id_clean))
peve_coverage <- length(intersect(peve_ortholog_genes, peve_gene_matrix$gene_id_clean))
ptua_coverage <- length(intersect(ptua_ortholog_genes, ptua_gene_matrix$gene_id_clean))

cat("Gene coverage in expression data (should be 100% for all):\n")
cat("Apul: ", apul_coverage, "/", length(apul_ortholog_genes), " (", round(apul_coverage/length(apul_ortholog_genes)*100, 1), "%)\n")
cat("Peve: ", peve_coverage, "/", length(peve_ortholog_genes), " (", round(peve_coverage/length(peve_ortholog_genes)*100, 1), "%)\n")
cat("Ptua: ", ptua_coverage, "/", length(ptua_ortholog_genes), " (", round(ptua_coverage/length(ptua_ortholog_genes)*100, 1), "%)\n\n")
```

### Preview expression data

```{r preview-expression-data, eval=TRUE}
cat("\n=== PREVIEW OF ORTHOLOG EXPRESSION DATA ===\n")

cat("Apul ortholog expression:\n\n")
str(apul_ortholog_expression)
cat("\n\n")

cat("\nPeve ortholog expression:\n")
str(peve_ortholog_expression)
cat("\n\n")

cat("\nPtua ortholog expression:\n")
str(ptua_ortholog_expression)
cat("\n\n")
```

# BARNACLE ANALYSIS

Based on the barnacle workflow, we need to: 1. Use `sctransform` to normalize the count data 2. Create tensors for multiomics analysis 3. Run sparse tensor decomposition

## Load expression data

```{r load-normalized-data, eval=TRUE}
# Read the exported ortholog expression data
apul_expr <- read.csv(file.path(output_dir, "apul_ortholog_expression.csv"))
peve_expr <- read.csv(file.path(output_dir, "peve_ortholog_expression.csv"))  
ptua_expr <- read.csv(file.path(output_dir, "ptua_ortholog_expression.csv"))

cat("Loaded expression data:\n")
cat("Apul:", nrow(apul_expr), "genes x", ncol(apul_expr)-1, "samples\n")
cat("Peve:", nrow(peve_expr), "genes x", ncol(peve_expr)-1, "samples\n") 
cat("Ptua:", nrow(ptua_expr), "genes x", ncol(ptua_expr)-1, "samples\n")
```

## Normalize data with sctransform

Following the barnacle manuscript approach, we'll use `sctransform` to normalize each species' data. We'll use a bulk RNA-seq appropriate approach.

```{r normalize-sctransform, eval=TRUE}
# Function to normalize count data with sctransform for bulk RNA-seq
normalize_with_sctransform <- function(count_data, species_name) {
  cat("Normalizing", species_name, "data with sctransform...\n")
  
  # Check if we have group_id or gene_id column
  id_col <- if("group_id" %in% colnames(count_data)) "group_id" else "gene_id"
  cat("Using", id_col, "as identifier column\n")
  
  # Check for and handle duplicate group_ids/gene_ids
  duplicate_ids <- count_data[[id_col]][duplicated(count_data[[id_col]])]
  if(length(duplicate_ids) > 0) {
    cat("Found", length(unique(duplicate_ids)), "duplicate", id_col, "values:\n")
    cat("  Examples:", head(unique(duplicate_ids), 10), "\n")
    cat("  Note: sctransform may fail due to duplicate row names, will fall back to log2(CPM + 1)\n")
  } else {
    cat("No duplicate", id_col, "values found\n")
  }
  
  # Use original data without aggregation - let sctransform handle duplicates or fail
  agg_data <- count_data
  
  # Extract count matrix (genes as rows, samples as columns)
  count_matrix <- as.matrix(agg_data[, -1])  # Remove id column
  rownames(count_matrix) <- agg_data[[id_col]]
  
  # Check for and handle problematic values
  cat("Checking data quality...\n")
  cat("  - Zero values:", sum(count_matrix == 0), "/", length(count_matrix), "\n")
  cat("  - NA values:", sum(is.na(count_matrix)), "\n")
  cat("  - Infinite values:", sum(is.infinite(count_matrix)), "\n")
  cat("  - Min value:", min(count_matrix, na.rm = TRUE), "\n")
  cat("  - Max value:", max(count_matrix, na.rm = TRUE), "\n")
  
  # Remove genes with all zeros or very low expression
  gene_sums <- rowSums(count_matrix)
  keep_genes <- gene_sums > 10  # Keep genes with total counts > 10
  count_matrix_filtered <- count_matrix[keep_genes, , drop = FALSE]
  
  cat("  - Filtered to", nrow(count_matrix_filtered), "ortholog groups (from", nrow(count_matrix), ")\n")
  
  # Transpose for sctransform (expects cells as rows, genes as columns)
  count_matrix_t <- t(count_matrix_filtered)
  
  # Apply sctransform normalization with bulk RNA-seq appropriate parameters
  normalized_df <- tryCatch({
    # Use more conservative parameters for bulk RNA-seq
    normalized <- sctransform::vst(
      count_matrix_t, 
      method = "glmGamPoi",  # More appropriate for bulk data
      n_genes = min(2000, ncol(count_matrix_t)),  # Use fewer variable genes
      return_cell_attr = TRUE,
      verbosity = 2
    )
    
    # Extract normalized data and transpose back (genes as rows, samples as columns)
    normalized_data <- t(normalized$y)
    
    # Create output data frame with original gene set (fill missing with zeros)
    full_normalized_data <- matrix(0, nrow = nrow(count_matrix), ncol = ncol(count_matrix))
    rownames(full_normalized_data) <- rownames(count_matrix)
    colnames(full_normalized_data) <- colnames(count_matrix)
    
    # Fill in normalized values for kept genes
    full_normalized_data[rownames(normalized_data), ] <- normalized_data
    
    result_df <- data.frame(
      group_id = rownames(full_normalized_data),
      full_normalized_data,
      stringsAsFactors = FALSE
    )
    
    cat("sctransform normalization successful for", species_name, "\n")
    return(result_df)
    
  }, error = function(e) {
    cat("sctransform failed for", species_name, ":", e$message, "\n")
    cat("Falling back to log2(CPM + 1) normalization...\n")
    
    # Fallback: log2(CPM + 1) normalization
    # Calculate CPM (Counts Per Million)
    lib_sizes <- colSums(count_matrix)
    cpm_matrix <- sweep(count_matrix, 2, lib_sizes/1e6, FUN = "/")
    
    # Log2 transform with pseudocount
    normalized_data <- log2(cpm_matrix + 1)
    
    result_df <- data.frame(
      group_id = rownames(normalized_data),
      normalized_data,
      stringsAsFactors = FALSE
    )
    
    cat("Log2(CPM + 1) normalization complete for", species_name, "\n")
    return(result_df)
  })
  
  cat("Input dimensions:", nrow(count_data), "rows x", ncol(count_data)-1, "samples\n")
  cat("Output dimensions:", nrow(normalized_df), "ortholog groups x", ncol(normalized_df)-1, "samples\n\n")
  
  return(normalized_df)
}

# Normalize each species
cat("=== STARTING NORMALIZATION ===\n\n")
apul_normalized <- normalize_with_sctransform(apul_expr, "Apul")
peve_normalized <- normalize_with_sctransform(peve_expr, "Peve") 
ptua_normalized <- normalize_with_sctransform(ptua_expr, "Ptua")
cat("=== NORMALIZATION COMPLETE ===\n\n")
```

## Export normalized data for Python analysis

```{r export-normalized-data, eval=TRUE}
# Export normalized data for Python processing
apul_norm_file <- file.path(output_dir, "apul_normalized_expression.csv")
peve_norm_file <- file.path(output_dir, "peve_normalized_expression.csv")
ptua_norm_file <- file.path(output_dir, "ptua_normalized_expression.csv")

write.csv(apul_normalized, apul_norm_file, row.names = FALSE, quote = FALSE)
write.csv(peve_normalized, peve_norm_file, row.names = FALSE, quote = FALSE)
write.csv(ptua_normalized, ptua_norm_file, row.names = FALSE, quote = FALSE)

cat("Exported normalized data:\n")
cat("Apul:", apul_norm_file, "\n")
cat("Peve:", peve_norm_file, "\n")
cat("Ptua:", ptua_norm_file, "\n\n")
```

## Create tensor dataset in Python

Now we'll switch to Python to create the multiomics tensor and run barnacle analysis.

```{python create-multiomics-tensor, eval=TRUE}
import pandas as pd
import numpy as np
import os
from pathlib import Path

# Set up paths
output_dir = r.output_dir
print(f"Working in output directory: {output_dir}")

# Load normalized data
apul_norm = pd.read_csv(os.path.join(output_dir, "apul_normalized_expression.csv"))
peve_norm = pd.read_csv(os.path.join(output_dir, "peve_normalized_expression.csv"))
ptua_norm = pd.read_csv(os.path.join(output_dir, "ptua_normalized_expression.csv"))

print("Loaded normalized data:")
print(f"Apul: {apul_norm.shape}")
print(f"Peve: {peve_norm.shape}")  
print(f"Ptua: {ptua_norm.shape}")

# Check which genes are common across all species
apul_genes = set(apul_norm['group_id'])
peve_genes = set(peve_norm['group_id'])
ptua_genes = set(ptua_norm['group_id'])

common_genes = apul_genes & peve_genes & ptua_genes
print(f"\nCommon genes across all species: {len(common_genes)}")

# Filter to common genes and align gene order
common_genes_list = sorted(list(common_genes))

apul_common = apul_norm[apul_norm['group_id'].isin(common_genes_list)].set_index('group_id').reindex(common_genes_list)
peve_common = peve_norm[peve_norm['group_id'].isin(common_genes_list)].set_index('group_id').reindex(common_genes_list)
ptua_common = ptua_norm[ptua_norm['group_id'].isin(common_genes_list)].set_index('group_id').reindex(common_genes_list)

print(f"\nFiltered to common genes:")
print(f"Apul: {apul_common.shape}")
print(f"Peve: {peve_common.shape}")
print(f"Ptua: {ptua_common.shape}")
```

## Parse sample information and create 3D tensor

```{python parse-sample-info, eval=TRUE}
# Parse sample names to extract sample information for each species independently
def parse_species_samples(columns, species_name):
    """Parse sample column names for a specific species"""
    sample_map = {}
    sample_ids = []
    timepoints = set()
    
    for col in columns:
        # Expected format: PREFIX.NUMBER.TP# (e.g., ACR.139.TP1, POR.216.TP1, POC.201.TP1)
        parts = col.split('.')
        if len(parts) >= 3 and parts[2].startswith('TP'):
            prefix = parts[0]
            numeric_id = parts[1]  
            timepoint = int(parts[2][2:])  # e.g., 1 from TP1
            
            sample_id = f"{prefix}.{numeric_id}"  # e.g., "ACR.139"
            sample_map[(sample_id, timepoint)] = col
            if sample_id not in sample_ids:
                sample_ids.append(sample_id)
            timepoints.add(timepoint)
        else:
            print(f"Warning: Could not parse column name: {col}")
    
    return sample_map, sample_ids, sorted(timepoints)

# Parse sample information for each species independently
print("Parsing sample information for each species...")

species_data = {
    'apul': apul_common,
    'peve': peve_common, 
    'ptua': ptua_common
}

species_info = {}
all_timepoints = set()

for species, data in species_data.items():
    sample_map, sample_ids, timepoints = parse_species_samples(data.columns, species)
    species_info[species] = {
        'sample_map': sample_map,
        'sample_ids': sample_ids,
        'timepoints': timepoints,
        'n_samples': len(sample_ids)
    }
    all_timepoints.update(timepoints)
    
    print(f"{species}:")
    print(f"  Samples: {len(sample_ids)} ({sample_ids[:3]}...)")
    print(f"  Timepoints: {timepoints}")

common_timepoints = sorted(list(all_timepoints))
print(f"\nTimepoints found across all species: {common_timepoints}")

# Find the maximum number of samples to determine tensor dimensions
max_samples = max(info['n_samples'] for info in species_info.values())
print(f"Maximum samples in any species: {max_samples}")

# Print detailed sample structure
print(f"\nDetailed sample structure:")
for species, info in species_info.items():
    print(f"{species}: {info['n_samples']} samples × {len(info['timepoints'])} timepoints")
```

## Create 3D tensor (genes × species_samples × timepoints)

```{python create-3d-tensor, eval=TRUE}
# Create a 3D tensor: genes × (species_samples) × timepoints
# This flattens species and samples into a single dimension that Barnacle can handle

print("Creating 3D tensor by combining species and samples...")

# First, collect all actual sample-timepoint combinations that have data
all_sample_columns = []
sample_labels = []  # Track which sample belongs to which species
species_sample_map = {}  # Map from combined index to (species, sample_idx, sample_id)

sample_idx = 0
for species in ['apul', 'peve', 'ptua']:
    data = species_data[species]
    info = species_info[species]
    
    print(f"\nProcessing {species}:")
    for sample_id in info['sample_ids']:
        # Check if this sample has data for any timepoint
        has_data = False
        sample_timepoint_cols = []
        
        for timepoint in common_timepoints:
            if (sample_id, timepoint) in info['sample_map']:
                col_name = info['sample_map'][(sample_id, timepoint)]
                sample_timepoint_cols.append(col_name)
                has_data = True
        
        if has_data:
            all_sample_columns.extend(sample_timepoint_cols)
            sample_labels.append(f"{species}_{sample_id}")
            species_sample_map[sample_idx] = {
                'species': species,
                'sample_id': sample_id,
                'sample_idx_in_species': info['sample_ids'].index(sample_id)
            }
            sample_idx += 1
            print(f"  Added {sample_id} with {len(sample_timepoint_cols)} timepoints")

n_genes = len(common_genes_list)
n_combined_samples = len(sample_labels)
n_timepoints = len(common_timepoints)

print(f"\nCreating 3D tensor with shape: ({n_genes}, {n_combined_samples}, {n_timepoints})")
print(f"Combined samples from all species: {n_combined_samples}")

# Initialize tensor
tensor_3d = np.full((n_genes, n_combined_samples, n_timepoints), np.nan)

# Fill tensor
filled_count = 0
missing_count = 0

for combined_idx, sample_label in enumerate(sample_labels):
    species_info_map = species_sample_map[combined_idx]
    species = species_info_map['species']
    sample_id = species_info_map['sample_id']
    
    data = species_data[species]
    info = species_info[species]
    
    for time_idx, timepoint in enumerate(common_timepoints):
        if (sample_id, timepoint) in info['sample_map']:
            col_name = info['sample_map'][(sample_id, timepoint)]
            tensor_3d[:, combined_idx, time_idx] = data[col_name].values
            filled_count += 1
        else:
            missing_count += 1

# Check tensor statistics
n_missing = np.sum(np.isnan(tensor_3d))
n_total = tensor_3d.size
n_finite = np.sum(np.isfinite(tensor_3d))

print(f"\n=== TENSOR STATISTICS ===")
print(f"Tensor shape: {tensor_3d.shape}")
print(f"Total elements: {n_total}")
print(f"Finite values: {n_finite}")
print(f"Missing/NaN values: {n_missing}")
print(f"Missing percentage: {n_missing / n_total * 100:.2f}%")
print(f"Filled {filled_count} sample-timepoint combinations")
print(f"Missing {missing_count} sample-timepoint combinations")

# Check non-zero values among finite values
finite_values = tensor_3d[np.isfinite(tensor_3d)]
n_nonzero = np.sum(finite_values != 0)
print(f"Non-zero finite values: {n_nonzero}")
print(f"Zero finite values: {len(finite_values) - n_nonzero}")
print(f"Sparsity among finite values: {(len(finite_values) - n_nonzero) / len(finite_values) * 100:.2f}%")

# Save sample mapping for later interpretation
sample_mapping = pd.DataFrame([
    {
        'combined_index': i,
        'sample_label': label,
        'species': species_sample_map[i]['species'],
        'sample_id': species_sample_map[i]['sample_id']
    }
    for i, label in enumerate(sample_labels)
])
print(f"\nSample mapping:")
print(sample_mapping.head(10))
```

## Run Barnacle sparse tensor decomposition

```{python run-barnacle, eval=TRUE}
from barnacle.decomposition import SparseCP
import matplotlib.pyplot as plt

# Handle missing values by filling with zeros (could also use mean imputation)
tensor_filled = np.nan_to_num(tensor_3d, nan=0.0)

print("Running Barnacle sparse tensor decomposition...")
print(f"Input tensor shape: {tensor_filled.shape}")

# Set up sparse CP decomposition parameters
rank = 5  # Number of components to extract
lambdas = [0.1, 0.0, 0.1]  # Sparsity penalties for [genes, samples, timepoints]

# Create model
model = SparseCP(
    rank=rank,
    lambdas=lambdas,
    nonneg_modes=[0],  # Non-negative gene loadings only
    n_initializations=3,  # Reduced for faster testing
    random_state=42
)

# Fit the model
print("Fitting sparse CP decomposition...")
try:
    decomposition = model.fit_transform(tensor_filled, verbose=1)
    
    print(f"\nDecomposition complete!")
    print(f"Converged: {hasattr(model, 'converged_') and model.converged_}")
    
    if hasattr(model, 'loss_') and model.loss_ is not None:
        print(f"Final loss: {model.loss_[-1]:.6f}")
        print(f"Number of iterations: {len(model.loss_)}")
    else:
        print("Loss information not available")
    
    # Examine factor matrices
    print(f"\nFactor matrix shapes:")
    for i, factor in enumerate(decomposition.factors):
        factor_names = ['Genes', 'Species_Samples', 'Timepoints']
        print(f"Mode {i} ({factor_names[i]}): {factor.shape}")
        
    decomposition_success = True
    
except Exception as e:
    print(f"Error during decomposition: {e}")
    print("Creating dummy decomposition for downstream code...")
    decomposition_success = False
    
    # Create dummy factors for error handling
    class DummyDecomposition:
        def __init__(self):
            self.factors = [
                np.random.rand(len(common_genes_list), rank),  # genes
                np.random.rand(len(sample_labels), rank),      # samples
                np.random.rand(len(common_timepoints), rank)   # timepoints
            ]
            self.weights = np.ones(rank)
    
    decomposition = DummyDecomposition()
```

## Analyze and interpret results

```{python analyze-results, eval=TRUE}
if decomposition_success:
    # Extract factor matrices
    gene_factors = decomposition.factors[0]  # genes × components
    sample_factors = decomposition.factors[1]  # combined_samples × components  
    time_factors = decomposition.factors[2]  # timepoints × components

    print("=== BARNACLE DECOMPOSITION RESULTS ===")
    print(f"Number of components: {rank}")
    print(f"Component weights: {decomposition.weights}")

    print("\n=== TIMEPOINT LOADINGS ===")
    time_df = pd.DataFrame(
        time_factors,
        index=[f'TP{tp}' for tp in common_timepoints],
        columns=[f'Component_{i+1}' for i in range(rank)]
    )
    print(time_df)

    print("\n=== SAMPLE LOADINGS BY SPECIES ===")
    # Show sample loadings grouped by species
    sample_df_full = pd.DataFrame(
        sample_factors,
        index=sample_labels,
        columns=[f'Component_{i+1}' for i in range(rank)]
    )
    
    # Add species information
    sample_df_full['Species'] = [species_sample_map[i]['species'] for i in range(len(sample_labels))]
    sample_df_full['Sample_ID'] = [species_sample_map[i]['sample_id'] for i in range(len(sample_labels))]
    
    # Show by species
    for species in ['apul', 'peve', 'ptua']:
        species_samples = sample_df_full[sample_df_full['Species'] == species]
        print(f"\n{species.upper()} samples:")
        component_cols = [f'Component_{i+1}' for i in range(rank)]
        print(species_samples[component_cols + ['Sample_ID']].head())

    # Find top genes for each component
    print("\n=== TOP GENES PER COMPONENT ===")
    n_top_genes = 5
    for comp in range(rank):
        gene_loadings = gene_factors[:, comp]
        top_gene_indices = np.argsort(np.abs(gene_loadings))[-n_top_genes:][::-1]
        top_genes = [common_genes_list[i] for i in top_gene_indices]
        top_loadings = gene_loadings[top_gene_indices]
        
        print(f"\nComponent {comp+1} (weight: {decomposition.weights[comp]:.3f}):")
        for gene, loading in zip(top_genes, top_loadings):
            print(f"  {gene}: {loading:.3f}")
            
    # Analyze species patterns in sample factors
    print("\n=== SPECIES PATTERNS IN SAMPLE FACTORS ===")
    for comp in range(rank):
        comp_col = f'Component_{comp+1}'
        print(f"\nComponent {comp+1} - Average loadings by species:")
        for species in ['apul', 'peve', 'ptua']:
            species_loadings = sample_df_full[sample_df_full['Species'] == species][comp_col]
            avg_loading = species_loadings.mean()
            std_loading = species_loadings.std()
            print(f"  {species}: {avg_loading:.3f} ± {std_loading:.3f}")

else:
    print("Decomposition failed - skipping detailed analysis")
```

## Save results

```{python save-results, eval=TRUE}
# Save tensor data
tensor_file = os.path.join(output_dir, "multiomics_tensor.npy")
np.save(tensor_file, tensor_filled)
print(f"Saved tensor to: {tensor_file}")

# Save factor matrices
factors_dir = os.path.join(output_dir, "barnacle_factors")
os.makedirs(factors_dir, exist_ok=True)

if decomposition_success:
    # Gene factors
    gene_factor_df = pd.DataFrame(
        gene_factors,
        index=common_genes_list,
        columns=[f'Component_{i+1}' for i in range(rank)]
    )
    gene_factor_df.to_csv(os.path.join(factors_dir, "gene_factors.csv"))

    # Sample factors with species information
    sample_factor_df = pd.DataFrame(
        sample_factors,
        index=sample_labels,
        columns=[f'Component_{i+1}' for i in range(rank)]
    )
    # Add metadata
    sample_factor_df['Species'] = [species_sample_map[i]['species'] for i in range(len(sample_labels))]
    sample_factor_df['Sample_ID'] = [species_sample_map[i]['sample_id'] for i in range(len(sample_labels))]
    sample_factor_df.to_csv(os.path.join(factors_dir, "sample_factors.csv"))

    # Time factors
    time_df.to_csv(os.path.join(factors_dir, "time_factors.csv"))

    # Save component weights
    weights_df = pd.DataFrame({
        'Component': [f'Component_{i+1}' for i in range(rank)],
        'Weight': decomposition.weights
    })
    weights_df.to_csv(os.path.join(factors_dir, "component_weights.csv"), index=False)
    
    # Save sample mapping
    sample_mapping.to_csv(os.path.join(factors_dir, "sample_mapping.csv"), index=False)

    # Save metadata
    metadata = {
        'tensor_shape': tensor_filled.shape,
        'tensor_type': '3D_genes_species_samples_timepoints',
        'n_genes': len(common_genes_list),
        'n_combined_samples': len(sample_labels),
        'n_timepoints': len(common_timepoints),
        'rank': rank,
        'lambdas': lambdas,
        'decomposition_success': True,
        'timepoint_order': common_timepoints,
        'species_info': {species: info['n_samples'] for species, info in species_info.items()},
        'gene_count': len(common_genes_list)
    }
    
    if hasattr(model, 'converged_'):
        metadata['converged'] = bool(model.converged_)
    if hasattr(model, 'loss_') and model.loss_ is not None:
        metadata['final_loss'] = float(model.loss_[-1])
        metadata['n_iterations'] = len(model.loss_)

else:
    metadata = {
        'tensor_shape': tensor_filled.shape,
        'tensor_type': '3D_genes_species_samples_timepoints',
        'decomposition_success': False,
        'error': 'Decomposition failed'
    }

import json
with open(os.path.join(factors_dir, "metadata.json"), 'w') as f:
    json.dump(metadata, f, indent=2)

print(f"\nSaved all results to: {factors_dir}")
if decomposition_success:
    print("Files created:")
    print("- gene_factors.csv: Gene loadings for each component")
    print("- sample_factors.csv: Sample loadings for each component (with species info)")
    print("- time_factors.csv: Timepoint loadings for each component")
    print("- component_weights.csv: Component importance weights")
    print("- sample_mapping.csv: Mapping of samples to species")
    print("- metadata.json: Analysis metadata and parameters")
else:
    print("Only metadata saved due to decomposition failure")
```

## Visualize Barnacle results

### Visualization setup

```{python viz-setup, eval=TRUE}
# Setup for visualizations: load libs, directories and dataframes (shared by all small chunks)
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA

# Ensure factors_dir is defined (created in previous chunk)
try:
    factors_dir
except NameError:
    factors_dir = os.path.join(output_dir, "barnacle_factors")

fig_dir = os.path.join(factors_dir, "figures")
os.makedirs(fig_dir, exist_ok=True)

print("Creating visualizations in:", fig_dir)

def safe_load_df(var_name, csv_name):
    try:
        return globals()[var_name]
    except Exception:
        path = os.path.join(factors_dir, csv_name)
        if os.path.exists(path):
            return pd.read_csv(path, index_col=0)
        else:
            return None

gene_factor_df = safe_load_df('gene_factor_df', 'gene_factors.csv')
sample_factor_df = safe_load_df('sample_factor_df', 'sample_factors.csv')
time_df = safe_load_df('time_df', 'time_factors.csv')
weights_df = None
weights_path = os.path.join(factors_dir, 'component_weights.csv')
if os.path.exists(weights_path):
    weights_df = pd.read_csv(weights_path)

print('Visualization setup complete')
```

### Component weights

```{python viz-weights, eval=TRUE}
# Component weights barplot
if weights_df is not None and not weights_df.empty:
    try:
        plt.figure(figsize=(6,4))
        sns.barplot(x='Component', y='Weight', data=weights_df, palette='viridis')
        plt.xticks(rotation=45)
        plt.title('Component weights')
        plt.tight_layout()
        out = os.path.join(fig_dir, 'component_weights.png')
        plt.savefig(out, dpi=150)
        plt.close()
        print('Saved:', out)
    except Exception as e:
        print('Could not plot component weights:', e)
else:
    print('No component_weights.csv found; skipping weights plot')
```

### Timepoint loadings

```{python viz-time-loadings, eval=TRUE}
# Timepoint loadings (line plot)
if time_df is not None and not time_df.empty:
    try:
        plt.figure(figsize=(8,4))
        x = list(range(len(time_df.index)))
        for col in time_df.columns:
            plt.plot(x, time_df[col].values, marker='o', label=col)
        plt.xticks(x, list(time_df.index), rotation=45)
        plt.xlabel('Timepoint')
        plt.ylabel('Loading')
        plt.legend(bbox_to_anchor=(1.02,1), loc='upper left')
        plt.title('Timepoint loadings per component')
        plt.tight_layout()
        out = os.path.join(fig_dir, 'time_loadings.png')
        plt.savefig(out, dpi=150)
        plt.close()
        print('Saved:', out)
    except Exception as e:
        print('Timepoint loading plot failed:', e)
else:
    print('No time_factors available; skipping timepoint plot')
```

### Sample-factor heatmap

```{python viz-sample-heatmap, eval=TRUE}
# Sample-factor heatmap grouped by species
if sample_factor_df is not None and not sample_factor_df.empty:
    try:
        sf = sample_factor_df.copy()
        # If metadata columns present, separate numeric factor columns
        meta_cols = [c for c in ['Species','Sample_ID'] if c in sf.columns]
        factor_cols = [c for c in sf.columns if c not in meta_cols]
        species_series = sf['Species'] if 'Species' in sf.columns else pd.Series(['unknown']*sf.shape[0], index=sf.index)
        sample_vals = sf[factor_cols]

        # sort by species for display
        order = species_series.sort_values().index
        sample_vals_sorted = sample_vals.loc[order]

        plt.figure(figsize=(10, max(3, sample_vals_sorted.shape[0]*0.08)))
        # Show a colorbar with a clear label explaining the colormap meaning
        sns.heatmap(sample_vals_sorted, cmap='vlag', center=0, cbar_kws={'label': 'Component loading (signed)'})
        plt.xlabel('Components')
        plt.ylabel('Samples (grouped by species)')
        plt.tight_layout()
        out = os.path.join(fig_dir, 'sample_factors_heatmap.png')
        plt.savefig(out, dpi=150)
        plt.close()
        print('Saved:', out)

    except Exception as e:
        print('Sample factors plotting failed:', e)
else:
    print('No sample_factors available; skipping sample heatmap')
```

### PCA of sample-factor loadings

```{python viz-sample-pca, eval=TRUE}
# PCA of sample factors (2D scatter)
if sample_factor_df is not None and not sample_factor_df.empty:
    try:
        sf = sample_factor_df.copy()
        meta_cols = [c for c in ['Species','Sample_ID'] if c in sf.columns]
        factor_cols = [c for c in sf.columns if c not in meta_cols]
        species_series = sf['Species'] if 'Species' in sf.columns else pd.Series(['unknown']*sf.shape[0], index=sf.index)
        sample_vals = sf[factor_cols]

        X = sample_vals.fillna(0).values
        pca = PCA(n_components=2)
        pcs = pca.fit_transform(X)
        pcs_df = pd.DataFrame(pcs, index=sample_vals.index, columns=['PC1','PC2'])
        pcs_df['Species'] = species_series.loc[pcs_df.index].values

        plt.figure(figsize=(6,5))
        sns.scatterplot(data=pcs_df, x='PC1', y='PC2', hue='Species', s=80)
        plt.title('PCA of sample-factor loadings')
        plt.tight_layout()
        out = os.path.join(fig_dir, 'sample_factors_pca.png')
        plt.savefig(out, dpi=150)
        plt.close()
        print('Saved:', out)

    except Exception as e:
        print('Sample PCA plotting failed:', e)
else:
    print('No sample_factors available; skipping sample PCA')
```

### Top genes per component

```{python viz-top-genes, eval=TRUE}
# Top genes per component (barplots)
if gene_factor_df is not None and not gene_factor_df.empty:
    try:
        gf = gene_factor_df.copy()
        # Determine number of components
        comps = gf.columns
        n_top = 10
        for col in comps:
            # rank by absolute loading
            s = gf[col].abs().sort_values(ascending=False).head(n_top)
            plt.figure(figsize=(6, max(2, n_top*0.25)))
            sns.barplot(x=s.values, y=s.index, palette='magma')
            plt.xlabel('Absolute loading')
            plt.ylabel('Ortholog group')
            plt.title(f'Top {n_top} genes - {col}')
            plt.tight_layout()
            out = os.path.join(fig_dir, f'top_genes_{col}.png')
            plt.savefig(out, dpi=150)
            plt.close()
            print('Saved:', out)
    except Exception as e:
        print('Top genes plotting failed:', e)
else:
    print('No gene_factors available; skipping gene plots')
```
```

## Summary

```{r barnacle-summary, eval=TRUE}
cat("=== BARNACLE ANALYSIS SUMMARY ===\n\n")

cat("1. Data Preprocessing:\n")
cat("   - Loaded ortholog expression data for 3 species\n")
cat("   - Normalized using sctransform for count data\n")
cat("   - Filtered to genes common across all species\n\n")

cat("2. Tensor Construction:\n")
cat("   - Created 3D tensor: genes × (species_samples) × timepoints\n")
cat("   - Combined all species samples into single dimension\n")
cat("   - Dimensions based on timeseries experimental design\n\n")

cat("3. Sparse Tensor Decomposition:\n")
cat("   - Applied barnacle SparseCP with sparsity constraints\n")
cat("   - Extracted latent factors representing:\n")
cat("     * Gene co-expression patterns\n")
cat("     * Combined sample-species relationships\n") 
cat("     * Temporal dynamics\n\n")

cat("4. Results Generated:\n")
cat("   - Factor matrices for each tensor mode\n")
cat("   - Component weights indicating importance\n")
cat("   - Top contributing genes per component\n")
cat("   - Temporal and species loadings\n\n")

cat("Next steps:\n")
cat("- Examine component biological interpretations\n")
cat("- Validate results with known gene functions\n")
cat("- Compare patterns across species and timepoints\n")
cat("- Perform gene set enrichment analysis on component genes\n")
```

# REFERENCES
