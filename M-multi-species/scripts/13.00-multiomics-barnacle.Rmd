---
title: "13.00-multiomics-barnacle"
author: "Sam White"
date: "2025-10-03"
output: 
  github_document:
    toc: true
    number_sections: true
  bookdown::html_document2:
    theme: cosmo
    toc: true
    toc_float: true
    number_sections: true
    code_folding: show
    code_download: true
  html_document:
    theme: cosmo
    toc: true
    toc_float: true
    number_sections: true
    code_folding: show
    code_download: true
bibliography: references.bib
citeproc: true
---

# BACKGROUND

This analysis prepares three-species ortholog expression matrices, normalizes the data, constructs a multi‑omics tensor (genes × combined species-samples × timepoints), and runs a sparse tensor decomposition using the [barnacle](https://github.com/blasks/barnacle) [@blaskowski2024] workflow to discover shared gene/time/species patterns.

## Workflow overview

1. **Data Preparation** - Extract and normalize ortholog expression data
2. **Tensor Construction** - Build 3D tensor combining species, samples, and timepoints
3. **Rank Comparison** - Test multiple ranks systematically to find optimal components
4. **Interpretation** - Choose best rank and examine factors, visualizations, and biological meaning

## Rank selection approach

**This analysis uses a systematic rank comparison approach.**

The workflow:
1. Configure `ranks_to_test` in the comparison chunk (e.g., `[5, 8, 10, 12, 15, 20]`)
2. Run decompositions for all specified ranks (10-30 minutes)
3. Evaluate synthetic factor match scores (FMS) across multiple random initializations
4. Review comparison metrics and elbow plots:
   - **Synthetic FMS elbow plots** - Shows how well factors can be recovered (MOST IMPORTANT)
   - Variance explained plots - Shows reconstruction accuracy
   - Look for the "elbow" where marginal gains diminish
5. Choose optimal rank based on:
   - Synthetic FMS elbow (where curve flattens)
   - Consistency across random initializations
   - Variance explained (aim for >60%)
   - Sparsity patterns
   - Component interpretability
6. Use chosen rank for final biological interpretation

**Key metrics for comparison:**
- **Synthetic FMS**: How well factors can be recovered from synthetic data (higher = better identifiability)
- **Variance explained**: How much of data variation is captured
- **Relative error**: Reconstruction accuracy
- **Sparsity**: How many genes/samples load strongly per component
- **Convergence**: Whether optimization completed successfully
- **Component weights**: Distribution of importance across components

## Key steps performed by this script:

- Load annotated ortholog groups and per-species count matrices.
- Filter ortholog groups to retain complete three‑way matches with available expression data.
- Extract per‑species expression matrices mapped to ortholog group IDs.
- Normalize counts (preferred: `sctransform`; fallback: log2(CPM + 1)).
- Build a 3D tensor combining species/sample and timepoint dimensions.
- Run [barnacle](https://github.com/blasks/barnacle) [@blaskowski2024] sparse CP decomposition and save factor matrices, metadata and figures.
- Assess rank appropriateness through multiple metrics and visualizations.

## Input files

- `../output/12-ortho-annot/ortholog_groups_annotated.csv` : annotated ortholog groups with columns for `group_id`, `apul`, `peve`, `ptua`, `type`, etc.
- `../../D-Apul/output/02.20-D-Apul-RNAseq-alignment-HiSat2/apul-gene_count_matrix.csv` : Apul gene count matrix (contains `gene_id` plus sample columns).
- `../../E-Peve/output/02.20-E-Peve-RNAseq-alignment-HiSat2/peve-gene_count_matrix.csv` : Peve gene count matrix.
- `../../F-Ptua/output/02.20-F-Ptua-RNAseq-alignment-HiSat2/ptua-gene_count_matrix.csv` : Ptua gene count matrix.
- (Also referenced) transcript-level matrices: `apul-transcript_count_matrix.csv`, `peve-transcript_count_matrix.csv`, `ptua-transcript_count_matrix.csv` when needed.

## Output files

- `apul_ortholog_expression.csv`, `peve_ortholog_expression.csv`, `ptua_ortholog_expression.csv` : per‑species expression matrices aligned to ortholog `group_id`.
- `apul_normalized_expression.csv`, `peve_normalized_expression.csv`, `ptua_normalized_expression.csv` : normalized expression matrices (sctransform or log2(CPM+1)).
- `multiomics_tensor.npy` : saved NumPy array of the 3D tensor used for decomposition (genes × combined_samples × timepoints).
- `barnacle_factors/` directory containing:
    - `gene_factors.csv` : gene loadings per component (genes × components).
    - `sample_factors.csv` : combined sample (species_sample) loadings per component with `Species` and `Sample_ID` metadata.
    - `time_factors.csv` : timepoint loadings per component.
    - `component_weights.csv` : component weights / importance.
    - `sample_mapping.csv` : mapping of combined sample indices to species and sample IDs.
    - `metadata.json` : analysis parameters and tensor metadata (shape, rank, lambdas, convergence, etc.).
    - `figures/` : generated visualizations (component weights, time loadings, sample heatmap, PCA, top ortholog plots).

## Notes / assumptions:

- Sample column names are parsed expecting a dot-separated format with a `TP#` timepoint token (e.g., `ACR.139.TP1`).
- Apul ortholog IDs in the ortholog table include transcript suffixes (e.g., `-T1`) which are removed for matching to the gene count matrix.
- Missing values in the tensor are handled by substitution (current workflow fills NaNs with zeros before decomposition).
- Samples must have all four timepoints (TP1, TP2, TP3, TP4) to be included in the analysis.


# SETUP

## Libraries

```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(reticulate)
library(matrixStats)  # Required for sctransform internal functions
library(sctransform)
library(glmGamPoi)

knitr::opts_chunk$set(
  echo = TRUE,         # Display code chunks
    eval = FALSE,        # Evaluate code chunks
    results = "hold",   # Hold outputs and show them after the full code chunk
  warning = FALSE,     # Hide warnings
        collapse = FALSE,    # Keep code and output in separate blocks
        warning = FALSE,     # Hide warnings
        message = FALSE,     # Hide messages
        comment = "##"      # Prefix output lines with '##' so output is visually distinct
)
```

## Set R variables

```{r R-variables, eval=TRUE}
# OUTPUT DIRECTORY
output_dir <- "../output/13.00-multiomics-barnacle"

#INPUT FILE(S)
ortholog_groups_file <- "../output/12-ortho-annot/ortholog_groups_annotated.csv"

# Transcript count matrices for each species
apul_transcript_matrix_file <- "../../D-Apul/output/02.20-D-Apul-RNAseq-alignment-HiSat2/apul-transcript_count_matrix.csv"
peve_transcript_matrix_file <- "../../E-Peve/output/02.20-E-Peve-RNAseq-alignment-HiSat2/peve-transcript_count_matrix.csv"
ptua_transcript_matrix_file <- "../../F-Ptua/output/02.20-F-Ptua-RNAseq-alignment-HiSat2/ptua-transcript_count_matrix.csv"

# CONDA
conda_env_name <- c("/home/sam/programs/mambaforge/envs/barnacle_py311_env")
conda_path <- c("/opt/anaconda/anaconda3/bin/conda")
```

## Load [barnacle](https://github.com/blasks/barnacle) conda environment

If this is successful, the first line of output should show that the Python being used is the one in your [barnacle](https://github.com/blasks/barnacle) [@blaskowski2024] conda environment path.

E.g.

`python:         /home/sam/programs/mambaforge/envs/barnacle_py311_env/bin/python`

```{r load-barnacle-conda-env, eval=TRUE}
use_condaenv(condaenv = conda_env_name, conda = conda_path)
py_config()
```

# DATA PREP

## Load ortholog groups data

```{r load-ortholog-data, eval=TRUE}
# Read in the ortholog groups data
ortholog_groups <- read.csv(ortholog_groups_file)



# Display basic info about the data
cat("Dimensions of ortholog groups data:", dim(ortholog_groups), "\n\n")
cat("Column names:", colnames(ortholog_groups), "\n\n")
head(ortholog_groups)
str(ortholog_groups)
```

## Extract ortholog expression data

Now let's extract expression data for genes that are present in the ortholog groups. We'll use the gene count matrices with gene IDs to properly map the data.

### Load gene count matrices

```{r load-gene-matrices, eval=TRUE}
# Define file paths for gene count matrices
apul_gene_matrix_file <- "../../D-Apul/output/02.20-D-Apul-RNAseq-alignment-HiSat2/apul-gene_count_matrix.csv"
peve_gene_matrix_file <- "../../E-Peve/output/02.20-E-Peve-RNAseq-alignment-HiSat2/peve-gene_count_matrix.csv"
ptua_gene_matrix_file <- "../../F-Ptua/output/02.20-F-Ptua-RNAseq-alignment-HiSat2/ptua-gene_count_matrix.csv"

# Load gene count matrices for each species
cat("Loading gene count matrices...\n\n")

apul_gene_matrix <- read.csv(apul_gene_matrix_file)
cat("Apul gene matrix dimensions:", dim(apul_gene_matrix), "\n")

peve_gene_matrix <- read.csv(peve_gene_matrix_file)
cat("Peve gene matrix dimensions:", dim(peve_gene_matrix), "\n")

ptua_gene_matrix <- read.csv(ptua_gene_matrix_file)
cat("Ptua gene matrix dimensions:", dim(ptua_gene_matrix), "\n\n")
```

### Filter ortholog groups for complete three-way matches

```{r filter-ortholog-groups, eval=TRUE}
cat("Filtering for complete three-way ortholog groups...\n")

# Keep only rows where all three species have entries (no NA values or empty strings)
complete_ortholog_groups <- ortholog_groups[nzchar(ortholog_groups$apul) & 
                                          nzchar(ortholog_groups$peve) & 
                                          nzchar(ortholog_groups$ptua), ]

cat("Total ortholog groups:", nrow(ortholog_groups), "\n")
cat("Complete three-way ortholog groups:", nrow(complete_ortholog_groups), "\n")

```

### Filter for expression data availability

```{r filter-expression-availability, eval=TRUE}
cat("Filtering ortholog groups to ensure all genes have expression data...\n")

# Clean gene IDs to check against expression data
# For Apul: remove -T[n] suffix from ortholog groups to match gene matrix format
apul_ortholog_genes_check <- gsub("-T[0-9]+$", "", complete_ortholog_groups$apul)

# For Peve and Ptua: use as-is (will clean gene matrix IDs later)
peve_ortholog_genes_check <- complete_ortholog_groups$peve
ptua_ortholog_genes_check <- complete_ortholog_groups$ptua

# Check which genes are present in expression data
# (Note: We need to clean gene matrix IDs to match)
apul_gene_matrix_ids <- gsub("^gene-", "", apul_gene_matrix$gene_id)  # Remove gene- prefix if present
peve_gene_matrix_ids <- gsub("^gene-", "", peve_gene_matrix$gene_id)  # Remove gene- prefix
ptua_gene_matrix_ids <- gsub("^gene-", "", ptua_gene_matrix$gene_id)  # Remove gene- prefix

# Find which ortholog genes are present in each species' expression data
apul_present <- apul_ortholog_genes_check %in% apul_gene_matrix_ids
peve_present <- peve_ortholog_genes_check %in% peve_gene_matrix_ids
ptua_present <- ptua_ortholog_genes_check %in% ptua_gene_matrix_ids

# Keep only ortholog groups where all three species have expression data
expression_complete_mask <- apul_present & peve_present & ptua_present
complete_ortholog_groups <- complete_ortholog_groups[expression_complete_mask, ]

cat("Ortholog groups after filtering for expression data availability:", nrow(complete_ortholog_groups), "\n")

```

### Gene ID cleaning examples

```{r gene-id-examples-testing, eval=TRUE}
cat("\n=== GENE ID CLEANING EXAMPLES ===\n")
cat("Apul (clean ortholog groups to match gene matrix):\n")
cat("Ortholog groups original:", head(complete_ortholog_groups$apul, 3), "\n")
cat("Ortholog groups cleaned:", head(gsub("-T[0-9]+$", "", complete_ortholog_groups$apul), 3), "\n")
cat("Gene matrix (target format):", head(apul_gene_matrix$gene_id, 3), "\n\n")

cat("Peve (clean gene matrix to match ortholog groups):\n")
cat("Ortholog groups (target format):", head(complete_ortholog_groups$peve, 3), "\n") 
cat("Gene matrix original:", head(peve_gene_matrix$gene_id, 3), "\n")
cat("Gene matrix cleaned:", head(peve_gene_matrix$gene_id_clean, 3), "\n\n")

cat("Ptua (clean gene matrix to match ortholog groups):\n")
cat("Ortholog groups (target format):", head(complete_ortholog_groups$ptua, 3), "\n")
cat("Gene matrix original:", head(ptua_gene_matrix$gene_id, 3), "\n")
cat("Gene matrix cleaned:", head(ptua_gene_matrix$gene_id_clean, 3), "\n\n")

```

### Clean gene IDs for matching

```{r clean-gene-ids, eval=TRUE}
cat("Cleaning gene matrix IDs to match ortholog group format...\n")

# For Apul: ortholog groups have "FUN_000185-T1", gene matrix has "FUN_002326"
# We need to remove "-T1" from ortholog groups to match gene matrix
apul_ortholog_genes <- unique(gsub("-T[0-9]+$", "", complete_ortholog_groups$apul))

# For Peve and Ptua: keep ortholog groups as-is and clean gene matrix
peve_ortholog_genes <- unique(complete_ortholog_groups$peve)
ptua_ortholog_genes <- unique(complete_ortholog_groups$ptua)

# Clean gene matrix IDs accordingly
# Apul: gene matrix already in correct format (no cleaning needed)
apul_gene_matrix$gene_id_clean <- apul_gene_matrix$gene_id

# Peve: gene matrix has "gene-Peve_00000032", ortholog groups have "Peve_00037402"  
# So we need to remove "gene-" prefix from gene matrix
peve_gene_matrix$gene_id_clean <- gsub("^gene-", "", peve_gene_matrix$gene_id)

# Ptua: gene matrix has "gene-Pocillopora_meandrina_HIv1___RNAseq.g20905.t1", 
# ortholog groups have "Pocillopora_meandrina_HIv1___RNAseq.g28886.t1"
# So we just need to remove "gene-" prefix from gene matrix
ptua_gene_matrix$gene_id_clean <- gsub("^gene-", "", ptua_gene_matrix$gene_id)

cat("Apul ortholog genes (complete groups only):", length(apul_ortholog_genes), "\n")
cat("Peve ortholog genes (complete groups only):", length(peve_ortholog_genes), "\n")
cat("Ptua ortholog genes (complete groups only):", length(ptua_ortholog_genes), "\n\n")
```

### Define sample filtering function

```{r define-sample-filter-function, eval=TRUE}
# Function to filter samples that have all four timepoints (TP1, TP2, TP3, TP4)
filter_complete_samples <- function(gene_matrix, species_name) {
  cat("  Filtering samples with complete timepoints for", species_name, "...\n")
  
  # Get expression columns (exclude gene_id column, gene_id_clean may not exist yet)
  all_cols <- colnames(gene_matrix)
  id_cols <- c("gene_id", "gene_id_clean")
  expr_cols <- setdiff(all_cols, id_cols)
  
  if(length(expr_cols) == 0) {
    cat("  ERROR: No expression columns found! Returning original matrix.\n")
    return(gene_matrix)
  }
  
  # Parse sample column names to identify sample groups and timepoints
  sample_timepoint_map <- list()
  
  for(col in expr_cols) {
    # Expected formats: ACR.139.TP1, POR.216.TP1, POC.201.TP1, etc.
    # Note: Using dots (.) as separators, not hyphens (-)
    parts <- strsplit(col, "\\.")[[1]]  # Split on dots, not hyphens
    
    if(length(parts) >= 3) {
      # Extract timepoint from last part (e.g., "TP1", "TP2", etc.)
      tp_part <- parts[length(parts)]
      
      if(grepl("^TP[0-9]+$", tp_part)) {
        timepoint <- as.numeric(gsub("TP", "", tp_part))
        
        # Sample ID is everything except the timepoint part, joined with dots
        sample_id <- paste(parts[1:(length(parts)-1)], collapse = ".")
        
        if(!sample_id %in% names(sample_timepoint_map)) {
          sample_timepoint_map[[sample_id]] <- list()
        }
        sample_timepoint_map[[sample_id]][[as.character(timepoint)]] <- col
      }
    }
  }
  
  # Identify samples with all four timepoints (1, 2, 3, 4)
  complete_samples <- c()
  incomplete_samples <- c()
  required_timepoints <- c("1", "2", "3", "4")
  
  for(sample_id in names(sample_timepoint_map)) {
    available_timepoints <- names(sample_timepoint_map[[sample_id]])
    
    if(all(required_timepoints %in% available_timepoints)) {
      complete_samples <- c(complete_samples, sample_id)
    } else {
      incomplete_samples <- c(incomplete_samples, sample_id)
      missing_tps <- setdiff(required_timepoints, available_timepoints)
      cat("    Sample", sample_id, "missing timepoints:", paste(paste0("TP", missing_tps), collapse = ", "), "\n")
    }
  }
  
  cat("  Complete samples (all 4 timepoints):", length(complete_samples), "\n")
  cat("  Incomplete samples:", length(incomplete_samples), "\n")
  
  if(length(incomplete_samples) > 0) {
    cat("  Removing incomplete samples:", paste(incomplete_samples, collapse = ", "), "\n")
  }
  
  # Build list of columns to keep (include gene_id columns plus ONLY complete sample columns)
  if(length(complete_samples) == 0) {
    cat("  WARNING: No complete samples found! Keeping all expression columns.\n")
    # Fallback: keep all expression columns
    keep_cols <- c("gene_id")
    if("gene_id_clean" %in% colnames(gene_matrix)) {
      keep_cols <- c(keep_cols, "gene_id_clean")
    }
    keep_cols <- c(keep_cols, expr_cols)
  } else {
    keep_cols <- c("gene_id")
    if("gene_id_clean" %in% colnames(gene_matrix)) {
      keep_cols <- c(keep_cols, "gene_id_clean")
    }
    
    # ONLY add columns from complete samples (this excludes incomplete sample columns)
    for(sample_id in complete_samples) {
      for(tp in required_timepoints) {
        if(tp %in% names(sample_timepoint_map[[sample_id]])) {
          keep_cols <- c(keep_cols, sample_timepoint_map[[sample_id]][[tp]])
        }
      }
    }
    
    # Double-check: ensure we're not accidentally including incomplete sample columns
    incomplete_cols <- c()
    for(sample_id in incomplete_samples) {
      for(tp in names(sample_timepoint_map[[sample_id]])) {
        incomplete_cols <- c(incomplete_cols, sample_timepoint_map[[sample_id]][[tp]])
      }
    }
    
    if(length(incomplete_cols) > 0) {
      # Remove any incomplete sample columns that might have been included
      keep_cols <- setdiff(keep_cols, incomplete_cols)
    }
  }
  
  # Filter the gene matrix to keep only complete samples
  keep_cols <- intersect(keep_cols, colnames(gene_matrix))  # Ensure columns exist
  filtered_matrix <- gene_matrix[, keep_cols, drop = FALSE]
  
  # Count ID vs expression columns in final result
  final_expr_cols <- setdiff(colnames(filtered_matrix), c("gene_id", "gene_id_clean"))
  cat("  Final expression columns:", length(final_expr_cols), "\n")
  
  return(filtered_matrix)
}
```

### Define expression extraction function

```{r define-expression-extraction-function, eval=TRUE}
# Function to extract expression data for a species using ortholog group mapping
extract_species_expression <- function(ortholog_groups_df, gene_matrix, species_col, species_name) {
  cat("Processing", species_name, "...\n")
  
  # First, filter the gene matrix to keep only samples with complete timepoints
  filtered_gene_matrix <- filter_complete_samples(gene_matrix, species_name)
  
  # Remove duplicate group_ids, keeping first occurrence of each
  unique_groups <- ortholog_groups_df[!duplicated(ortholog_groups_df$group_id), ]
  cat("  Unique ortholog groups:", nrow(unique_groups), "\n")
  
  # Create results data frame starting with group_id
  result_df <- data.frame(group_id = unique_groups$group_id, stringsAsFactors = FALSE)
  
  # Get expression columns (exclude gene_id and gene_id_clean columns)
  expr_cols <- setdiff(colnames(filtered_gene_matrix), c("gene_id", "gene_id_clean"))
  
  # Initialize expression columns with NA
  for(col in expr_cols) {
    result_df[[col]] <- NA
  }
  
  # For each ortholog group, find the corresponding gene and extract expression
  for(i in seq_len(nrow(unique_groups))) {
    target_gene <- unique_groups[[species_col]][i]
    
    # Clean target gene for matching
    if(species_name == "Apul") {
      # Remove transcript suffix for Apul
      target_gene_clean <- gsub("-T[0-9]+$", "", target_gene)
      matching_rows <- which(filtered_gene_matrix$gene_id_clean == target_gene_clean)
    } else {
      # For Peve and Ptua, match cleaned gene_id
      matching_rows <- which(filtered_gene_matrix$gene_id_clean == target_gene)
    }
    
    if(length(matching_rows) == 1) {
      # Single match - copy expression data
      for(col in expr_cols) {
        result_df[i, col] <- filtered_gene_matrix[matching_rows, col]
      }
    } else if(length(matching_rows) > 1) {
      # Multiple matches - take first gene (no averaging)
      first_match <- matching_rows[1]
      for(col in expr_cols) {
        result_df[i, col] <- filtered_gene_matrix[first_match, col]
      }
    } else {
      # No match - leave as NA (will be removed later)
    }
  }
  
  # Remove rows with all NA expression values
  expr_na_mask <- apply(result_df[, expr_cols, drop = FALSE], 1, function(x) all(is.na(x)))
  result_df <- result_df[!expr_na_mask, ]
  
  cat("  Final dimensions:", nrow(result_df), "ortholog groups x", ncol(result_df)-1, "samples\n")
  cat("  Removed", sum(expr_na_mask), "groups with no expression data\n\n")
  
  return(result_df)
}
```

### Extract ortholog expression data

```{r extract-ortholog-expression-data, eval=TRUE}
cat("Creating ortholog expression data with proper group_id mapping...\n")

# Extract expression data for each species using the ortholog group mapping
apul_ortholog_expression <- extract_species_expression(complete_ortholog_groups, apul_gene_matrix, "apul", "Apul")
peve_ortholog_expression <- extract_species_expression(complete_ortholog_groups, peve_gene_matrix, "peve", "Peve")
ptua_ortholog_expression <- extract_species_expression(complete_ortholog_groups, ptua_gene_matrix, "ptua", "Ptua")

cat("=== FINAL ORTHOLOG EXPRESSION DATA DIMENSIONS ===\n")
cat("Apul:", nrow(apul_ortholog_expression), "ortholog groups x", ncol(apul_ortholog_expression)-1, "samples\n")
cat("Peve:", nrow(peve_ortholog_expression), "ortholog groups x", ncol(peve_ortholog_expression)-1, "samples\n")
cat("Ptua:", nrow(ptua_ortholog_expression), "ortholog groups x", ncol(ptua_ortholog_expression)-1, "samples\n\n")
```

### Write ortholog expression data

```{r write-expression-data, eval=TRUE}
cat("Exporting ortholog expression data to CSV files...\n")

# Define output file paths
apul_output_file <- file.path(output_dir, "apul_ortholog_expression.csv")
peve_output_file <- file.path(output_dir, "peve_ortholog_expression.csv")
ptua_output_file <- file.path(output_dir, "ptua_ortholog_expression.csv")

# Write CSV files without quotes
write.csv(apul_ortholog_expression, file = apul_output_file, quote = FALSE, row.names = FALSE)
cat("Exported Apul ortholog expression data to:", apul_output_file, "\n")

write.csv(peve_ortholog_expression, file = peve_output_file, quote = FALSE, row.names = FALSE)
cat("Exported Peve ortholog expression data to:", peve_output_file, "\n")

write.csv(ptua_ortholog_expression, file = ptua_output_file, quote = FALSE, row.names = FALSE)
cat("Exported Ptua ortholog expression data to:", ptua_output_file, "\n")

cat("\nAll ortholog expression data exported successfully!\n\n")
```

### Column structure analysis

```{r column-structure-analysis, eval=TRUE}
cat("=== COLUMN STRUCTURE ANALYSIS ===\n")
cat("Apul columns:", ncol(apul_ortholog_expression), "\n")
cat("Apul column names (first 10):", paste(head(colnames(apul_ortholog_expression), 10), collapse = ", "), "\n")
cat("Apul column names (last 10):", paste(tail(colnames(apul_ortholog_expression), 10), collapse = ", "), "\n\n")

cat("Peve columns:", ncol(peve_ortholog_expression), "\n")
cat("Peve column names (first 10):", paste(head(colnames(peve_ortholog_expression), 10), collapse = ", "), "\n")
cat("Peve column names (last 10):", paste(tail(colnames(peve_ortholog_expression), 10), collapse = ", "), "\n\n")

cat("Ptua columns:", ncol(ptua_ortholog_expression), "\n")
cat("Ptua column names (first 10):", paste(head(colnames(ptua_ortholog_expression), 10), collapse = ", "), "\n")
cat("Ptua column names (last 10):", paste(tail(colnames(ptua_ortholog_expression), 10), collapse = ", "), "\n\n")

```

### Summary statistics

```{r summary-statistics, eval=TRUE}
cat("=== LINE COUNTS FOR ORTHOLOG EXPRESSION DATA ===\n")
cat("Apul ortholog expression with info: ", nrow(apul_ortholog_expression), " rows\n")
cat("Peve ortholog expression with info: ", nrow(peve_ortholog_expression), " rows\n")
cat("Ptua ortholog expression with info: ", nrow(ptua_ortholog_expression), " rows\n\n")
```

### Sample filtering verification

```{r sample-filtering-verification, eval=TRUE}
cat("=== SAMPLE FILTERING VERIFICATION ===\n")
cat("This analysis filters samples to include ONLY those with all four timepoints (TP1, TP2, TP3, TP4)\n\n")

# Function to analyze sample completeness in the filtered data
analyze_sample_completeness <- function(data_df, species_name) {
  cat("Analyzing", species_name, "sample completeness:\n")
  
  # Get expression columns (exclude group_id)
  expr_cols <- setdiff(colnames(data_df), "group_id")
  
  # Parse sample column names to identify sample groups and timepoints
  sample_timepoint_map <- list()
  
  for(col in expr_cols) {
    parts <- strsplit(col, "\\.")[[1]]  # Split on dots to match actual format
    
    if(length(parts) >= 3) {
      tp_part <- parts[length(parts)]
      
      if(grepl("^TP[0-9]+$", tp_part)) {
        timepoint <- as.numeric(gsub("TP", "", tp_part))
        sample_id <- paste(parts[1:(length(parts)-1)], collapse = ".")
        
        if(!sample_id %in% names(sample_timepoint_map)) {
          sample_timepoint_map[[sample_id]] <- c()
        }
        sample_timepoint_map[[sample_id]] <- c(sample_timepoint_map[[sample_id]], timepoint)
      }
    }
  }
  
  # Check completeness
  complete_samples <- 0
  incomplete_samples <- 0
  required_timepoints <- c(1, 2, 3, 4)
  
  for(sample_id in names(sample_timepoint_map)) {
    available_timepoints <- sort(sample_timepoint_map[[sample_id]])
    
    if(all(required_timepoints %in% available_timepoints)) {
      complete_samples <- complete_samples + 1
    } else {
      incomplete_samples <- incomplete_samples + 1
      missing_tps <- setdiff(required_timepoints, available_timepoints)
      cat("  WARNING: Sample", sample_id, "missing timepoints:", paste(paste0("TP", missing_tps), collapse = ", "), "\n")
    }
  }
  
  cat("  Total samples:", length(sample_timepoint_map), "\n")
  cat("  Complete samples (all 4 timepoints):", complete_samples, "\n")
  cat("  Incomplete samples:", incomplete_samples, "\n")
  
  if(incomplete_samples > 0) {
    cat("  ERROR: Filtering did not work correctly!\n")
  } else {
    cat("  SUCCESS: All samples have complete timepoints\n")
  }
  
  cat("\n")
  
  return(list(
    total = length(sample_timepoint_map),
    complete = complete_samples,
    incomplete = incomplete_samples
  ))
}

# Verify each species
apul_stats <- analyze_sample_completeness(apul_ortholog_expression, "Apul")
peve_stats <- analyze_sample_completeness(peve_ortholog_expression, "Peve")
ptua_stats <- analyze_sample_completeness(ptua_ortholog_expression, "Ptua")

cat("=== FILTERING SUMMARY ===\n")
cat("All samples in the filtered dataset should have exactly 4 timepoints (TP1, TP2, TP3, TP4)\n")
cat("Apul: ", apul_stats$complete, "/", apul_stats$total, " complete samples\n")
cat("Peve: ", peve_stats$complete, "/", peve_stats$total, " complete samples\n")
cat("Ptua: ", ptua_stats$complete, "/", ptua_stats$total, " complete samples\n")

total_incomplete <- apul_stats$incomplete + peve_stats$incomplete + ptua_stats$incomplete
if(total_incomplete == 0) {
  cat("SUCCESS: All samples across all species have complete timepoints!\n")
} else {
  cat("ERROR: Found", total_incomplete, "incomplete samples - filtering needs to be fixed!\n")
}
cat("\n")
```

### Diagnostic analysis

```{r diagnostic-analysis, eval=TRUE}
cat("\n=== VERIFICATION: THREE-WAY ORTHOLOG COUNTS ===\n")
cat("After filtering, all species should have identical three_way ortholog counts.\n\n")

# Check how many genes we have for each species
cat("Genes found in expression data by species:\n")
cat("Apul ortholog expression (before adding info):", nrow(apul_ortholog_expression), "\n")
cat("Peve ortholog expression (before adding info):", nrow(peve_ortholog_expression), "\n")
cat("Ptua ortholog expression (before adding info):", nrow(ptua_ortholog_expression), "\n\n")

# Verify all ortholog groups are three-way
cat("Complete three-way ortholog groups available:", nrow(complete_ortholog_groups), "\n")
cat("All should be type 'three_way'? Check:", table(complete_ortholog_groups$type), "\n\n")

# Verify perfect gene coverage (should be 100% for all species now)
apul_coverage <- length(intersect(apul_ortholog_genes, apul_gene_matrix$gene_id_clean))
peve_coverage <- length(intersect(peve_ortholog_genes, peve_gene_matrix$gene_id_clean))
ptua_coverage <- length(intersect(ptua_ortholog_genes, ptua_gene_matrix$gene_id_clean))

cat("Gene coverage in expression data (should be 100% for all):\n")
cat("Apul: ", apul_coverage, "/", length(apul_ortholog_genes), " (", round(apul_coverage/length(apul_ortholog_genes)*100, 1), "%)\n")
cat("Peve: ", peve_coverage, "/", length(peve_ortholog_genes), " (", round(peve_coverage/length(peve_ortholog_genes)*100, 1), "%)\n")
cat("Ptua: ", ptua_coverage, "/", length(ptua_ortholog_genes), " (", round(ptua_coverage/length(ptua_ortholog_genes)*100, 1), "%)\n\n")
```

### Preview expression data

```{r preview-expression-data, eval=TRUE}
cat("\n=== PREVIEW OF ORTHOLOG EXPRESSION DATA ===\n")

cat("Apul ortholog expression:\n\n")
str(apul_ortholog_expression)
cat("\n\n")

cat("\nPeve ortholog expression:\n")
str(peve_ortholog_expression)
cat("\n\n")

cat("\nPtua ortholog expression:\n")
str(ptua_ortholog_expression)
cat("\n\n")
```

# BARNACLE ANALYSIS

This workflow focuses on comparing multiple ranks to find the optimal number of components.

Based on the barnacle workflow, we: 1. Normalize count data with `sctransform` 2. Create tensors for multiomics analysis 3. Compare multiple ranks systematically 4. Choose optimal rank based on metrics

## Load expression data

```{r load-normalized-data, eval=TRUE}
# Read the exported ortholog expression data
apul_expr <- read.csv(file.path(output_dir, "apul_ortholog_expression.csv"))
peve_expr <- read.csv(file.path(output_dir, "peve_ortholog_expression.csv"))  
ptua_expr <- read.csv(file.path(output_dir, "ptua_ortholog_expression.csv"))

cat("Loaded expression data:\n")
cat("Apul:", nrow(apul_expr), "genes x", ncol(apul_expr)-1, "samples\n")
cat("Peve:", nrow(peve_expr), "genes x", ncol(peve_expr)-1, "samples\n") 
cat("Ptua:", nrow(ptua_expr), "genes x", ncol(ptua_expr)-1, "samples\n")
```

## Normalize data with sctransform

Following the barnacle manuscript approach, we'll use `sctransform` to normalize each species' data. We'll use a bulk RNA-seq appropriate approach.

```{r normalize-sctransform, eval=TRUE}
# Function to normalize count data with sctransform for bulk RNA-seq
normalize_with_sctransform <- function(count_data, species_name) {
  cat("Normalizing", species_name, "data with sctransform...\n")
  
  # Check if we have group_id or gene_id column
  id_col <- if("group_id" %in% colnames(count_data)) "group_id" else "gene_id"
  cat("Using", id_col, "as identifier column\n")
  
  # Check for and handle duplicate group_ids/gene_ids
  duplicate_ids <- count_data[[id_col]][duplicated(count_data[[id_col]])]
  if(length(duplicate_ids) > 0) {
    cat("Found", length(unique(duplicate_ids)), "duplicate", id_col, "values:\n")
    cat("  Examples:", head(unique(duplicate_ids), 10), "\n")
    cat("  Note: sctransform may fail due to duplicate row names, will fall back to log2(CPM + 1)\n")
  } else {
    cat("No duplicate", id_col, "values found\n")
  }
  
  # Use original data without aggregation - let sctransform handle duplicates or fail
  agg_data <- count_data
  
  # Extract count matrix (genes as rows, samples as columns)
  count_matrix <- as.matrix(agg_data[, -1])  # Remove id column
  rownames(count_matrix) <- agg_data[[id_col]]
  
  # Check for and handle problematic values
  cat("Checking data quality...\n")
  cat("  - Zero values:", sum(count_matrix == 0), "/", length(count_matrix), "\n")
  cat("  - NA values:", sum(is.na(count_matrix)), "\n")
  cat("  - Infinite values:", sum(is.infinite(count_matrix)), "\n")
  cat("  - Min value:", min(count_matrix, na.rm = TRUE), "\n")
  cat("  - Max value:", max(count_matrix, na.rm = TRUE), "\n")
  
  # Remove genes with all zeros or very low expression
  gene_sums <- rowSums(count_matrix)
  keep_genes <- gene_sums > 10  # Keep genes with total counts > 10
  count_matrix_filtered <- count_matrix[keep_genes, , drop = FALSE]
  
  cat("  - Filtered to", nrow(count_matrix_filtered), "ortholog groups (from", nrow(count_matrix), ")\n")
  
  # Ensure data is numeric and handle any potential issues
  count_matrix_filtered <- apply(count_matrix_filtered, c(1,2), function(x) {
    if(is.na(x) || !is.finite(x)) return(0)
    return(as.numeric(x))
  })
  
  # Ensure row and column names are character strings (not factors)
  rownames(count_matrix_filtered) <- as.character(rownames(count_matrix_filtered))
  colnames(count_matrix_filtered) <- as.character(colnames(count_matrix_filtered))
  
  # sctransform expects genes as rows and cells (samples) as columns —
  # our matrix is already genes x samples (rows=genes, cols=samples), so do NOT transpose.
  count_matrix_for_vst <- count_matrix_filtered
  
  # Apply sctransform normalization with bulk RNA-seq appropriate parameters
  normalized_df <- tryCatch({
    # First try with minimal parameters to isolate the issue
    cat("  Attempting sctransform with minimal parameters...\n")
    # Pass genes x samples matrix directly
    normalized <- sctransform::vst(
      count_matrix_for_vst,
      verbosity = 1
    )

    # Extract normalized data (should be genes x samples)
    normalized_data <- normalized$y
    
    # Create output data frame with original gene set (fill missing with zeros)
    full_normalized_data <- matrix(0, nrow = nrow(count_matrix), ncol = ncol(count_matrix))
    rownames(full_normalized_data) <- rownames(count_matrix)
    colnames(full_normalized_data) <- colnames(count_matrix)
    
    # Fill in normalized values for kept genes
    full_normalized_data[rownames(normalized_data), ] <- normalized_data
    
    result_df <- data.frame(
      group_id = rownames(full_normalized_data),
      full_normalized_data,
      stringsAsFactors = FALSE
    )
    
    cat("sctransform normalization successful for", species_name, "\n")
    return(result_df)
    
  }, error = function(e) {
    cat("sctransform with minimal parameters failed for", species_name, ":", e$message, "\n")
    cat("Trying sctransform with glmGamPoi method...\n")
    
    # Try again with glmGamPoi method
    tryCatch({
      normalized <- sctransform::vst(
        count_matrix_for_vst, 
        method = "glmGamPoi",
        n_genes = min(2000, nrow(count_matrix_for_vst)),
        return_cell_attr = TRUE,
        verbosity = 1
      )

      # Extract normalized data (genes x samples)
      normalized_data <- normalized$y
      
      # Create output data frame with original gene set (fill missing with zeros)
      full_normalized_data <- matrix(0, nrow = nrow(count_matrix), ncol = ncol(count_matrix))
      rownames(full_normalized_data) <- rownames(count_matrix)
      colnames(full_normalized_data) <- colnames(count_matrix)
      
      # Fill in normalized values for kept genes
      full_normalized_data[rownames(normalized_data), ] <- normalized_data
      
      result_df <- data.frame(
        group_id = rownames(full_normalized_data),
        full_normalized_data,
        stringsAsFactors = FALSE
      )
      
      cat("sctransform normalization with glmGamPoi successful for", species_name, "\n")
      return(result_df)
      
    }, error = function(e2) {
      cat("sctransform failed for", species_name, ":", e2$message, "\n")
      cat("Falling back to log2(CPM + 1) normalization...\n")
      
  # Fallback: log2(CPM + 1) normalization
  # Calculate CPM (Counts Per Million) using the filtered count matrix
  lib_sizes <- colSums(count_matrix_filtered)
  cpm_matrix <- sweep(count_matrix_filtered, 2, lib_sizes/1e6, FUN = "/")
      
      # Log2 transform with pseudocount
      normalized_data <- log2(cpm_matrix + 1)
      
      result_df <- data.frame(
        group_id = rownames(normalized_data),
        normalized_data,
        stringsAsFactors = FALSE
      )
      
      cat("Log2(CPM + 1) normalization complete for", species_name, "\n")
      return(result_df)
    })
  })
  
  cat("Input dimensions:", nrow(count_data), "rows x", ncol(count_data)-1, "samples\n")
  cat("Output dimensions:", nrow(normalized_df), "ortholog groups x", ncol(normalized_df)-1, "samples\n\n")
  
  return(normalized_df)
}

# Normalize each species
cat("=== STARTING NORMALIZATION ===\n\n")
apul_normalized <- normalize_with_sctransform(apul_expr, "Apul")
peve_normalized <- normalize_with_sctransform(peve_expr, "Peve") 
ptua_normalized <- normalize_with_sctransform(ptua_expr, "Ptua")
cat("=== NORMALIZATION COMPLETE ===\n\n")
```

## Export normalized data for Python analysis

```{r export-normalized-data, eval=TRUE}
# Export normalized data for Python processing
apul_norm_file <- file.path(output_dir, "apul_normalized_expression.csv")
peve_norm_file <- file.path(output_dir, "peve_normalized_expression.csv")
ptua_norm_file <- file.path(output_dir, "ptua_normalized_expression.csv")

write.csv(apul_normalized, apul_norm_file, row.names = FALSE, quote = FALSE)
write.csv(peve_normalized, peve_norm_file, row.names = FALSE, quote = FALSE)
write.csv(ptua_normalized, ptua_norm_file, row.names = FALSE, quote = FALSE)

cat("Exported normalized data:\n")
cat("Apul:", apul_norm_file, "\n")
cat("Peve:", peve_norm_file, "\n")
cat("Ptua:", ptua_norm_file, "\n\n")
```

## Create tensor dataset in Python

Now we'll switch to Python to create the multiomics tensor and run barnacle analysis.

### Setup logging
```{python setup-logging, eval=TRUE}
import sys
import os
from datetime import datetime

# Set up logging to both console and file
class Logger:
    def __init__(self, log_file):
        self.terminal = sys.stdout
        self.log = open(log_file, 'w')
        
    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
        self.log.flush()
        
    def flush(self):
        self.terminal.flush()
        self.log.flush()

# Create log file path
output_dir = r.output_dir
log_file = os.path.join(output_dir, f'barnacle_analysis_log_{datetime.now().strftime("%Y%m%d_%H%M%S")}.txt')

# Redirect stdout to both console and log file
sys.stdout = Logger(log_file)
sys.stderr = sys.stdout  # Also capture error messages

print("="*60)
print(f"BARNACLE ANALYSIS LOG")
print(f"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"Log file: {log_file}")
print("="*60)
print()
```


### Create multiomics tensor
```{python create-multiomics-tensor, eval=TRUE}
import pandas as pd
import numpy as np
import os
from pathlib import Path

# Set up paths
output_dir = r.output_dir
print(f"Working in output directory: {output_dir}")

# Load normalized data
apul_norm = pd.read_csv(os.path.join(output_dir, "apul_normalized_expression.csv"))
peve_norm = pd.read_csv(os.path.join(output_dir, "peve_normalized_expression.csv"))
ptua_norm = pd.read_csv(os.path.join(output_dir, "ptua_normalized_expression.csv"))

print("Loaded normalized data:")
print(f"Apul: {apul_norm.shape}")
print(f"Peve: {peve_norm.shape}")  
print(f"Ptua: {ptua_norm.shape}")

# Check which genes are common across all species
apul_genes = set(apul_norm['group_id'])
peve_genes = set(peve_norm['group_id'])
ptua_genes = set(ptua_norm['group_id'])

common_genes = apul_genes & peve_genes & ptua_genes
print(f"\nCommon genes across all species: {len(common_genes)}")

# Filter to common genes and align gene order
common_genes_list = sorted(list(common_genes))

apul_common = apul_norm[apul_norm['group_id'].isin(common_genes_list)].set_index('group_id').reindex(common_genes_list)
peve_common = peve_norm[peve_norm['group_id'].isin(common_genes_list)].set_index('group_id').reindex(common_genes_list)
ptua_common = ptua_norm[ptua_norm['group_id'].isin(common_genes_list)].set_index('group_id').reindex(common_genes_list)

print(f"\nFiltered to common genes:")
print(f"Apul: {apul_common.shape}")
print(f"Peve: {peve_common.shape}")
print(f"Ptua: {ptua_common.shape}")
```

## Parse sample information

```{python parse-sample-info, eval=TRUE}
# Parse sample names to extract sample information for each species independently
def parse_species_samples(columns, species_name):
    """Parse sample column names for a specific species"""
    sample_map = {}
    sample_ids = []
    timepoints = set()
    
    for col in columns:
        # Expected format: PREFIX-NUMBER-TP# (e.g., ACR-139-TP1, POR-216-TP1, POC-201-TP1)
        # Also support legacy format: PREFIX.NUMBER.TP# for backwards compatibility
        parts = col.split('-') if '-' in col else col.split('.')
        
        if len(parts) >= 3 and parts[-1].startswith('TP'):
            # Extract timepoint from last part (e.g., "TP1", "TP2", etc.)
            tp_part = parts[-1]
            timepoint = int(tp_part[2:])  # e.g., 1 from TP1
            
            # Sample ID is everything except the timepoint part
            sample_id_parts = parts[:-1]
            sample_id = '-'.join(sample_id_parts) if '-' in col else '.'.join(sample_id_parts)
            
            sample_map[(sample_id, timepoint)] = col
            if sample_id not in sample_ids:
                sample_ids.append(sample_id)
            timepoints.add(timepoint)
        else:
            print(f"Warning: Could not parse column name: {col}")
    
    return sample_map, sample_ids, sorted(timepoints)

# Parse sample information for each species independently
print("Parsing sample information for each species...")

species_data = {
    'apul': apul_common,
    'peve': peve_common, 
    'ptua': ptua_common
}

species_info = {}
all_timepoints = set()

for species, data in species_data.items():
    sample_map, sample_ids, timepoints = parse_species_samples(data.columns, species)
    species_info[species] = {
        'sample_map': sample_map,
        'sample_ids': sample_ids,
        'timepoints': timepoints,
        'n_samples': len(sample_ids)
    }
    all_timepoints.update(timepoints)
    
    print(f"{species}:")
    print(f"  Samples: {len(sample_ids)} ({sample_ids[:3]}...)")
    print(f"  Timepoints: {timepoints}")

common_timepoints = sorted(list(all_timepoints))
print(f"\nTimepoints found across all species: {common_timepoints}")

# Find the maximum number of samples to determine tensor dimensions
max_samples = max(info['n_samples'] for info in species_info.values())
print(f"Maximum samples in any species: {max_samples}")

# Print detailed sample structure
print(f"\nDetailed sample structure:")
for species, info in species_info.items():
    print(f"{species}: {info['n_samples']} samples × {len(info['timepoints'])} timepoints")
```

## Create 3D tensor (genes × species_samples × timepoints)

```{python create-3d-tensor, eval=TRUE}
# Create a 3D tensor: genes × (species_samples) × timepoints
# This flattens species and samples into a single dimension that Barnacle can handle

print("Creating 3D tensor by combining species and samples...")
print("Note: R-level filtering has already removed samples without all 4 timepoints")

# First, collect all actual sample-timepoint combinations that have data
all_sample_columns = []
sample_labels = []  # Track which sample belongs to which species
species_sample_map = {}  # Map from combined index to (species, sample_idx, sample_id)

sample_idx = 0
for species in ['apul', 'peve', 'ptua']:
    data = species_data[species]
    info = species_info[species]
    
    print(f"\nProcessing {species}:")
    for sample_id in info['sample_ids']:
        # Check if this sample has data for ALL required timepoints
        available_timepoints = []
        sample_timepoint_cols = []
        
        for timepoint in common_timepoints:
            if (sample_id, timepoint) in info['sample_map']:
                col_name = info['sample_map'][(sample_id, timepoint)]
                sample_timepoint_cols.append(col_name)
                available_timepoints.append(timepoint)
        
        # Only include samples that have ALL timepoints (should be all samples due to R filtering)
        if len(available_timepoints) == len(common_timepoints):
            all_sample_columns.extend(sample_timepoint_cols)
            sample_labels.append(f"{species}_{sample_id}")
            species_sample_map[sample_idx] = {
                'species': species,
                'sample_id': sample_id,
                'sample_idx_in_species': info['sample_ids'].index(sample_id)
            }
            sample_idx += 1
            print(f"  Added {sample_id} with {len(sample_timepoint_cols)} timepoints: {sorted(available_timepoints)}")
        else:
            print(f"  Skipped {sample_id} - incomplete timepoints: {sorted(available_timepoints)} (expected: {sorted(common_timepoints)})")

n_genes = len(common_genes_list)
n_combined_samples = len(sample_labels)
n_timepoints = len(common_timepoints)

print(f"\nCreating 3D tensor with shape: ({n_genes}, {n_combined_samples}, {n_timepoints})")
print(f"Combined samples from all species: {n_combined_samples}")

# Initialize tensor
tensor_3d = np.full((n_genes, n_combined_samples, n_timepoints), np.nan)

# Fill tensor
filled_count = 0
missing_count = 0

for combined_idx, sample_label in enumerate(sample_labels):
    species_info_map = species_sample_map[combined_idx]
    species = species_info_map['species']
    sample_id = species_info_map['sample_id']
    
    data = species_data[species]
    info = species_info[species]
    
    for time_idx, timepoint in enumerate(common_timepoints):
        if (sample_id, timepoint) in info['sample_map']:
            col_name = info['sample_map'][(sample_id, timepoint)]
            tensor_3d[:, combined_idx, time_idx] = data[col_name].values
            filled_count += 1
        else:
            missing_count += 1

# Check tensor statistics
n_missing = np.sum(np.isnan(tensor_3d))
n_total = tensor_3d.size
n_finite = np.sum(np.isfinite(tensor_3d))

print(f"\n=== TENSOR STATISTICS ===")
print(f"Tensor shape: {tensor_3d.shape}")
print(f"Total elements: {n_total}")
print(f"Finite values: {n_finite}")
print(f"Missing/NaN values: {n_missing}")
print(f"Missing percentage: {n_missing / n_total * 100:.2f}%")
print(f"Filled {filled_count} sample-timepoint combinations")
print(f"Missing {missing_count} sample-timepoint combinations")

# Check non-zero values among finite values
finite_values = tensor_3d[np.isfinite(tensor_3d)]
n_nonzero = np.sum(finite_values != 0)
print(f"Non-zero finite values: {n_nonzero}")
print(f"Zero finite values: {len(finite_values) - n_nonzero}")
print(f"Sparsity among finite values: {(len(finite_values) - n_nonzero) / len(finite_values) * 100:.2f}%")

# Save sample mapping for later interpretation
sample_mapping = pd.DataFrame([
    {
        'combined_index': i,
        'sample_label': label,
        'species': species_sample_map[i]['species'],
        'sample_id': species_sample_map[i]['sample_id']
    }
    for i, label in enumerate(sample_labels)
])
print(f"\nSample mapping:")
print(sample_mapping.head(10))
```

## Dissertation-Validated Rank Selection

**IMPORTANT**: This section implements the cross-validated rank selection methodology from Blaskowski (2024) dissertation, Section 1.2.3.

The dissertation methodology uses biological replicates to validate rank and lambda selection through:
1. Cross-validated SSE (Sum of Squared Errors) for rank selection
2. Cross-validated FMS (Factor Match Score) with 1SE rule for lambda selection

This is the RECOMMENDED approach when biological replicates are available.

### Identify biological replicate groups

```{python identify-replicate-groups, eval=TRUE}
# ===========================================
# IDENTIFY BIOLOGICAL REPLICATE GROUPS
# ===========================================
# For cross-validation, we need to group samples that are biological replicates
# 
# TENSOR ORGANIZATION (important!):
# ---------------------------------
# Biological samples: ACR-139_TP1, ACR-139_TP2, ACR-139_TP3, ACR-139_TP4, ACR-145_TP1, ...
#                     (each is a physical sample collected at a timepoint)
#
# How stored in tensor:
#   genes × colonies × timepoints
#   
#   tensor[:, 0, 0] = ACR-139 at TP1  ─┐
#   tensor[:, 0, 1] = ACR-139 at TP2   │── Colony ACR-139
#   tensor[:, 0, 2] = ACR-139 at TP3   │   (sample dimension index 0)
#   tensor[:, 0, 3] = ACR-139 at TP4  ─┘
#   
#   tensor[:, 1, 0] = ACR-145 at TP1  ─┐
#   tensor[:, 1, 1] = ACR-145 at TP2   │── Colony ACR-145
#   tensor[:, 1, 2] = ACR-145 at TP3   │   (sample dimension index 1)
#   tensor[:, 1, 3] = ACR-145 at TP4  ─┘
#
# Cross-validation approach:
#   - Hold out colony ACR-139 (all 4 timepoints)
#   - Train on all other colonies (all their timepoints)
#   - Test on ACR-139 (all 4 timepoints)
# ===========================================

def identify_replicate_groups_for_cv(sample_labels, species_sample_map, n_timepoints=4):
    """
    Identify biological replicate groups for cross-validation.
    
    TENSOR STRUCTURE EXPLANATION:
    - Biological reality: Each sample collected at a timepoint (e.g., "ACR-139 at TP1")
    - Tensor organization: genes × colonies × timepoints
      * Sample dimension (axis 1) = individual COLONIES (e.g., ACR-139, ACR-145, etc.)
      * Timepoint dimension (axis 2) = measurement times (TP1, TP2, TP3, TP4)
      * Example: tensor[:, 0, :] = all timepoints for colony #0
      * Example: tensor[:, 0, 2] = colony #0 measured at TP3
    
    CROSS-VALIDATION APPROACH:
    - Each CV fold holds out ONE COLONY (all its timepoints)
    - Training set: All other colonies (all their timepoints)
    - Test set: Held-out colony (all its timepoints)
    - Question: Can the model predict a NEW colony's timeseries pattern?
    
    NOTE: This is NOT testing biological replicates at the same timepoint
    (e.g., multiple colonies measured at TP1). Instead, it tests whether
    the temporal patterns learned from some colonies can predict other colonies.
    
    Parameters:
    -----------
    sample_labels : list
        Sample labels (e.g., 'apul_ACR-139', 'peve_POR-216')
    species_sample_map : dict
        Mapping of sample indices to species and sample_id
    n_timepoints : int
        Number of timepoints per sample (default: 4)
    
    Returns:
    --------
    replicate_groups : dict
        Keys: colony identifier (e.g., 'apul_ACR-139')
        Values: list with single sample index for that colony [idx]
    """
    
    groups = {}
    
    for idx, label in enumerate(sample_labels):
        species = species_sample_map[idx]['species']
        sample_id = species_sample_map[idx]['sample_id']
        # Each colony is its own group (since each sample = colony across timepoints)
        groups[label] = [idx]
    
    print("="*80)
    print("CROSS-VALIDATION STRUCTURE FOR TIMESERIES TENSOR")
    print("="*80)
    print(f"Tensor structure: genes × samples × timepoints")
    print(f"  - Total samples: {len(sample_labels)}")
    print(f"  - Timepoints per sample: {n_timepoints}")
    print(f"\nHOW THE TENSOR IS ORGANIZED:")
    print(f"  • Each sample dimension = one COLONY (e.g., ACR-139)")
    print(f"  • That colony's data at each timepoint is in the 3rd dimension")
    print(f"  • Example: tensor[:, 0, :] contains all 4 timepoints for colony #0")
    print(f"  • Example: tensor[:, 0, 2] is colony #0 at timepoint TP3")
    print(f"\nCV approach: Leave-one-COLONY-out")
    print(f"  • Train on: N-1 colonies (all their timepoints)")
    print(f"  • Test on: 1 held-out colony (all its timepoints)")
    print(f"\nIMPORTANT CLARIFICATION:")
    print(f"  • Biological reality: Each sample is collected at ONE timepoint")
    print(f"  • Tensor organization: Timepoints grouped by colony")
    print(f"  • CV tests: Can model predict a NEW colony's timeseries?")
    print(f"\nCV Groups (one per colony):")
    
    # Group by species for display
    species_groups = {}
    for label, indices in groups.items():
        idx = indices[0]
        species = species_sample_map[idx]['species']
        if species not in species_groups:
            species_groups[species] = []
        species_groups[species].append(label)
    
    for species, colonies in sorted(species_groups.items()):
        print(f"  {species}: {len(colonies)} colonies")
        print(f"    {', '.join(colonies[:5])}{'...' if len(colonies) > 5 else ''}")
    
    print(f"\nTotal CV folds: {len(groups)}")
    print(f"CV method: Leave-one-colony-out (train on N-1 colonies, test on 1)")
    print(f"\nWARNING: This is computationally expensive!")
    print(f"  - Each rank tested will require {len(groups)} model fits")
    print(f"  - Consider subsetting to fewer colonies or using species-level CV")
    print("="*80)
    
    return groups

# Identify replicate groups
replicate_groups = identify_replicate_groups_for_cv(
    sample_labels, 
    species_sample_map,
    n_timepoints=len(common_timepoints)
)

# Store for later use
print(f"\nReplicate groups stored for cross-validation")
print(f"Available for dissertation-validated rank selection")

# ===========================================
# ALTERNATIVE: Species-level CV groups
# ===========================================
# For practical reasons, you may want to use species-level CV instead:
# - Faster: only 3 folds instead of ~30-40 folds
# - Tests species generalization: can model learn patterns that work across species?
# - More interpretable: "does decomposition work on new species?"

def identify_species_groups_for_cv(sample_labels, species_sample_map):
    """
    Create species-level CV groups (alternative to colony-level).
    
    This groups all colonies of the same species together.
    CV will train on 2 species, test on the 3rd species.
    
    Advantage: Fast (only 3 folds)
    Disadvantage: Tests species generalization, not colony generalization
    """
    from collections import defaultdict
    
    groups = defaultdict(list)
    
    for idx, label in enumerate(sample_labels):
        species = species_sample_map[idx]['species']
        groups[species].append(idx)
    
    print("="*80)
    print("SPECIES-LEVEL CV GROUPS (ALTERNATIVE APPROACH)")
    print("="*80)
    print(f"CV approach: Leave-one-SPECIES-out")
    print(f"  - Train on 2 species")
    print(f"  - Test on 1 held-out species")
    print(f"\nThis tests: Can the decomposition generalize to a NEW SPECIES?")
    print(f"\nSpecies groups:")
    
    for species, indices in sorted(groups.items()):
        print(f"  {species}: {len(indices)} colonies")
        colony_names = [species_sample_map[i]['sample_id'] for i in indices[:5]]
        print(f"    Colonies: {', '.join(colony_names)}{'...' if len(indices) > 5 else ''}")
    
    print(f"\nTotal CV folds: {len(groups)}")
    print(f"This is MUCH faster than colony-level CV")
    print("="*80)
    
    return dict(groups)

# Create species-level groups as well
species_groups = identify_species_groups_for_cv(sample_labels, species_sample_map)

print(f"\n{'='*80}")
print("RECOMMENDATION FOR CV APPROACH:")
print("="*80)
print("1. SPECIES-LEVEL CV (species_groups):")
print("   → Use this for PRACTICAL rank selection")
print("   → Fast: only 3 folds")
print("   → Tests: Does model work on new species?")
print("   → Similar to dissertation's leave-one-dataset-out approach")
print("")
print("2. COLONY-LEVEL CV (replicate_groups):")
print("   → Use for COMPREHENSIVE validation")
print("   → Slow: ~30-40 folds")
print("   → Tests: Does model work on new colony within same species?")
print("   → More traditional biological replicate CV")
print("")
print("SUGGESTED: Start with species-level CV for initial rank selection")
print("="*80)
```

### Cross-validated rank selection (Dissertation Method)

```{python cv-rank-selection-functions, eval=TRUE}
# ===========================================
# DISSERTATION-VALIDATED RANK SELECTION
# Implementation of Blaskowski (2024) Section 1.2.3
# ===========================================

def calculate_cv_sse(tensor, decomposition, train_indices, test_indices):
    """
    Calculate Sum of Squared Errors on test data.
    
    This reconstructs the tensor using factors learned from training data
    and evaluates SSE on held-out test data.
    
    Parameters:
    -----------
    tensor : 3D array
        Full tensor (genes × samples × timepoints)
    decomposition : barnacle decomposition object
        Fitted decomposition from training data
    train_indices : list
        Sample indices used for training
    test_indices : list
        Sample indices for test set
    
    Returns:
    --------
    sse : float
        Sum of squared errors on test data
    """
    # Extract factors from training
    gene_factors = decomposition.factors[0]  # genes × rank
    sample_factors_train = decomposition.factors[1]  # train_samples × rank
    time_factors = decomposition.factors[2]  # timepoints × rank
    weights = decomposition.weights  # rank
    
    # Get test tensor
    test_tensor = tensor[:, test_indices, :]
    n_genes, n_test_samples, n_timepoints = test_tensor.shape
    rank = len(weights)
    
    # For test samples, we need to fit sample factors while keeping gene and time factors fixed
    # This is a simplified approach: use mean of training sample factors as proxy
    # More sophisticated: solve least-squares for test sample factors
    sample_factors_test = np.tile(
        sample_factors_train.mean(axis=0, keepdims=True),
        (n_test_samples, 1)
    )
    
    # Reconstruct test tensor
    reconstructed = np.zeros(test_tensor.shape)
    for r in range(rank):
        # Outer product: genes × test_samples × times
        component = np.einsum('i,j,k->ijk',
                            gene_factors[:, r],
                            sample_factors_test[:, r],
                            time_factors[:, r])
        reconstructed += weights[r] * component
    
    # Calculate SSE
    sse = np.sum((test_tensor - reconstructed) ** 2)
    
    return sse


def cross_validated_rank_selection(
    tensor,
    replicate_groups,
    rank_range=[5, 10, 15, 20, 25, 30, 35, 40],
    lambdas=[0.0],
    n_iter_max=10000,
    random_state=42
):
    """
    Dissertation-validated rank selection via cross-validation.
    
    Implementation of Blaskowski (2024) Section 1.2.3:
    "We selected the R value of best fit based on the minimum cross-validated SSE"
    
    Method:
    1. For each rank R, perform leave-one-group-out cross-validation
    2. Fit model on training groups, evaluate SSE on test group
    3. Select rank with minimum mean CV-SSE
    
    Parameters:
    -----------
    tensor : 3D array
        Full tensor (genes × samples × timepoints)
    replicate_groups : dict
        Keys: group names (e.g., species)
        Values: list of sample indices
    rank_range : list
        Ranks to test
    lambdas : list
        Lambda values (use [0.0] for rank selection per dissertation)
    n_iter_max : int
        Maximum iterations for optimization
    random_state : int
        Random seed
    
    Returns:
    --------
    optimal_rank : int
        Rank with minimum CV-SSE
    cv_results_df : DataFrame
        Full cross-validation results
    """
    from barnacle.decomposition import SparseCP
    
    print("="*80)
    print("CROSS-VALIDATED RANK SELECTION")
    print("Dissertation Method (Blaskowski 2024, Section 1.2.3)")
    print("="*80)
    print(f"Testing ranks: {rank_range}")
    print(f"Lambda: {lambdas[0]} (no regularization for rank selection)")
    print(f"Cross-validation: Leave-one-group-out ({len(replicate_groups)} folds)")
    print(f"Random state: {random_state}")
    print("="*80)
    
    cv_results = []
    
    # Handle missing values once
    tensor_filled = np.nan_to_num(tensor, nan=0.0)
    
    # For each rank, do leave-one-group-out cross-validation
    for R in rank_range:
        print(f"\n{'='*60}")
        print(f"Testing Rank {R}")
        print(f"{'='*60}")
        
        fold_sse_scores = []
        fold_details = []
        
        # Leave-one-group-out CV
        for fold_idx, (group_name, test_indices) in enumerate(replicate_groups.items()):
            print(f"  Fold {fold_idx+1}/{len(replicate_groups)}: Hold out {group_name} ({len(test_indices)} samples)")
            
            # Create train/test split
            train_indices = []
            for other_name, other_indices in replicate_groups.items():
                if other_name != group_name:
                    train_indices.extend(other_indices)
            
            print(f"    Training on {len(train_indices)} samples")
            
            # Fit model on training data
            train_tensor = tensor_filled[:, train_indices, :]
            
            try:
                model = SparseCP(
                    rank=R,
                    lambdas=[lambdas[0], 0.0, lambdas[0]],
                    nonneg_modes=[0],
                    norm_constraint=True,
                    init='random',
                    tol=1e-5,
                    n_iter_max=n_iter_max,
                    random_state=random_state,
                    n_initializations=1
                )
                
                decomposition = model.fit_transform(train_tensor, verbose=0)
                
                # Calculate SSE on test data
                sse = calculate_cv_sse(tensor_filled, decomposition, train_indices, test_indices)
                fold_sse_scores.append(sse)
                
                # Get relative error for reference
                test_tensor = tensor_filled[:, test_indices, :]
                test_norm = np.linalg.norm(test_tensor)
                relative_error = np.sqrt(sse) / test_norm if test_norm > 0 else np.inf
                
                print(f"    SSE: {sse:.2e}, Relative Error: {relative_error:.4f}")
                
                fold_details.append({
                    'fold': fold_idx + 1,
                    'group': group_name,
                    'sse': sse,
                    'relative_error': relative_error,
                    'converged': True
                })
                
            except Exception as e:
                print(f"    Error: {e}")
                fold_sse_scores.append(np.nan)
                fold_details.append({
                    'fold': fold_idx + 1,
                    'group': group_name,
                    'sse': np.nan,
                    'relative_error': np.nan,
                    'converged': False
                })
        
        # Calculate mean and std of CV-SSE
        valid_scores = [s for s in fold_sse_scores if not np.isnan(s)]
        if len(valid_scores) > 0:
            mean_cv_sse = np.mean(valid_scores)
            std_cv_sse = np.std(valid_scores)
            min_cv_sse = np.min(valid_scores)
            max_cv_sse = np.max(valid_scores)
        else:
            mean_cv_sse = np.inf
            std_cv_sse = np.inf
            min_cv_sse = np.inf
            max_cv_sse = np.inf
        
        cv_results.append({
            'rank': R,
            'lambda': lambdas[0],
            'mean_cv_sse': mean_cv_sse,
            'std_cv_sse': std_cv_sse,
            'min_cv_sse': min_cv_sse,
            'max_cv_sse': max_cv_sse,
            'n_successful_folds': len(valid_scores),
            'fold_details': fold_details
        })
        
        print(f"  → Mean CV-SSE: {mean_cv_sse:.2e} ± {std_cv_sse:.2e}")
        print(f"  → Range: [{min_cv_sse:.2e}, {max_cv_sse:.2e}]")
        print(f"  → Successful folds: {len(valid_scores)}/{len(replicate_groups)}")
    
    # Convert to DataFrame
    cv_df = pd.DataFrame(cv_results)
    
    # Select optimal rank (minimum CV-SSE)
    if cv_df['mean_cv_sse'].isna().all():
        print("\n" + "="*80)
        print("ERROR: All ranks failed. Cannot select optimal rank.")
        print("="*80)
        return None, cv_df
    
    optimal_idx = cv_df['mean_cv_sse'].idxmin()
    optimal_rank = int(cv_df.loc[optimal_idx, 'rank'])
    optimal_sse = cv_df.loc[optimal_idx, 'mean_cv_sse']
    optimal_std = cv_df.loc[optimal_idx, 'std_cv_sse']
    
    print(f"\n{'='*80}")
    print("OPTIMAL RANK SELECTION RESULTS")
    print(f"{'='*80}")
    print(f"✓ Optimal Rank: {optimal_rank}")
    print(f"  CV-SSE: {optimal_sse:.2e} ± {optimal_std:.2e}")
    print(f"\nFull Results (sorted by CV-SSE):")
    display_df = cv_df[['rank', 'mean_cv_sse', 'std_cv_sse', 'n_successful_folds']].copy()
    display_df = display_df.sort_values('mean_cv_sse')
    print(display_df.to_string(index=False, float_format=lambda x: f'{x:.2e}'))
    print(f"{'='*80}")
    
    return optimal_rank, cv_df


print("Cross-validated rank selection functions loaded")
print("Ready to run dissertation-validated parameter selection")
```

### Run cross-validated rank selection

```{python run-cv-rank-selection, eval=FALSE}
# ===========================================
# RUN CROSS-VALIDATED RANK SELECTION
# ===========================================
# IMPLEMENTING BLASKOWSKI (2024) DISSERTATION METHOD
# ===========================================

print("="*80)
print("STARTING DISSERTATION-VALIDATED RANK SELECTION")
print("="*80)

# Configure parameters
CV_RANK_RANGE = [5, 10, 15, 20, 25, 35]  # Start with quick test, expand if needed
CV_LAMBDA = [0.0]  # No regularization for rank selection (per dissertation)
CV_RANDOM_STATE = 42

# ============================================
# CHOOSE CV APPROACH:
# ============================================
# Option 1: SPECIES-LEVEL CV (RECOMMENDED - Fast, 3 folds)
#   Tests: Can model generalize to new species?
#   Similar to dissertation's leave-one-dataset-out approach
USE_CV_GROUPS = species_groups  

# Option 2: COLONY-LEVEL CV (Comprehensive but slow, ~30-40 folds)
#   Tests: Can model generalize to new colony?
#   More traditional biological replicate validation
# USE_CV_GROUPS = replicate_groups  # Uncomment to use this instead

print(f"\nCV Configuration:")
print(f"  Approach: {'SPECIES-LEVEL' if len(USE_CV_GROUPS) == 3 else 'COLONY-LEVEL'}")
print(f"  Number of CV folds: {len(USE_CV_GROUPS)}")
print(f"  Ranks to test: {CV_RANK_RANGE}")
print(f"  Lambda: {CV_LAMBDA[0]} (no regularization)")
print(f"  Random state: {CV_RANDOM_STATE}")

if len(USE_CV_GROUPS) == 3:
    print(f"\n  Species-level CV: Training on 2 species, testing on 1")
    print(f"  This tests whether decomposition generalizes across species")
else:
    print(f"\n  Colony-level CV: Training on N-1 colonies, testing on 1")
    print(f"  WARNING: This will be computationally expensive!")
    estimated_time = len(USE_CV_GROUPS) * len(CV_RANK_RANGE) * 2  # rough estimate in minutes
    print(f"  Estimated time: ~{estimated_time} minutes")

print("="*80)

# Run cross-validation
optimal_rank_cv, cv_rank_results = cross_validated_rank_selection(
    tensor=tensor_3d,
    replicate_groups=USE_CV_GROUPS,  # Use the chosen CV groups (species or colony level)
    rank_range=CV_RANK_RANGE,
    lambdas=CV_LAMBDA,
    n_iter_max=10000,
    random_state=CV_RANDOM_STATE
)

# Save results
cv_results_dir = os.path.join(output_dir, 'dissertation_cv_results')
os.makedirs(cv_results_dir, exist_ok=True)

if optimal_rank_cv is not None:
    # Save CV results
    cv_rank_results.to_csv(
        os.path.join(cv_results_dir, 'cv_rank_selection_results.csv'),
        index=False
    )
    
    # Save optimal parameters
    import json
    
    cv_method_name = 'leave_one_species_out' if len(USE_CV_GROUPS) == 3 else 'leave_one_colony_out'
    
    with open(os.path.join(cv_results_dir, 'optimal_rank_cv.json'), 'w') as f:
        json.dump({
            'method': 'cross_validated_sse',
            'reference': 'Blaskowski 2024, Section 1.2.3, pg. 15',
            'optimal_rank': int(optimal_rank_cv),
            'criterion': 'minimum_cross_validated_sse',
            'cv_method': cv_method_name,
            'n_cv_folds': len(USE_CV_GROUPS),
            'cv_groups': list(USE_CV_GROUPS.keys()),
            'ranks_tested': CV_RANK_RANGE,
            'lambda_used': CV_LAMBDA[0],
            'random_state': CV_RANDOM_STATE,
            'note': 'Species-level CV tests generalization across species. Colony-level CV tests generalization to new colonies.',
            'timestamp': pd.Timestamp.now().isoformat()
        }, f, indent=2)
    
    print(f"\n✓ Results saved to: {cv_results_dir}")
    print(f"  - cv_rank_selection_results.csv")
    print(f"  - optimal_rank_cv.json")
    print(f"\n✓ OPTIMAL RANK (Dissertation Method): {optimal_rank_cv}")
    print(f"  CV approach used: {cv_method_name}")
else:
    print("\n✗ Cross-validation failed. Check error messages above.")

print("="*80)
```

### Plot Factor Match Score vs SSE (Dissertation Figure)

This recreates the diagnostic plots from the dissertation showing the relationship between Factor Match Score (FMS) and reconstruction error (SSE).

```{python plot-fms-vs-sse-cv, eval=FALSE}
# ===========================================
# FACTOR MATCH SCORE vs SSE ANALYSIS
# Recreates dissertation diagnostic plots
# ===========================================

import matplotlib.pyplot as plt
import numpy as np
from barnacle.decomposition import SparseCP
from tlviz.factor_tools import factor_match_score

print("="*80)
print("FACTOR MATCH SCORE vs SSE ANALYSIS")
print("Dissertation-style diagnostic plots")
print("="*80)

if 'cv_rank_results' not in locals() or cv_rank_results is None:
    print("CV results not available. Run run-cv-rank-selection first.")
else:
    # Initialize storage for FMS calculations
    fms_sse_data = []
    
    # For each rank tested, we'll calculate FMS between CV folds
    print(f"\nCalculating FMS between CV folds for each rank...")
    print(f"This shows consistency of factors across different training sets")
    
    for _, row in cv_rank_results.iterrows():
        rank = int(row['rank'])
        fold_details = row['fold_details']
        
        print(f"\n  Rank {rank}:")
        
        # Get successful folds
        successful_folds = [f for f in fold_details if f['converged']]
        
        if len(successful_folds) < 2:
            print(f"    Insufficient successful folds ({len(successful_folds)}) for FMS calculation")
            continue
        
        # We need to refit models for each fold to get the factors
        # Store decomposition objects from each fold
        fold_decompositions = []
        fold_sse = []
        
        for fold_info in successful_folds[:5]:  # Limit to first 5 folds to save time
            fold_idx = fold_info['fold']
            group_name = fold_info['group']
            sse = fold_info['sse']
            
            print(f"    Fold {fold_idx} ({group_name}): SSE = {sse:.2e}")
            
            # Get train indices for this fold
            train_indices = []
            for other_name, other_indices in USE_CV_GROUPS.items():
                if other_name != group_name:
                    train_indices.extend(other_indices)
            
            # Fit model on training data
            train_tensor = np.nan_to_num(tensor_3d[:, train_indices, :], nan=0.0)
            
            try:
                model = SparseCP(
                    rank=rank,
                    lambdas=[CV_LAMBDA[0], 0.0, CV_LAMBDA[0]],
                    nonneg_modes=[0],
                    norm_constraint=True,
                    init='random',
                    tol=1e-5,
                    n_iter_max=10000,
                    random_state=CV_RANDOM_STATE,
                    n_initializations=1
                )
                
                decomposition = model.fit_transform(train_tensor, verbose=0)
                
                # Store the decomposition object (has .factors and .weights)
                fold_decompositions.append(decomposition)
                fold_sse.append(sse)
                
            except Exception as e:
                print(f"      Error refitting: {e}")
                continue
        
        # Calculate pairwise FMS between folds
        if len(fold_decompositions) >= 2:
            fms_scores = []
            
            for i in range(len(fold_decompositions)):
                for j in range(i+1, len(fold_decompositions)):
                    try:
                        # Calculate FMS between decomposition objects
                        # FMS compares how similar two decompositions are
                        # Use return_permutation=False to get only the score
                        fms = factor_match_score(
                            fold_decompositions[i], 
                            fold_decompositions[j],
                            return_permutation=False
                        )
                        fms_scores.append(fms)
                    except Exception as e:
                        print(f"      Error calculating FMS: {e}")
                        continue
            
            if len(fms_scores) > 0:
                mean_fms = np.mean(fms_scores)
                std_fms = np.std(fms_scores)
                mean_sse = np.mean(fold_sse)
                
                fms_sse_data.append({
                    'rank': rank,
                    'mean_fms': mean_fms,
                    'std_fms': std_fms,
                    'mean_sse': mean_sse,
                    'n_comparisons': len(fms_scores)
                })
                
                print(f"    → Mean FMS: {mean_fms:.3f} ± {std_fms:.3f} ({len(fms_scores)} comparisons)")
            else:
                print(f"    → No FMS scores calculated")
        else:
            print(f"    → Insufficient factors for FMS calculation")
    
    # Create plots if we have data
    if len(fms_sse_data) > 0:
        fms_df = pd.DataFrame(fms_sse_data)
        
        # Create figure with two subplots
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # PLOT 1: FMS vs SSE (main dissertation plot)
        ax1 = axes[0]
        
        ax1.errorbar(fms_df['mean_sse'], fms_df['mean_fms'],
                    xerr=None,  # We don't have SSE std here
                    yerr=fms_df['std_fms'],
                    fmt='o', markersize=10, linewidth=2,
                    capsize=5, capthick=2,
                    color='steelblue', ecolor='lightblue')
        
        # Annotate points with rank
        for _, row in fms_df.iterrows():
            ax1.annotate(f"R={int(row['rank'])}", 
                        xy=(row['mean_sse'], row['mean_fms']),
                        xytext=(8, 8), textcoords='offset points',
                        fontsize=9, fontweight='bold',
                        bbox=dict(boxstyle='round,pad=0.3', 
                                facecolor='yellow', alpha=0.5))
        
        ax1.set_xlabel('Cross-Validated SSE', fontsize=12, fontweight='bold')
        ax1.set_ylabel('Mean Factor Match Score (FMS)', fontsize=12, fontweight='bold')
        ax1.set_title('FMS vs SSE: Model Consistency vs Fit\n(Dissertation-style diagnostic)', 
                     fontsize=13, fontweight='bold')
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim([0, 1.05])
        
        # Add interpretation text
        ax1.text(0.02, 0.02, 
                'High FMS = Consistent factors\nLow SSE = Good fit',
                transform=ax1.transAxes,
                fontsize=9, 
                verticalalignment='bottom',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        
        # PLOT 2: FMS and SSE vs Rank (dual y-axis)
        ax2 = axes[1]
        ax2_twin = ax2.twinx()
        
        # Sort by rank for line plot
        fms_df_sorted = fms_df.sort_values('rank')
        
        # Plot FMS
        line1 = ax2.plot(fms_df_sorted['rank'], fms_df_sorted['mean_fms'],
                        'o-', linewidth=2.5, markersize=8,
                        color='green', label='FMS (left axis)')
        ax2.fill_between(fms_df_sorted['rank'],
                        fms_df_sorted['mean_fms'] - fms_df_sorted['std_fms'],
                        fms_df_sorted['mean_fms'] + fms_df_sorted['std_fms'],
                        alpha=0.2, color='green')
        
        # Plot SSE
        line2 = ax2_twin.plot(fms_df_sorted['rank'], fms_df_sorted['mean_sse'],
                             's-', linewidth=2.5, markersize=8,
                             color='red', label='SSE (right axis)')
        
        ax2.set_xlabel('Rank', fontsize=12, fontweight='bold')
        ax2.set_ylabel('Factor Match Score (FMS)', fontsize=12, fontweight='bold', color='green')
        ax2_twin.set_ylabel('Cross-Validated SSE', fontsize=12, fontweight='bold', color='red')
        ax2.set_title('FMS and SSE vs Rank\n(Trade-off between consistency and fit)', 
                     fontsize=13, fontweight='bold')
        ax2.grid(True, alpha=0.3)
        ax2.set_ylim([0, 1.05])
        ax2.tick_params(axis='y', labelcolor='green')
        ax2_twin.tick_params(axis='y', labelcolor='red')
        
        # Mark optimal rank if available
        if 'optimal_rank_cv' in locals() and optimal_rank_cv is not None:
            if optimal_rank_cv in fms_df_sorted['rank'].values:
                ax2.axvline(x=optimal_rank_cv, color='purple', 
                           linestyle='--', linewidth=2, alpha=0.7,
                           label='Optimal Rank (CV)')
        
        # Combined legend
        lines = line1 + line2
        labels = [l.get_label() for l in lines]
        ax2.legend(lines, labels, loc='best', fontsize=10)
        
        plt.tight_layout()
        
        # Save figure
        fms_sse_plot_file = os.path.join(cv_results_dir, 'fms_vs_sse_diagnostic.png')
        plt.savefig(fms_sse_plot_file, dpi=300, bbox_inches='tight')
        print(f"\n✓ FMS vs SSE diagnostic plot saved to:")
        print(f"  {fms_sse_plot_file}")
        
        plt.show()
        plt.close()
        
        # Save FMS-SSE data
        fms_df.to_csv(os.path.join(cv_results_dir, 'fms_sse_data.csv'), index=False)
        print(f"✓ FMS-SSE data saved to fms_sse_data.csv")
        
        # Interpretation
        print(f"\n{'='*80}")
        print("INTERPRETATION:")
        print("="*80)
        print("From the dissertation (Blaskowski 2024):")
        print("  • High FMS = Decomposition produces consistent factors across CV folds")
        print("  • Low SSE = Better reconstruction of test data")
        print("  • Ideal rank: High FMS AND Low SSE")
        print("  • Trade-off: Sometimes lower rank has higher FMS but higher SSE")
        print("\nLook for:")
        print("  1. Ranks where FMS is high (> 0.7) and relatively stable")
        print("  2. SSE is minimized (lower-left corner in FMS vs SSE plot)")
        print("  3. Diminishing returns in both metrics at higher ranks")
        print("="*80)
        
    else:
        print("\n⚠ No FMS-SSE data available. Check if CV folds ran successfully.")

print("="*80)
```

### Lambda Selection with 1SE Rule (Step 2)

After determining the optimal rank, the next step is selecting the optimal lambda (regularization parameter) using cross-validated Factor Match Score (FMS) with the **1SE rule** from the dissertation.

**Dissertation Reference:** Blaskowski (2024), Section 1.2.3, page 15:
> "We selected the λ value of best fit as the maximum λ value at which the cross-validated FMS remained within one standard error of the maximum FMS."

```{python lambda-selection-1se-rule, eval=FALSE}
# ===========================================
# LAMBDA SELECTION WITH 1SE RULE
# Dissertation Step 2: Find optimal regularization
# ===========================================

from barnacle.decomposition import SparseCP
from tlviz.factor_tools import factor_match_score
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

print("="*80)
print("LAMBDA SELECTION WITH 1SE RULE")
print("Dissertation Method (Blaskowski 2024, Section 1.2.3)")
print("="*80)

# Check if we have an optimal rank
if 'optimal_rank_cv' not in locals() or optimal_rank_cv is None:
    print("ERROR: No optimal rank available. Run CV rank selection first.")
    print("Using default rank = 10 for demonstration")
    RANK_FOR_LAMBDA = 10
else:
    RANK_FOR_LAMBDA = optimal_rank_cv
    print(f"Using optimal rank from CV: {RANK_FOR_LAMBDA}")

# Define lambda values to test
# Start broad, then can refine based on results
LAMBDA_VALUES = [0.0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]

print(f"\nTesting {len(LAMBDA_VALUES)} lambda values:")
print(f"  {LAMBDA_VALUES}")
print(f"\nCross-validation approach: {len(USE_CV_GROUPS)}-fold CV")
print(f"Metric: Factor Match Score (FMS) between folds")
print("="*80)

# Storage for results
lambda_results = []

# For each lambda value, perform CV and calculate FMS
for lambda_val in LAMBDA_VALUES:
    print(f"\n{'='*60}")
    print(f"Testing lambda = {lambda_val}")
    print(f"{'='*60}")
    
    fold_decompositions = []
    fold_fms_scores = []
    
    # Perform leave-one-group-out CV
    for fold_idx, (group_name, test_indices) in enumerate(USE_CV_GROUPS.items()):
        print(f"  Fold {fold_idx+1}/{len(USE_CV_GROUPS)}: Hold out {group_name}")
        
        # Create train/test split
        train_indices = []
        for other_name, other_indices in USE_CV_GROUPS.items():
            if other_name != group_name:
                train_indices.extend(other_indices)
        
        # Fit model on training data
        train_tensor = np.nan_to_num(tensor_3d[:, train_indices, :], nan=0.0)
        
        try:
            model = SparseCP(
                rank=RANK_FOR_LAMBDA,
                lambdas=[lambda_val, 0.0, lambda_val],  # Apply to gene and time factors
                nonneg_modes=[0],
                norm_constraint=True,
                init='random',
                tol=1e-5,
                n_iter_max=10000,
                random_state=CV_RANDOM_STATE,
                n_initializations=1
            )
            
            decomposition = model.fit_transform(train_tensor, verbose=0)
            fold_decompositions.append(decomposition)
            
            print(f"    ✓ Converged in {model.n_iter_} iterations")
            
        except Exception as e:
            print(f"    ✗ Error: {e}")
            fold_decompositions.append(None)
    
    # Calculate pairwise FMS between successful folds
    successful_decompositions = [d for d in fold_decompositions if d is not None]
    
    if len(successful_decompositions) >= 2:
        fms_scores = []
        
        for i in range(len(successful_decompositions)):
            for j in range(i+1, len(successful_decompositions)):
                try:
                    # Use return_permutation=False to get only the score
                    fms = factor_match_score(
                        successful_decompositions[i], 
                        successful_decompositions[j],
                        return_permutation=False
                    )
                    fms_scores.append(fms)
                except Exception as e:
                    print(f"    Warning: FMS calculation failed: {e}")
                    continue
        
        if len(fms_scores) > 0:
            mean_fms = np.mean(fms_scores)
            std_fms = np.std(fms_scores)
            se_fms = std_fms / np.sqrt(len(fms_scores))  # Standard error
            
            lambda_results.append({
                'lambda': lambda_val,
                'mean_fms': mean_fms,
                'std_fms': std_fms,
                'se_fms': se_fms,
                'n_comparisons': len(fms_scores),
                'n_successful_folds': len(successful_decompositions)
            })
            
            print(f"  → Mean FMS: {mean_fms:.4f} ± {std_fms:.4f} (SE = {se_fms:.4f})")
            print(f"     {len(fms_scores)} pairwise comparisons from {len(successful_decompositions)} folds")
        else:
            print(f"  → No FMS scores calculated")
    else:
        print(f"  → Insufficient successful folds: {len(successful_decompositions)}")

# Convert to DataFrame
lambda_df = pd.DataFrame(lambda_results)

if len(lambda_df) > 0:
    print(f"\n{'='*80}")
    print("APPLYING 1SE RULE")
    print("="*80)
    
    # Find maximum FMS
    max_fms_idx = lambda_df['mean_fms'].idxmax()
    max_fms = lambda_df.loc[max_fms_idx, 'mean_fms']
    max_fms_se = lambda_df.loc[max_fms_idx, 'se_fms']
    
    # Calculate 1SE threshold
    fms_1se_threshold = max_fms - max_fms_se
    
    print(f"Maximum FMS: {max_fms:.4f} (at lambda = {lambda_df.loc[max_fms_idx, 'lambda']})")
    print(f"Standard Error: {max_fms_se:.4f}")
    print(f"1SE Threshold: {fms_1se_threshold:.4f}")
    
    # Find maximum lambda within 1SE of maximum FMS
    within_1se = lambda_df[lambda_df['mean_fms'] >= fms_1se_threshold]
    
    if len(within_1se) > 0:
        optimal_lambda_idx = within_1se['lambda'].idxmax()
        optimal_lambda = within_1se.loc[optimal_lambda_idx, 'lambda']
        optimal_fms = within_1se.loc[optimal_lambda_idx, 'mean_fms']
        
        print(f"\n{'='*80}")
        print("OPTIMAL LAMBDA SELECTED")
        print("="*80)
        print(f"✓ Optimal Lambda: {optimal_lambda}")
        print(f"  FMS: {optimal_fms:.4f}")
        print(f"  (Maximum lambda where FMS ≥ {fms_1se_threshold:.4f})")
        print(f"\nInterpretation:")
        print(f"  • Provides most regularization (sparsity)")
        print(f"  • While maintaining factor quality within 1SE of max")
        print(f"  • Follows dissertation's validated approach")
        print("="*80)
        
        # Save results
        lambda_dir = os.path.join(output_dir, 'lambda_selection')
        os.makedirs(lambda_dir, exist_ok=True)
        
        lambda_df.to_csv(os.path.join(lambda_dir, 'lambda_cv_results.csv'), index=False)
        
        import json
        with open(os.path.join(lambda_dir, 'optimal_lambda.json'), 'w') as f:
            json.dump({
                'method': '1se_rule',
                'reference': 'Blaskowski 2024, Section 1.2.3, pg. 15',
                'optimal_lambda': float(optimal_lambda),
                'optimal_fms': float(optimal_fms),
                'max_fms': float(max_fms),
                'fms_1se_threshold': float(fms_1se_threshold),
                'rank_used': int(RANK_FOR_LAMBDA),
                'lambdas_tested': LAMBDA_VALUES,
                'cv_method': 'leave_one_group_out',
                'n_cv_folds': len(USE_CV_GROUPS),
                'timestamp': pd.Timestamp.now().isoformat()
            }, f, indent=2)
        
        print(f"\n✓ Results saved to: {lambda_dir}")
        
        # Create plot
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # Plot 1: FMS vs Lambda
        ax1 = axes[0]
        ax1.errorbar(lambda_df['lambda'], lambda_df['mean_fms'],
                    yerr=lambda_df['std_fms'],
                    marker='o', markersize=10, linewidth=2.5,
                    capsize=5, capthick=2,
                    color='steelblue', ecolor='lightblue')
        
        # Mark optimal lambda
        ax1.axvline(x=optimal_lambda, color='red', linestyle='--', 
                   linewidth=2, alpha=0.7, label=f'Optimal λ = {optimal_lambda}')
        ax1.axhline(y=fms_1se_threshold, color='orange', linestyle='--',
                   linewidth=2, alpha=0.7, label=f'1SE threshold = {fms_1se_threshold:.3f}')
        ax1.axhline(y=max_fms, color='green', linestyle='-',
                   linewidth=1.5, alpha=0.5, label=f'Max FMS = {max_fms:.3f}')
        
        ax1.plot(optimal_lambda, optimal_fms, '*', markersize=20, 
                color='red', markeredgecolor='darkred', markeredgewidth=2, zorder=10)
        
        ax1.set_xlabel('Lambda (λ)', fontsize=12, fontweight='bold')
        ax1.set_ylabel('Mean Factor Match Score (FMS)', fontsize=12, fontweight='bold')
        ax1.set_title('Lambda Selection via 1SE Rule\n(Dissertation Method)', 
                     fontsize=13, fontweight='bold')
        ax1.set_xscale('log')
        ax1.grid(True, alpha=0.3)
        ax1.legend(fontsize=10)
        ax1.set_ylim([0, 1.05])
        
        # Plot 2: Explanation
        ax2 = axes[1]
        ax2.axis('off')
        
        summary = "1SE RULE EXPLANATION\n"
        summary += "="*40 + "\n\n"
        summary += f"Rank: {RANK_FOR_LAMBDA}\n"
        summary += f"Optimal Lambda: {optimal_lambda}\n"
        summary += f"Optimal FMS: {optimal_fms:.4f}\n\n"
        summary += "Method:\n"
        summary += "1. Find max FMS across lambdas\n"
        summary += "2. Calculate 1 standard error\n"
        summary += "3. Select largest lambda where\n"
        summary += "   FMS ≥ (max FMS - 1SE)\n\n"
        summary += "Why this works:\n"
        summary += "• Larger lambda = more sparsity\n"
        summary += "• 1SE rule balances sparsity\n"
        summary += "  with factor quality\n"
        summary += "• Validated on 100 simulations\n"
        summary += "  in dissertation\n\n"
        summary += "Next step:\n"
        summary += "Run final decomposition with:\n"
        summary += f"  Rank = {RANK_FOR_LAMBDA}\n"
        summary += f"  Lambda = {optimal_lambda}"
        
        ax2.text(0.05, 0.95, summary,
                transform=ax2.transAxes,
                verticalalignment='top',
                fontsize=11,
                family='monospace',
                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.4))
        
        plt.tight_layout()
        plt.savefig(os.path.join(lambda_dir, 'lambda_selection_1se_rule.png'), 
                   dpi=300, bbox_inches='tight')
        print(f"✓ Plot saved: lambda_selection_1se_rule.png")
        
        plt.show()
        plt.close()
        
    else:
        print("\n⚠ No lambda values satisfy 1SE criterion")
        optimal_lambda = None
else:
    print("\n⚠ No lambda results available")
    optimal_lambda = None

print("="*80)
```

## Rank Comparison

**This workflow systematically compares multiple ranks to find the optimal number of components.**

The comparison runs decompositions for each specified rank and generates:
1. Metrics comparison table
2. Elbow plot (variance explained vs rank)
3. Multi-panel summary plots
4. Automated recommendation

**Results:** Each rank is saved to `rank_XX/` directory with complete factor matrices.

### Define helper functions for rank comparison

```{python rank-comparison-functions, eval=TRUE}
def run_single_rank_decomposition(tensor, rank, lambdas, n_iter_max, random_state=42):
    """
    Run sparse CP decomposition for a given rank.
    Returns dict with results and metrics.
    """
    from barnacle.decomposition import SparseCP
    
    print(f"\n{'='*60}")
    print(f"Running decomposition with rank = {rank}")
    print(f"{'='*60}")
    
    # Handle missing values
    tensor_filled = np.nan_to_num(tensor, nan=0.0)
    
    # Create model
    model = SparseCP(
        rank=rank,
        lambdas=lambdas,
        nonneg_modes=[0],
        norm_constraint=True,
        init='random',
        tol=1e-5,
        n_iter_max=n_iter_max,
        random_state=random_state,
        n_initializations=1
    )
    
    # Fit model
    try:
        decomposition = model.fit_transform(tensor_filled, verbose=0)
        
        # Calculate metrics
        metrics = calculate_decomposition_metrics(tensor_filled, decomposition, model)
        
        results = {
            'rank': rank,
            'decomposition': decomposition,
            'model': model,
            'metrics': metrics,
            'success': True
        }
        
        print(f"✓ Rank {rank} complete")
        print(f"  Final loss: {metrics['final_loss']:.6f}")
        print(f"  Variance explained: {metrics['variance_explained']*100:.2f}%")
        print(f"  Converged: {metrics['converged']}")
        
        return results
        
    except Exception as e:
        print(f"✗ Rank {rank} failed: {e}")
        return {
            'rank': rank,
            'success': False,
            'error': str(e)
        }


def calculate_decomposition_metrics(tensor, decomposition, model):
    """
    Calculate reconstruction quality and other metrics.
    """
    # Reconstruct tensor
    gene_factors = decomposition.factors[0]
    sample_factors = decomposition.factors[1]
    time_factors = decomposition.factors[2]
    weights = decomposition.weights
    
    rank = len(weights)
    reconstructed = np.zeros_like(tensor)
    
    for r in range(rank):
        outer_prod = np.outer(gene_factors[:, r], sample_factors[:, r])
        outer_prod = np.outer(outer_prod.flatten(), time_factors[:, r])
        outer_prod = outer_prod.reshape(tensor.shape)
        reconstructed += weights[r] * outer_prod
    
    # Calculate errors
    mse = np.mean((tensor - reconstructed) ** 2)
    rmse = np.sqrt(mse)
    
    # Relative error
    tensor_norm = np.linalg.norm(tensor)
    error_norm = np.linalg.norm(tensor - reconstructed)
    relative_error = error_norm / tensor_norm if tensor_norm > 0 else np.inf
    
    # Variance explained
    total_var = np.var(tensor)
    residual_var = np.var(tensor - reconstructed)
    variance_explained = 1 - (residual_var / total_var) if total_var > 0 else 0
    
    # Sparsity metrics
    gene_sparsity = np.mean(np.abs(gene_factors) < 0.1)
    sample_sparsity = np.mean(np.abs(sample_factors) < 0.1)
    time_sparsity = np.mean(np.abs(time_factors) < 0.1)
    
    # Component weight statistics
    weight_max = np.max(weights)
    weight_min = np.min(weights)
    weight_ratio = weight_max / weight_min if weight_min > 0 else np.inf
    weight_cv = np.std(weights) / np.mean(weights) if np.mean(weights) > 0 else np.inf
    
    # Convergence info
    # Note: Barnacle's SparseCP class does not have a converged_ attribute
    # We determine convergence by checking if iterations < max_iter
    n_iterations = len(model.loss_) if hasattr(model, 'loss_') and model.loss_ is not None else 0
    final_loss = model.loss_[-1] if hasattr(model, 'loss_') and model.loss_ is not None else np.nan
    # Converged if stopped before reaching max iterations (met tolerance criterion)
    converged = (n_iterations < model.n_iter_max) if n_iterations > 0 else False
    
    return {
        'mse': mse,
        'rmse': rmse,
        'relative_error': relative_error,
        'variance_explained': variance_explained,
        'gene_sparsity': gene_sparsity,
        'sample_sparsity': sample_sparsity,
        'time_sparsity': time_sparsity,
        'weight_max': weight_max,
        'weight_min': weight_min,
        'weight_ratio': weight_ratio,
        'weight_cv': weight_cv,
        'converged': converged,
        'n_iterations': n_iterations,
        'final_loss': final_loss
    }


def save_rank_results(results, output_dir, gene_ids, sample_labels, timepoint_labels, species_sample_map):
    """
    Save decomposition results for a specific rank.
    """
    rank = results['rank']
    rank_dir = os.path.join(output_dir, f'rank_{rank:02d}')
    os.makedirs(rank_dir, exist_ok=True)
    
    decomposition = results['decomposition']
    metrics = results['metrics']
    
    # Save factor matrices
    gene_factors = pd.DataFrame(
        decomposition.factors[0],
        index=gene_ids,
        columns=[f'Component_{i+1}' for i in range(rank)]
    )
    gene_factors.to_csv(os.path.join(rank_dir, 'gene_factors.csv'))
    
    sample_factors = pd.DataFrame(
        decomposition.factors[1],
        index=sample_labels,
        columns=[f'Component_{i+1}' for i in range(rank)]
    )
    sample_factors['Species'] = [species_sample_map[i]['species'] for i in range(len(sample_labels))]
    sample_factors['Sample_ID'] = [species_sample_map[i]['sample_id'] for i in range(len(sample_labels))]
    sample_factors.to_csv(os.path.join(rank_dir, 'sample_factors.csv'))
    
    time_factors = pd.DataFrame(
        decomposition.factors[2],
        index=timepoint_labels,
        columns=[f'Component_{i+1}' for i in range(rank)]
    )
    time_factors.to_csv(os.path.join(rank_dir, 'time_factors.csv'))
    
    # Save weights
    weights_df = pd.DataFrame({
        'Component': [f'Component_{i+1}' for i in range(rank)],
        'Weight': decomposition.weights
    })
    weights_df.to_csv(os.path.join(rank_dir, 'component_weights.csv'), index=False)
    
    # Save loss history
    if hasattr(results['model'], 'loss_') and results['model'].loss_ is not None:
        loss_df = pd.DataFrame({
            'Iteration': list(range(1, len(results['model'].loss_) + 1)),
            'Loss': results['model'].loss_
        })
        loss_df.to_csv(os.path.join(rank_dir, 'loss_history.csv'), index=False)
    
    # Save metadata
    metadata = {
        'rank': rank,
        'metrics': {k: float(v) if not isinstance(v, bool) else v for k, v in metrics.items()},
        'tensor_shape': list(decomposition.factors[0].shape[0:1]) + 
                       [decomposition.factors[1].shape[0]] + 
                       [decomposition.factors[2].shape[0]]
    }
    
    import json
    with open(os.path.join(rank_dir, 'metadata.json'), 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"  Saved results to: {rank_dir}")


print("Rank comparison helper functions loaded")
```

### Run barnacle

```{python barnacle-multiple-ranks, eval=TRUE}
# ===========================================
# CONFIGURE RANKS TO TEST
# ===========================================
# NOTE: This chunk creates a machine-readable log file
# (rank_comparison_log.json) that subsequent chunks will use.
# This allows you to skip re-running this chunk if you've
# already generated results.
# ===========================================
ranks_to_test = [5, 8, 10, 12, 15, 20, 25, 35, 45, 55, 65, 75]  # Modify this list as needed
# ===========================================

import os
import json
import datetime

print("="*60)
print("COMPARING MULTIPLE RANKS")
print("="*60)
print(f"Ranks to test: {ranks_to_test}")
print(f"This will run {len(ranks_to_test)} decompositions")
print(f"Estimated time: {len(ranks_to_test) * 2}-{len(ranks_to_test) * 5} minutes")
print("="*60)

# Run decompositions for each rank
all_rank_results = []

for test_rank in sorted(ranks_to_test):
    result = run_single_rank_decomposition(
        tensor=tensor_3d,
        rank=test_rank,
        lambdas=[0.1, 0.0, 0.1],
        n_iter_max=10000,
        random_state=41
    )
    all_rank_results.append(result)
    
    # Save individual results
    if result['success']:
        save_rank_results(
            result,
            output_dir,
            common_genes_list,
            sample_labels,
            [f'TP{tp}' for tp in common_timepoints],
            species_sample_map
        )

print(f"\n{'='*60}")
print(f"ALL RANKS COMPLETE")
print(f"{'='*60}")
print(f"Successful: {sum(1 for r in all_rank_results if r['success'])}/{len(all_rank_results)}")

# ===========================================
# SAVE COMPREHENSIVE LOG FILE
# ===========================================
import json
import datetime

log_file = os.path.join(output_dir, 'rank_comparison_log.json')

log_data = {
    'timestamp': datetime.datetime.now().isoformat(),
    'ranks_tested': ranks_to_test,
    'total_runs': len(all_rank_results),
    'successful_runs': sum(1 for r in all_rank_results if r['success']),
    'failed_runs': sum(1 for r in all_rank_results if not r['success']),
    'results': []
}

for result in all_rank_results:
    rank_info = {
        'rank': result['rank'],
        'success': result['success']
    }
    
    if result['success']:
        rank_info['metrics'] = {
            k: float(v) if isinstance(v, (int, float, np.number)) else bool(v) if isinstance(v, (bool, np.bool_)) else v
            for k, v in result['metrics'].items()
        }
        rank_info['output_dir'] = os.path.join(output_dir, f'rank_{result["rank"]:02d}')
        rank_info['has_loss_history'] = hasattr(result['model'], 'loss_') and result['model'].loss_ is not None
    else:
        rank_info['error'] = result.get('error', 'Unknown error')
    
    log_data['results'].append(rank_info)

with open(log_file, 'w') as f:
    json.dump(log_data, f, indent=2)

print(f"\nLog file saved to: {log_file}")
print("This log can be used by subsequent chunks to avoid re-running decompositions")
```

### Run barnacle with zero regularization (lambda = 0.0)

This chunk runs barnacle decomposition with all lambdas set to 0.0 (no regularization) to test how the model performs without sparsity constraints. Results are saved to a separate subdirectory for comparison.

```{python barnacle-zero-lambda, eval=TRUE}
# ===========================================
# CONFIGURE RANKS TO TEST - ZERO LAMBDA
# ===========================================
# This chunk tests decomposition with NO regularization
# All lambda values are set to 0.0
# ===========================================
ranks_to_test_zero_lambda = [10, 20, 35, 40]  # Specific ranks to test
# ===========================================

import os
import json
import datetime

# Create separate output directory for zero-lambda results
zero_lambda_output_dir = os.path.join(output_dir, 'zero_lambda_results')
os.makedirs(zero_lambda_output_dir, exist_ok=True)

print("="*60)
print("BARNACLE DECOMPOSITION - ZERO REGULARIZATION")
print("="*60)
print(f"Ranks to test: {ranks_to_test_zero_lambda}")
print(f"Lambda values: [0.0, 0.0, 0.0] (no regularization)")
print(f"Output directory: {zero_lambda_output_dir}")
print(f"This will run {len(ranks_to_test_zero_lambda)} decompositions")
print(f"Estimated time: {len(ranks_to_test_zero_lambda) * 2}-{len(ranks_to_test_zero_lambda) * 5} minutes")
print("="*60)

# Run decompositions for each rank with zero lambda
all_rank_results_zero_lambda = []

for test_rank in sorted(ranks_to_test_zero_lambda):
    result = run_single_rank_decomposition(
        tensor=tensor_3d,
        rank=test_rank,
        lambdas=[0.0, 0.0, 0.0],  # All lambdas set to 0.0
        n_iter_max=10000,
        random_state=41
    )
    all_rank_results_zero_lambda.append(result)
    
    # Save individual results
    if result['success']:
        save_rank_results(
            result,
            zero_lambda_output_dir,
            common_genes_list,
            sample_labels,
            [f'TP{tp}' for tp in common_timepoints],
            species_sample_map
        )

print(f"\n{'='*60}")
print(f"ZERO LAMBDA RANKS COMPLETE")
print(f"{'='*60}")
print(f"Successful: {sum(1 for r in all_rank_results_zero_lambda if r['success'])}/{len(all_rank_results_zero_lambda)}")

# ===========================================
# SAVE COMPREHENSIVE LOG FILE FOR ZERO LAMBDA
# ===========================================
import json
import datetime

log_file_zero_lambda = os.path.join(zero_lambda_output_dir, 'rank_comparison_log_zero_lambda.json')

log_data_zero_lambda = {
    'timestamp': datetime.datetime.now().isoformat(),
    'lambda_values': [0.0, 0.0, 0.0],
    'ranks_tested': ranks_to_test_zero_lambda,
    'total_runs': len(all_rank_results_zero_lambda),
    'successful_runs': sum(1 for r in all_rank_results_zero_lambda if r['success']),
    'failed_runs': sum(1 for r in all_rank_results_zero_lambda if not r['success']),
    'results': []
}

for result in all_rank_results_zero_lambda:
    rank_info = {
        'rank': result['rank'],
        'success': result['success']
    }
    
    if result['success']:
        rank_info['metrics'] = {
            k: float(v) if isinstance(v, (int, float, np.number)) else bool(v) if isinstance(v, (bool, np.bool_)) else v
            for k, v in result['metrics'].items()
        }
        rank_info['output_dir'] = os.path.join(zero_lambda_output_dir, f'rank_{result["rank"]:02d}')
        rank_info['has_loss_history'] = hasattr(result['model'], 'loss_') and result['model'].loss_ is not None
    else:
        rank_info['error'] = result.get('error', 'Unknown error')
    
    log_data_zero_lambda['results'].append(rank_info)

with open(log_file_zero_lambda, 'w') as f:
    json.dump(log_data_zero_lambda, f, indent=2)

print(f"\nLog file saved to: {log_file_zero_lambda}")
print(f"Results directory: {zero_lambda_output_dir}")
print("This log can be used to compare zero-lambda results with regularized decompositions")
```

### Plot Ranks vs Variance Explained (Zero Lambda Results)

This chunk creates a visualization showing the relationship between rank and variance explained for the zero-lambda decompositions, helping to identify the optimal rank based on the variance explained metric.

```{python plot-ranks-vs-variance-zero-lambda, eval=TRUE}
# ===========================================
# PLOT RANKS VS VARIANCE EXPLAINED - ZERO LAMBDA
# ===========================================
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

print(f"\n{'='*60}")
print("PLOTTING RANKS VS VARIANCE EXPLAINED (ZERO LAMBDA)")
print(f"{'='*60}")

# Extract data from zero-lambda results
ranks_zero_lambda = []
variance_explained_zero_lambda = []
successful_results = []

for result in all_rank_results_zero_lambda:
    if result['success'] and 'metrics' in result:
        ranks_zero_lambda.append(result['rank'])
        variance_explained_zero_lambda.append(result['metrics']['variance_explained'])
        successful_results.append(result)

if len(successful_results) == 0:
    print("WARNING: No successful decompositions found for zero-lambda results!")
    print("Cannot create variance plot.")
else:
    # Sort by rank for plotting
    sorted_data = sorted(zip(ranks_zero_lambda, variance_explained_zero_lambda))
    ranks_zero_lambda, variance_explained_zero_lambda = zip(*sorted_data)
    
    # Create the plot
    plt.figure(figsize=(10, 6))
    plt.plot(ranks_zero_lambda, [v*100 for v in variance_explained_zero_lambda], 
             'o-', linewidth=2.5, markersize=8, color='steelblue', markerfacecolor='lightblue', markeredgecolor='steelblue', markeredgewidth=2)
    
    # Customize the plot
    plt.xlabel('Rank', fontsize=12, fontweight='bold')
    plt.ylabel('Variance Explained (%)', fontsize=12, fontweight='bold')
    plt.title('Variance Explained vs Rank', fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.xticks(ranks_zero_lambda)
    
    # Add value labels on points
    for rank, var_exp in zip(ranks_zero_lambda, variance_explained_zero_lambda):
        plt.annotate(f'{var_exp*100:.1f}%', 
                    (rank, var_exp*100), 
                    textcoords="offset points", 
                    xytext=(0,10), 
                    ha='center',
                    fontsize=10,
                    fontweight='bold')
    
    # Formatting
    plt.ylim(bottom=0)
    plt.tight_layout()
    
    # Save the plot
    plot_file = os.path.join(zero_lambda_output_dir, 'ranks_vs_variance_explained_zero_lambda.png')
    plt.savefig(plot_file, dpi=300, bbox_inches='tight')
    print(f"Plot saved to: {plot_file}")
    
    plt.show()
    
    # Print summary statistics
    print(f"\nSUMMARY - Zero Lambda Results:")
    print(f"{'='*50}")
    print(f"Number of successful decompositions: {len(successful_results)}")
    print(f"Ranks tested: {sorted(ranks_zero_lambda)}")
    print(f"Variance explained range: {min(variance_explained_zero_lambda)*100:.1f}% - {max(variance_explained_zero_lambda)*100:.1f}%")
    
    # Find best rank
    best_idx = np.argmax(variance_explained_zero_lambda)
    best_rank = ranks_zero_lambda[best_idx]
    best_variance = variance_explained_zero_lambda[best_idx]
    
    print(f"Best performing rank: {best_rank} (variance explained: {best_variance*100:.1f}%)")
    
    # Create a DataFrame for easy inspection
    results_df = pd.DataFrame({
        'Rank': ranks_zero_lambda,
        'Variance_Explained_Percent': [v*100 for v in variance_explained_zero_lambda]
    })
    
    print(f"\nDetailed Results:")
    print(results_df.to_string(index=False))

print(f"{'='*60}")
```

### Run barnacle across multiple random states

This chunk runs barnacle decompositions for multiple random states to enable stability analysis of optimal rank selection. Each random state gets its own subdirectory with full decomposition results.

**Purpose**: Generate decompositions needed for multi-random-state FMS evaluation.

```{python barnacle-multiple-random-states, eval=TRUE}
# ===========================================
# RUN DECOMPOSITIONS ACROSS MULTIPLE RANDOM STATES
# ===========================================
# This chunk runs the same ranks across different random states
# to test stability of optimal rank selection via Synthetic FMS
# ===========================================

import os
import json
import datetime

print("="*60)
print("BARNACLE DECOMPOSITION - MULTIPLE RANDOM STATES")
print("="*60)

# ===========================================
# CONFIGURATION
# ===========================================
RANDOM_STATES_TO_TEST = [41, 42, 43, 44, 45]  # 5 different random states
# Use the same ranks that were tested in the main decomposition
ranks_to_test = [5, 8, 10, 12, 15, 20, 25, 35, 45, 55, 65, 75]
# ===========================================

print(f"Random states to test: {RANDOM_STATES_TO_TEST}")
print(f"Ranks to test: {ranks_to_test}")
print(f"Total decompositions: {len(RANDOM_STATES_TO_TEST)} states × {len(ranks_to_test)} ranks = {len(RANDOM_STATES_TO_TEST) * len(ranks_to_test)}")
print(f"Estimated time: {len(RANDOM_STATES_TO_TEST) * len(ranks_to_test) * 2}-{len(RANDOM_STATES_TO_TEST) * len(ranks_to_test) * 5} minutes")
print("="*60)

# Track results for all random states
all_random_state_results = {}

for random_state in RANDOM_STATES_TO_TEST:
    print(f"\n{'='*60}")
    print(f"RANDOM STATE: {random_state}")
    print(f"{'='*60}")
    
    # Create subdirectory for this random state
    state_output_dir = os.path.join(output_dir, f'random_state_{random_state}')
    os.makedirs(state_output_dir, exist_ok=True)
    
    print(f"Output directory: {state_output_dir}")
    
    # Run decompositions for each rank
    state_rank_results = []
    
    for test_rank in sorted(ranks_to_test):
        result = run_single_rank_decomposition(
            tensor=tensor_3d,
            rank=test_rank,
            lambdas=[0.1, 0.0, 0.1],
            n_iter_max=10000,
            random_state=random_state
        )
        state_rank_results.append(result)
        
        # Save individual results
        if result['success']:
            save_rank_results(
                result,
                state_output_dir,
                common_genes_list,
                sample_labels,
                [f'TP{tp}' for tp in common_timepoints],
                species_sample_map
            )
    
    # Store results for this random state
    all_random_state_results[random_state] = state_rank_results
    
    # Print summary for this random state
    successful = sum(1 for r in state_rank_results if r['success'])
    print(f"\nRandom State {random_state} Complete:")
    print(f"  Successful: {successful}/{len(state_rank_results)}")
    print(f"  Failed: {len(state_rank_results) - successful}/{len(state_rank_results)}")
    
    # Save log file for this random state
    log_file_state = os.path.join(state_output_dir, f'rank_comparison_log_rs{random_state}.json')
    
    log_data_state = {
        'timestamp': datetime.datetime.now().isoformat(),
        'random_state': random_state,
        'ranks_tested': ranks_to_test,
        'total_runs': len(state_rank_results),
        'successful_runs': successful,
        'failed_runs': len(state_rank_results) - successful,
        'results': []
    }
    
    for result in state_rank_results:
        rank_info = {
            'rank': result['rank'],
            'success': result['success']
        }
        
        if result['success']:
            rank_info['metrics'] = {
                k: float(v) if isinstance(v, (int, float, np.number)) else bool(v) if isinstance(v, (bool, np.bool_)) else v
                for k, v in result['metrics'].items()
            }
            rank_info['output_dir'] = os.path.join(state_output_dir, f'rank_{result["rank"]:02d}')
            rank_info['has_loss_history'] = hasattr(result['model'], 'loss_') and result['model'].loss_ is not None
        else:
            rank_info['error'] = result.get('error', 'Unknown error')
        
        log_data_state['results'].append(rank_info)
    
    with open(log_file_state, 'w') as f:
        json.dump(log_data_state, f, indent=2)
    
    print(f"  Log file saved to: {log_file_state}")

# ===========================================
# SAVE COMPREHENSIVE SUMMARY FOR ALL RANDOM STATES
# ===========================================
print(f"\n{'='*60}")
print("ALL RANDOM STATES COMPLETE")
print(f"{'='*60}")

# Calculate overall statistics
total_decompositions = sum(len(results) for results in all_random_state_results.values())
total_successful = sum(sum(1 for r in results if r['success']) for results in all_random_state_results.values())
total_failed = total_decompositions - total_successful

print(f"Total decompositions: {total_decompositions}")
print(f"Successful: {total_successful}/{total_decompositions} ({total_successful/total_decompositions*100:.1f}%)")
print(f"Failed: {total_failed}/{total_decompositions} ({total_failed/total_decompositions*100:.1f}%)")

print(f"\nResults by Random State:")
for random_state in sorted(all_random_state_results.keys()):
    results = all_random_state_results[random_state]
    successful = sum(1 for r in results if r['success'])
    print(f"  Random State {random_state}: {successful}/{len(results)} successful")

# Save master summary file
master_summary_file = os.path.join(output_dir, 'multi_random_state_summary.json')

master_summary = {
    'timestamp': datetime.datetime.now().isoformat(),
    'random_states_tested': RANDOM_STATES_TO_TEST,
    'ranks_tested': ranks_to_test,
    'total_decompositions': total_decompositions,
    'successful_decompositions': total_successful,
    'failed_decompositions': total_failed,
    'by_random_state': {}
}

for random_state, results in all_random_state_results.items():
    successful = sum(1 for r in results if r['success'])
    master_summary['by_random_state'][str(random_state)] = {
        'total': len(results),
        'successful': successful,
        'failed': len(results) - successful,
        'output_dir': os.path.join(output_dir, f'random_state_{random_state}')
    }

with open(master_summary_file, 'w') as f:
    json.dump(master_summary, f, indent=2)

print(f"\nMaster summary saved to: {master_summary_file}")
print(f"\n{'='*60}")
print("Ready for Factor Match Score evaluation across random states!")
print(f"{'='*60}")
```

### Factor Match Score evaluation (Multiple Random States)

This section evaluates decomposition quality using Synthetic FMS across multiple random states to assess stability of optimal rank selection.

**Key Question**: Does the synthetic FMS elbow plot consistently identify the same optimal rank, or does it vary with random initialization?

**Approach**:
- Run decompositions for each rank with 5 different random states
- Calculate Synthetic FMS for each (random_state, rank) combination
- Plot FMS curves for all random states to visualize consistency
- Identify optimal rank for each random state

```{python factor-match-score-evaluation, eval=TRUE}
# ===========================================
# FACTOR MATCH SCORE EVALUATION - MULTIPLE RANDOM STATES
# ===========================================
import numpy as np
import pandas as pd
import os

print(f"\n{'='*60}")
print("FACTOR MATCH SCORE (FMS) EVALUATION - MULTIPLE RANDOM STATES")
print(f"{'='*60}")
print("This analysis evaluates decomposition quality across random initializations:")
print("  • Synthetic FMS: Tests factor recovery from noisy data")
print("  • Multiple random states: Tests stability of optimal rank selection")
print("  • Goal: Determine if FMS elbow is consistent or random_state dependent")
print(f"{'='*60}")


def calculate_synthetic_fms(tensor, decomposition, rank, noise_level=0.1, random_state=42):
    """
    RECOMMENDED METRIC: Synthetic validation approach for FMS.
    
    This creates a synthetic tensor from the decomposition, adds noise,
    re-decomposes it, then measures how well we recover the original factors.
    
    This is the most rigorous validation of decomposition quality because it tests
    the method's ability to recover known factors in the presence of noise.
    
    Higher scores indicate:
    - Better factor recovery from noisy data
    - More robust decomposition
    - Less sensitivity to noise
    - Better rank selection
    
    NOTE: Computationally expensive (requires additional decomposition per rank).
    """
    try:
        import tensorly as tl
        import tlviz.factor_tools
        from barnacle.decomposition import SparseCP
        
        # Step 1: Create synthetic "ground truth" tensor from current decomposition
        gene_factors = decomposition.factors[0]
        sample_factors = decomposition.factors[1]
        time_factors = decomposition.factors[2]
        weights = decomposition.weights
        
        # Reconstruct clean synthetic tensor
        synthetic_clean = np.zeros(tensor.shape)
        for r in range(len(weights)):
            outer_prod = np.outer(gene_factors[:, r], sample_factors[:, r])
            outer_prod = np.outer(outer_prod.flatten(), time_factors[:, r])
            outer_prod = outer_prod.reshape(tensor.shape)
            synthetic_clean += weights[r] * outer_prod
        
        # Step 2: Add realistic noise
        np.random.seed(random_state)
        noise_std = noise_level * np.std(synthetic_clean)
        noise = np.random.normal(0, noise_std, synthetic_clean.shape)
        synthetic_noisy = synthetic_clean + noise
        
        # Step 3: Decompose the noisy synthetic tensor
        # This tests: "Can we recover the factors from noisy data?"
        model_synthetic = SparseCP(
            rank=rank,
            lambdas=[0.1, 0.0, 0.1],
            nonneg_modes=[0],
            norm_constraint=True,
            init='random',
            tol=1e-5,
            n_iter_max=5000,  # Fewer iterations for speed
            random_state=random_state + 1,
            n_initializations=1
        )
        
        recovered_decomposition = model_synthetic.fit_transform(synthetic_noisy, verbose=0)
        
        # Step 4: Compare recovered factors to ground truth (original decomposition)
        # Create proper CPTensor objects
        ground_truth_cp = tl.cp_tensor.CPTensor((weights, [gene_factors, sample_factors, time_factors]))
        
        # MockDecomposition for recovered (same structure as before)
        class MockDecomposition:
            def __init__(self, weights, factors):
                self.weights = weights
                self.factors = factors
                self._tuple = (weights, factors)
            def __getitem__(self, index):
                return self._tuple[index]
            def __len__(self):
                return 2
        
        recovered_mock = MockDecomposition(recovered_decomposition.weights, recovered_decomposition.factors)
        
        # Calculate FMS between ground truth and recovered
        fms, perm = tlviz.factor_tools.factor_match_score(
            ground_truth_cp,
            recovered_mock,
            return_permutation=True,
            allow_smaller_rank=True
        )
        
        return {
            'fms': float(fms),
            'permutation': perm,
            'success': True,
            'method': 'synthetic_validation',
            'noise_level': noise_level,
            'recovery_quality': 'excellent' if fms > 0.9 else 'good' if fms > 0.7 else 'fair' if fms > 0.5 else 'poor'
        }
        
    except ImportError as e:
        error_msg = "TLViz not available (scipy incompatibility)"
        if '_lazywhere' in str(e):
            error_msg = "SciPy version incompatibility (_lazywhere removed in scipy >= 1.14)"
        return {
            'fms': np.nan,
            'success': False,
            'error': error_msg,
            'method': 'import_failed'
        }
    except Exception as e:
        return {
            'fms': np.nan,
            'success': False,
            'error': str(e),
            'method': 'failed'
        }


def evaluate_fms_for_rank_and_state(rank, random_state, output_dir, tensor):
    """
    Evaluate Synthetic FMS for a specific rank and random state combination.
    
    Parameters:
    -----------
    rank : int
        Rank to evaluate
    random_state : int
        Random state used for decomposition
    output_dir : str
        Base directory containing results
    tensor : np.ndarray
        Original tensor
    """
    print(f"\n  --- Evaluating Rank {rank}, Random State {random_state} ---")
    
    # Construct path to this random_state's results
    state_dir = os.path.join(output_dir, f'random_state_{random_state}')
    rank_dir = os.path.join(state_dir, f'rank_{rank:02d}')
    
    # Check if rank directory exists
    if not os.path.exists(rank_dir):
        print(f"    Rank directory not found: {rank_dir}")
        return None
    
    # Load factor matrices
    try:
        gene_factors = pd.read_csv(os.path.join(rank_dir, 'gene_factors.csv'), index_col=0)
        sample_factors = pd.read_csv(os.path.join(rank_dir, 'sample_factors.csv'), index_col=0)
        time_factors = pd.read_csv(os.path.join(rank_dir, 'time_factors.csv'), index_col=0)
        weights_df = pd.read_csv(os.path.join(rank_dir, 'component_weights.csv'))
        
        # Remove metadata columns from sample_factors if present
        factor_cols = [col for col in sample_factors.columns if col.startswith('Component_')]
        sample_factor_values = sample_factors[factor_cols]
        
        print(f"    Loaded factors - Genes: {gene_factors.shape}, Samples: {sample_factor_values.shape}, Time: {time_factors.shape}")
        
    except Exception as e:
        print(f"    Error loading factor matrices: {e}")
        return None
    
    # Create mock decomposition object
    class MockDecomposition:
        def __init__(self, weights, factors):
            self.weights = weights
            self.factors = factors
            self._tuple = (weights, factors)
        
        def __getitem__(self, index):
            return self._tuple[index]
        
        def __len__(self):
            return 2
    
    weights = weights_df['Weight'].values
    factors = [gene_factors.values, sample_factor_values.values, time_factors.values]
    mock_decomposition = MockDecomposition(weights, factors)
    
    # Calculate Synthetic FMS
    print(f"    Running synthetic validation...")
    fms_result = calculate_synthetic_fms(tensor, mock_decomposition, rank, random_state=random_state*100)
    
    # Combine results
    result = {
        'rank': rank,
        'random_state': random_state,
        'fms_synthetic': fms_result['fms'] if fms_result['success'] else np.nan,
        'synthetic_success': fms_result['success'],
        'synthetic_method': fms_result.get('method', 'unknown'),
        'recovery_quality': fms_result.get('recovery_quality', 'unknown') if fms_result['success'] else 'failed'
    }
    
    # Print results
    if fms_result['success']:
        recovery = fms_result.get('recovery_quality', 'unknown')
        noise = fms_result.get('noise_level', 0.1)
        print(f"    ⭐ SYNTHETIC FMS: {fms_result['fms']:.4f} ({recovery} recovery with {noise*100:.0f}% noise)")
    else:
        print(f"    Synthetic FMS: Failed ({fms_result.get('error', 'unknown error')})")
    
    # Save individual rank FMS results
    fms_file = os.path.join(rank_dir, 'synthetic_fms.json')
    import json
    with open(fms_file, 'w') as f:
        json.dump({
            'rank': rank,
            'random_state': random_state,
            'synthetic_fms': fms_result,
            'summary': result
        }, f, indent=2, default=str)
    
    print(f"    Saved FMS results to: synthetic_fms.json")
    
    return result


# ===========================================
# CONFIGURATION: Random States and Ranks
# ===========================================
RANDOM_STATES_TO_TEST = [41, 42, 43, 44, 45]  # 5 different random states
# ===========================================

# Get output directory if not available
if 'output_dir' not in globals():
    output_dir = r.output_dir

# Get tensor if not available
if 'tensor_3d' not in globals():
    print("ERROR: tensor_3d not found in memory!")
    print("Please run the tensor creation chunks first.")
    tensor_3d = None

if tensor_3d is not None:
    # Get successful ranks from the base decomposition
    # (assuming they've been run with the default random_state=41)
    log_file = os.path.join(output_dir, 'rank_comparison_log.json')
    
    if os.path.exists(log_file):
        import json
        with open(log_file, 'r') as f:
            log_data = json.load(f)
        
        successful_ranks = [r['rank'] for r in log_data['results'] if r['success']]
        print(f"Found {len(successful_ranks)} successful ranks to evaluate: {successful_ranks}")
        
    else:
        # Fallback: find rank directories
        import glob
        rank_dirs = sorted(glob.glob(os.path.join(output_dir, 'rank_*')))
        successful_ranks = []
        for rank_dir in rank_dirs:
            try:
                rank_num = int(os.path.basename(rank_dir).split('_')[1])
                successful_ranks.append(rank_num)
            except:
                pass
        print(f"Found {len(successful_ranks)} rank directories: {successful_ranks}")
    
    if len(successful_ranks) == 0:
        print("\nERROR: No successful ranks found!")
        print("Please run the barnacle decomposition chunk first.")
    else:
        print("\n" + "="*60)
        print("⭐ SYNTHETIC FMS EVALUATION ACROSS RANDOM STATES")
        print("="*60)
        print(f"Random states to test: {RANDOM_STATES_TO_TEST}")
        print(f"Ranks to evaluate: {successful_ranks}")
        print(f"Total evaluations: {len(RANDOM_STATES_TO_TEST)} states × {len(successful_ranks)} ranks = {len(RANDOM_STATES_TO_TEST) * len(successful_ranks)}")
        print(f"\nNote: If decompositions haven't been run for all random states,")
        print(f"      you'll need to run the decomposition chunk for each random state first.")
        print("="*60 + "\n")
        
        # Evaluate FMS for each random state and rank combination
        all_fms_results = []
        
        for random_state in RANDOM_STATES_TO_TEST:
            print(f"\n{'='*60}")
            print(f"EVALUATING RANDOM STATE: {random_state}")
            print(f"{'='*60}")
            
            # Check if this random_state has been run
            state_dir = os.path.join(output_dir, f'random_state_{random_state}')
            if not os.path.exists(state_dir):
                print(f"\nWARNING: Directory not found: {state_dir}")
                print(f"Decompositions for random_state={random_state} may not have been run yet.")
                print(f"Skipping this random state...")
                continue
            
            for rank in sorted(successful_ranks):
                fms_result = evaluate_fms_for_rank_and_state(rank, random_state, output_dir, tensor_3d)
                if fms_result is not None:
                    all_fms_results.append(fms_result)
        
        # ===========================================
        # CREATE FMS COMPARISON ACROSS RANDOM STATES
        # ===========================================
        if len(all_fms_results) > 0:
            print(f"\n{'='*60}")
            print("FMS RESULTS SUMMARY - ALL RANDOM STATES")
            print(f"{'='*60}")
            
            # Create comparison dataframe
            fms_df = pd.DataFrame(all_fms_results)
            
            # Save comprehensive FMS results
            comparison_dir = os.path.join(output_dir, 'rank_comparison')
            os.makedirs(comparison_dir, exist_ok=True)
            
            fms_output_file = os.path.join(comparison_dir, 'synthetic_fms_all_random_states.csv')
            fms_df.to_csv(fms_output_file, index=False)
            print(f"Saved FMS comparison table to: {fms_output_file}")
            
            # Display results
            print("\n" + "="*60)
            print("SYNTHETIC FMS RESULTS ACROSS RANDOM STATES")
            print("="*60)
            
            # Pivot table for easier viewing
            pivot_table = fms_df.pivot(index='rank', columns='random_state', values='fms_synthetic')
            print("\nSynthetic FMS by Rank and Random State:")
            print(pivot_table.to_string(float_format='%.4f'))
            
            # Calculate statistics across random states for each rank
            print("\n" + "="*60)
            print("STATISTICS ACROSS RANDOM STATES (by Rank)")
            print("="*60)
            
            stats_df = fms_df.groupby('rank')['fms_synthetic'].agg([
                ('Mean', 'mean'),
                ('Std', 'std'),
                ('Min', 'min'),
                ('Max', 'max'),
                ('CV', lambda x: x.std() / x.mean() if x.mean() > 0 else np.nan)
            ])
            print(stats_df.to_string(float_format='%.4f'))
            
            # Find optimal rank for each random state
            print("\n" + "="*60)
            print("OPTIMAL RANK BY RANDOM STATE")
            print("="*60)
            
            optimal_ranks = []
            for random_state in RANDOM_STATES_TO_TEST:
                state_data = fms_df[fms_df['random_state'] == random_state]
                if len(state_data) > 0 and state_data['synthetic_success'].any():
                    valid_data = state_data[state_data['synthetic_success']]
                    if len(valid_data) > 0:
                        best_idx = valid_data['fms_synthetic'].idxmax()
                        best_rank = valid_data.loc[best_idx, 'rank']
                        best_score = valid_data.loc[best_idx, 'fms_synthetic']
                        optimal_ranks.append({
                            'random_state': random_state,
                            'optimal_rank': int(best_rank),
                            'fms_score': best_score
                        })
                        print(f"Random State {random_state}: Rank {int(best_rank)} (FMS = {best_score:.4f})")
            
            # Check consistency of optimal rank selection
            if len(optimal_ranks) > 0:
                optimal_ranks_df = pd.DataFrame(optimal_ranks)
                unique_optimal_ranks = optimal_ranks_df['optimal_rank'].unique()
                
                print("\n" + "="*60)
                print("CONSISTENCY ANALYSIS")
                print("="*60)
                print(f"Number of random states evaluated: {len(optimal_ranks)}")
                print(f"Unique optimal ranks identified: {sorted(unique_optimal_ranks)}")
                
                if len(unique_optimal_ranks) == 1:
                    print(f"\n✓ CONSISTENT: All random states identify rank {unique_optimal_ranks[0]} as optimal")
                    print(f"  → This suggests the FMS elbow is REAL and stable")
                else:
                    print(f"\n⚠ INCONSISTENT: Different random states identify different optimal ranks")
                    print(f"  → This suggests the FMS elbow may be sensitive to initialization")
                    print(f"\nFrequency of each optimal rank:")
                    rank_counts = optimal_ranks_df['optimal_rank'].value_counts().sort_index()
                    for rank, count in rank_counts.items():
                        pct = count / len(optimal_ranks) * 100
                        print(f"  Rank {rank}: {count}/{len(optimal_ranks)} ({pct:.1f}%)")
                    
                    # Most common optimal rank
                    most_common_rank = rank_counts.idxmax()
                    print(f"\n→ Most frequently optimal rank: {most_common_rank}")
            
            print(f"\n{'='*60}")
        
        else:
            print("\nNo FMS results to analyze.")
            print("Please ensure decompositions have been run for all random states.")

else:
    print("Skipping FMS evaluation - tensor not available.")

print(f"\n{'='*60}")
print("FACTOR MATCH SCORE EVALUATION COMPLETE")
print(f"{'='*60}")
if 'all_fms_results' in locals() and len(all_fms_results) > 0:
    print(f"Summary: {len(all_fms_results)} (random_state, rank) combinations evaluated")
    successful_evals = sum(1 for r in all_fms_results if r['synthetic_success'])
    print(f"         {successful_evals}/{len(all_fms_results)} evaluations successful")
else:
    print("No evaluations performed")
```

## Compare Rank Selection Methods

This section compares the dissertation-validated cross-validation method with other approaches.

### Compare dissertation CV-SSE with other methods

```{python compare-rank-selection-methods, eval=FALSE}
# ===========================================
# COMPARE RANK SELECTION METHODS
# ===========================================
# Compares:
# 1. Dissertation CV-SSE (validated method)
# 2. Synthetic FMS (current approach)
# 3. Variance explained (elbow plot)
# 4. Manual inspection (your choice)
# ===========================================

print("="*80)
print("COMPARISON: RANK SELECTION METHODS")
print("="*80)

comparison_results = {}

# METHOD 1: Dissertation cross-validation (CV-SSE)
if 'optimal_rank_cv' in locals() and optimal_rank_cv is not None:
    comparison_results['Dissertation CV-SSE'] = {
        'rank': optimal_rank_cv,
        'method': 'Cross-validated SSE (leave-one-species-out)',
        'reference': 'Blaskowski 2024, Section 1.2.3',
        'validated': True,
        'priority': 'PRIMARY'
    }
    print(f"\n1. Dissertation CV-SSE (VALIDATED)")
    print(f"   Optimal Rank: {optimal_rank_cv}")
    print(f"   Method: Leave-one-species-out cross-validation")
    print(f"   Criterion: Minimum cross-validated SSE")
    print(f"   Reference: Blaskowski (2024) Section 1.2.3, pg. 15")
else:
    print(f"\n1. Dissertation CV-SSE: NOT RUN")
    print(f"   (Set eval=TRUE in run-cv-rank-selection chunk)")

# METHOD 2: Synthetic FMS (your current approach)
synthetic_fms_file = os.path.join(output_dir, 'rank_comparison/synthetic_fms_all_random_states.csv')
if os.path.exists(synthetic_fms_file):
    synthetic_df = pd.read_csv(synthetic_fms_file)
    
    # Find rank with highest mean synthetic FMS across random states
    if 'fms_synthetic' in synthetic_df.columns and 'rank' in synthetic_df.columns:
        # Filter successful runs
        valid_synthetic = synthetic_df[synthetic_df['synthetic_success'] == True]
        
        if len(valid_synthetic) > 0:
            # Calculate mean FMS by rank
            mean_fms_by_rank = valid_synthetic.groupby('rank')['fms_synthetic'].agg(['mean', 'std', 'count'])
            best_rank_idx = mean_fms_by_rank['mean'].idxmax()
            synthetic_optimal_rank = int(best_rank_idx)
            synthetic_fms_score = mean_fms_by_rank.loc[best_rank_idx, 'mean']
            synthetic_fms_std = mean_fms_by_rank.loc[best_rank_idx, 'std']
            
            comparison_results['Synthetic FMS'] = {
                'rank': synthetic_optimal_rank,
                'method': 'Synthetic validation with noise injection',
                'score': synthetic_fms_score,
                'std': synthetic_fms_std,
                'validated': False,
                'priority': 'SUPPLEMENTARY'
            }
            
            print(f"\n2. Synthetic FMS (CURRENT METHOD)")
            print(f"   Optimal Rank: {synthetic_optimal_rank}")
            print(f"   FMS Score: {synthetic_fms_score:.4f} ± {synthetic_fms_std:.4f}")
            print(f"   Method: Factor recovery from synthetic noisy data")
            print(f"   Note: Alternative validation, not in dissertation")
        else:
            print(f"\n2. Synthetic FMS: No valid results")
    else:
        print(f"\n2. Synthetic FMS: Data format issue")
else:
    print(f"\n2. Synthetic FMS: NOT RUN")

# METHOD 3: Variance Explained (simple elbow)
rank_comparison_log = os.path.join(output_dir, 'rank_comparison_log.json')
if os.path.exists(rank_comparison_log):
    import json
    with open(rank_comparison_log, 'r') as f:
        log_data = json.load(f)
    
    # Extract variance explained for each rank
    var_explained_by_rank = {}
    for result in log_data.get('results', []):
        if result['success'] and 'metrics' in result:
            rank = result['rank']
            var_exp = result['metrics'].get('variance_explained', None)
            if var_exp is not None:
                var_explained_by_rank[rank] = var_exp
    
    if len(var_explained_by_rank) > 0:
        # Simple heuristic: find where variance explained > 70% with minimum rank
        var_threshold = 0.70
        suitable_ranks = [r for r, v in var_explained_by_rank.items() if v >= var_threshold]
        
        if len(suitable_ranks) > 0:
            var_optimal_rank = min(suitable_ranks)
            var_optimal_score = var_explained_by_rank[var_optimal_rank]
            
            comparison_results['Variance Explained'] = {
                'rank': var_optimal_rank,
                'method': f'Minimum rank with >={var_threshold*100:.0f}% variance explained',
                'score': var_optimal_score,
                'validated': False,
                'priority': 'SUPPLEMENTARY'
            }
            
            print(f"\n3. Variance Explained (HEURISTIC)")
            print(f"   Optimal Rank: {var_optimal_rank}")
            print(f"   Variance Explained: {var_optimal_score*100:.1f}%")
            print(f"   Method: Minimum rank achieving ≥{var_threshold*100:.0f}% variance")
            print(f"   Note: Simple heuristic, not validated")
        else:
            print(f"\n3. Variance Explained: No rank reaches {var_threshold*100:.0f}% threshold")
    else:
        print(f"\n3. Variance Explained: No data available")
else:
    print(f"\n3. Variance Explained: NOT RUN")

# COMPARISON ANALYSIS
print(f"\n{'='*80}")
print("RANK SELECTION COMPARISON SUMMARY")
print(f"{'='*80}")

if len(comparison_results) > 0:
    print(f"\nMethods compared: {len(comparison_results)}")
    print(f"\nOptimal Rank by Method:")
    
    for method_name, result in comparison_results.items():
        priority = result.get('priority', 'N/A')
        validated = '✓ VALIDATED' if result.get('validated', False) else '  Not validated'
        print(f"  {method_name:25s}: Rank {result['rank']:2d}  [{priority:15s}] {validated}")
    
    # Check for agreement
    ranks = [r['rank'] for r in comparison_results.values()]
    unique_ranks = set(ranks)
    
    print(f"\n{'='*80}")
    print("AGREEMENT ANALYSIS")
    print(f"{'='*80}")
    
    if len(unique_ranks) == 1:
        consensus_rank = ranks[0]
        print(f"✓ PERFECT AGREEMENT")
        print(f"  All methods identify Rank {consensus_rank} as optimal")
        print(f"  HIGH confidence in this selection")
        
        if 'Dissertation CV-SSE' in comparison_results:
            print(f"\n  → RECOMMENDATION: Use Rank {consensus_rank}")
            print(f"    (Validated by dissertation method + confirmed by other approaches)")
    else:
        print(f"⚠ METHODS SHOW VARIABILITY")
        print(f"  Unique optimal ranks identified: {sorted(unique_ranks)}")
        
        # Frequency analysis
        from collections import Counter
        rank_counts = Counter(ranks)
        most_common_rank, most_common_count = rank_counts.most_common(1)[0]
        
        print(f"\n  Frequency by rank:")
        for rank in sorted(rank_counts.keys()):
            count = rank_counts[rank]
            methods = [name for name, r in comparison_results.items() if r['rank'] == rank]
            print(f"    Rank {rank:2d}: {count}/{len(comparison_results)} methods")
            print(f"             ({', '.join(methods)})")
        
        # Recommendation logic
        if 'Dissertation CV-SSE' in comparison_results:
            cv_rank = comparison_results['Dissertation CV-SSE']['rank']
            print(f"\n  → PRIMARY RECOMMENDATION: Use Rank {cv_rank}")
            print(f"    (Dissertation-validated cross-validation method)")
            
            # Check if other methods are close
            rank_diff = [abs(r - cv_rank) for r in unique_ranks if r != cv_rank]
            if len(rank_diff) > 0 and max(rank_diff) <= 5:
                print(f"    Other methods suggest ranks within ±5: {sorted(unique_ranks)}")
                print(f"    MODERATE confidence - consider examining nearby ranks")
            elif len(rank_diff) > 0:
                print(f"    Other methods differ substantially: {sorted(unique_ranks)}")
                print(f"    Recommend examining dissertation-selected rank {cv_rank}")
        else:
            print(f"\n  → RECOMMENDATION: Rank {most_common_rank}")
            print(f"    (Most common across methods, but not validated)")
            print(f"    MODERATE confidence - consider running dissertation CV method")

else:
    print("\nNo methods have been run yet.")
    print("Run the rank selection analyses first.")

# Save comparison results
if len(comparison_results) > 0:
    comparison_file = os.path.join(output_dir, 'rank_selection_comparison.json')
    import json
    with open(comparison_file, 'w') as f:
        json.dump({
            'timestamp': pd.Timestamp.now().isoformat(),
            'methods': comparison_results,
            'summary': {
                'n_methods': len(comparison_results),
                'unique_ranks': sorted(list(unique_ranks)),
                'agreement': 'perfect' if len(unique_ranks) == 1 else 'variable'
            }
        }, f, indent=2)
    print(f"\n✓ Comparison saved to: {comparison_file}")

print(f"\n{'='*80}")
```



# VISUALIZATION

## Plot rank comparisons

### Plot Cross-Validated SSE Results

```{python plot-cv-sse-results, eval=FALSE}
# ===========================================
# PLOT CROSS-VALIDATED SSE RESULTS
# Visualize dissertation-validated rank selection
# ===========================================

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Check if CV results exist
if 'cv_rank_results' not in locals() or cv_rank_results is None:
    print("Cross-validation results not available.")
    print("Run the run-cv-rank-selection chunk first.")
else:
    print("="*60)
    print("VISUALIZING CROSS-VALIDATED RANK SELECTION")
    print("="*60)
    
    # Create figure with subplots
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('Dissertation-Validated Rank Selection via Cross-Validation', 
                 fontsize=16, fontweight='bold', y=0.995)
    
    # Filter to successful results
    plot_data = cv_rank_results[cv_rank_results['n_successful_folds'] == len(replicate_groups)].copy()
    
    if len(plot_data) == 0:
        print("No successful CV results to plot")
    else:
        # PLOT 1: CV-SSE vs Rank (main criterion)
        ax1 = axes[0, 0]
        ax1.errorbar(plot_data['rank'], plot_data['mean_cv_sse'], 
                    yerr=plot_data['std_cv_sse'],
                    marker='o', markersize=8, linewidth=2.5, 
                    capsize=5, capthick=2,
                    color='steelblue', ecolor='lightblue')
        
        # Mark optimal rank
        if 'optimal_rank_cv' in locals() and optimal_rank_cv is not None:
            optimal_data = plot_data[plot_data['rank'] == optimal_rank_cv]
            if len(optimal_data) > 0:
                ax1.plot(optimal_rank_cv, optimal_data['mean_cv_sse'].values[0], 
                        '*', markersize=20, color='red', 
                        markeredgecolor='darkred', markeredgewidth=2,
                        label=f'Optimal Rank: {optimal_rank_cv}', zorder=10)
                ax1.legend(fontsize=11, loc='best')
        
        ax1.set_xlabel('Rank (Number of Components)', fontsize=12, fontweight='bold')
        ax1.set_ylabel('Cross-Validated SSE', fontsize=12, fontweight='bold')
        ax1.set_title('A) CV-SSE vs Rank\n(Primary Selection Criterion)', 
                     fontsize=12, fontweight='bold')
        ax1.grid(True, alpha=0.3)
        ax1.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))
        
        # PLOT 2: Relative CV-SSE (normalized)
        ax2 = axes[0, 1]
        min_sse = plot_data['mean_cv_sse'].min()
        plot_data['relative_cv_sse'] = plot_data['mean_cv_sse'] / min_sse
        
        ax2.plot(plot_data['rank'], plot_data['relative_cv_sse'], 
                'o-', linewidth=2.5, markersize=8, color='darkgreen')
        ax2.axhline(y=1.0, color='red', linestyle='--', alpha=0.5, linewidth=2)
        
        if 'optimal_rank_cv' in locals() and optimal_rank_cv is not None:
            optimal_data = plot_data[plot_data['rank'] == optimal_rank_cv]
            if len(optimal_data) > 0:
                ax2.plot(optimal_rank_cv, optimal_data['relative_cv_sse'].values[0],
                        '*', markersize=20, color='red',
                        markeredgecolor='darkred', markeredgewidth=2, zorder=10)
        
        ax2.set_xlabel('Rank (Number of Components)', fontsize=12, fontweight='bold')
        ax2.set_ylabel('Relative CV-SSE (normalized to min)', fontsize=12, fontweight='bold')
        ax2.set_title('B) Relative CV-SSE\n(Shows relative improvement)', 
                     fontsize=12, fontweight='bold')
        ax2.grid(True, alpha=0.3)
        
        # PLOT 3: SSE Range by Rank
        ax3 = axes[1, 0]
        
        # Create bar plot showing range
        ranks = plot_data['rank'].values
        means = plot_data['mean_cv_sse'].values
        mins = plot_data['min_cv_sse'].values
        maxs = plot_data['max_cv_sse'].values
        
        ax3.bar(ranks, means, alpha=0.6, color='steelblue', label='Mean CV-SSE')
        
        # Add error bars showing range
        for i, rank in enumerate(ranks):
            ax3.plot([rank, rank], [mins[i], maxs[i]], 
                    'k-', linewidth=2, alpha=0.5)
            ax3.plot([rank], [mins[i]], 'v', color='green', markersize=6)
            ax3.plot([rank], [maxs[i]], '^', color='orange', markersize=6)
        
        if 'optimal_rank_cv' in locals() and optimal_rank_cv is not None:
            ax3.axvline(x=optimal_rank_cv, color='red', linestyle='--', 
                       linewidth=2, alpha=0.7, label=f'Optimal: {optimal_rank_cv}')
        
        ax3.set_xlabel('Rank (Number of Components)', fontsize=12, fontweight='bold')
        ax3.set_ylabel('Cross-Validated SSE', fontsize=12, fontweight='bold')
        ax3.set_title('C) CV-SSE Variability Across Folds\n(Range: min to max)', 
                     fontsize=12, fontweight='bold')
        ax3.legend(fontsize=10)
        ax3.grid(True, alpha=0.3, axis='y')
        ax3.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))
        
        # PLOT 4: Summary text
        ax4 = axes[1, 1]
        ax4.axis('off')
        
        summary_text = "CROSS-VALIDATION SUMMARY\n"
        summary_text += "="*40 + "\n\n"
        summary_text += f"Method: Leave-one-species-out CV\n"
        summary_text += f"CV Folds: {len(replicate_groups)}\n"
        summary_text += f"Ranks Tested: {len(plot_data)}\n\n"
        
        if 'optimal_rank_cv' in locals() and optimal_rank_cv is not None:
            summary_text += f"OPTIMAL RANK: {optimal_rank_cv}\n"
            optimal_data = plot_data[plot_data['rank'] == optimal_rank_cv]
            if len(optimal_data) > 0:
                sse = optimal_data['mean_cv_sse'].values[0]
                std = optimal_data['std_cv_sse'].values[0]
                summary_text += f"CV-SSE: {sse:.2e} ± {std:.2e}\n\n"
        
        summary_text += "Criterion: Minimum CV-SSE\n"
        summary_text += "Reference: Blaskowski (2024)\n"
        summary_text += "            Section 1.2.3, pg. 15\n\n"
        
        summary_text += "Validated on 100 simulations:\n"
        summary_text += "  • 86% accuracy\n"
        summary_text += "  • Works to 10:1 noise ratio\n\n"
        
        summary_text += "This is the RECOMMENDED\n"
        summary_text += "method for rank selection."
        
        ax4.text(0.05, 0.95, summary_text,
                transform=ax4.transAxes,
                verticalalignment='top',
                fontsize=11,
                family='monospace',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))
        
        plt.tight_layout()
        
        # Save figure
        cv_plot_file = os.path.join(output_dir, 'dissertation_cv_results/cv_rank_selection_plot.png')
        plt.savefig(cv_plot_file, dpi=300, bbox_inches='tight')
        print(f"\n✓ Plot saved to: {cv_plot_file}")
        
        plt.show()
        plt.close()
        
        print("="*60)
```

### Plot Synthetic FMS Across Random States

This chunk creates visualizations comparing synthetic FMS across different random states to evaluate the stability of optimal rank selection.

```{python plot-synthetic-fms-random-states, eval=TRUE}
# ===========================================
# PLOT SYNTHETIC FMS ACROSS RANDOM STATES
# ===========================================
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Get output directory
output_dir = r.output_dir if 'r' in dir() else '../output/13.00-multiomics-barnacle'
comparison_dir = os.path.join(output_dir, 'rank_comparison')
fms_csv_file = os.path.join(comparison_dir, 'synthetic_fms_all_random_states.csv')

print(f"\n{'='*60}")
print("PLOTTING SYNTHETIC FMS ACROSS RANDOM STATES")
print(f"{'='*60}")
print(f"Loading FMS results from: {fms_csv_file}")

# Check if FMS results file exists
if not os.path.exists(fms_csv_file):
    print(f"\nERROR: FMS results file not found!")
    print(f"Expected location: {fms_csv_file}")
    print(f"\nPlease run the Factor Match Score evaluation chunk first.")
else:
    # Load FMS results
    fms_df = pd.read_csv(fms_csv_file)
    
    print(f"Loaded FMS data for {len(fms_df)} (rank, random_state) combinations")
    print(f"Columns available: {list(fms_df.columns)}")
    
    # Check if synthetic FMS data is available
    if 'fms_synthetic' not in fms_df.columns:
        print(f"\nERROR: No 'fms_synthetic' column found in FMS results!")
        print(f"Synthetic validation was not performed.")
    else:
        # Check if any have valid synthetic FMS
        valid_synthetic = fms_df['synthetic_success'].fillna(False)
        
        if not valid_synthetic.any():
            print(f"\nERROR: No valid synthetic FMS values found!")
        else:
            print(f"Found {valid_synthetic.sum()} valid FMS values")
            
            # Filter to valid data
            plot_data = fms_df[valid_synthetic].copy()
            
            # ===========================================
            # FIGURE 1: LINE PLOT - FMS CURVES BY RANDOM STATE
            # ===========================================
            fig, ax = plt.subplots(1, 1, figsize=(12, 7))
            
            # Color palette for different random states
            random_states = sorted(plot_data['random_state'].unique())
            colors = plt.cm.Set2(np.linspace(0, 1, len(random_states)))
            
            # Plot FMS curve for each random state
            optimal_ranks = {}
            for i, rs in enumerate(random_states):
                rs_data = plot_data[plot_data['random_state'] == rs].sort_values('rank')
                ax.plot(rs_data['rank'], rs_data['fms_synthetic'], 
                       'o-', linewidth=2.5, markersize=8, 
                       label=f'Random State {rs}', 
                       color=colors[i], alpha=0.8)
                
                # Mark optimal rank for this random state
                max_idx = rs_data['fms_synthetic'].idxmax()
                optimal_rank = rs_data.loc[max_idx, 'rank']
                optimal_fms = rs_data.loc[max_idx, 'fms_synthetic']
                optimal_ranks[rs] = int(optimal_rank)
                
                ax.plot(optimal_rank, optimal_fms, '*', 
                       markersize=15, color=colors[i], 
                       markeredgecolor='black', markeredgewidth=1.5, zorder=10)
            
            # Formatting
            ax.set_xlabel('Rank (Number of Components)', fontsize=13, fontweight='bold')
            ax.set_ylabel('Synthetic FMS', fontsize=13, fontweight='bold')
            ax.set_title('Synthetic Factor Match Score Across Random Initializations\n' +
                        'Does the FMS elbow consistently identify the same optimal rank?',
                        fontsize=14, fontweight='bold')
            ax.grid(True, alpha=0.3, linestyle='--')
            ax.legend(loc='best', fontsize=10, framealpha=0.9)
            ax.set_ylim([0, 1.05])
            
            # Add quality threshold lines
            ax.axhline(y=0.9, color='green', linestyle=':', alpha=0.3, linewidth=1.5)
            ax.text(ax.get_xlim()[1]*0.98, 0.91, 'Excellent', 
                   ha='right', va='bottom', fontsize=9, color='green', alpha=0.7, 
                   bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))
            ax.axhline(y=0.7, color='orange', linestyle=':', alpha=0.3, linewidth=1.5)
            ax.text(ax.get_xlim()[1]*0.98, 0.71, 'Good', 
                   ha='right', va='bottom', fontsize=9, color='orange', alpha=0.7,
                   bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))
            
            plt.tight_layout()
            plot_file = os.path.join(comparison_dir, 'synthetic_fms_by_random_state.png')
            plt.savefig(plot_file, dpi=300, bbox_inches='tight')
            print(f"\n✓ Saved FMS comparison plot to: {plot_file}")
            plt.show()
            plt.close()
            
            # ===========================================
            # FIGURE 2: HEATMAP - FMS BY RANK AND RANDOM STATE
            # ===========================================
            fig, ax = plt.subplots(1, 1, figsize=(12, 6))
            
            # Pivot data for heatmap
            pivot_data = plot_data.pivot(index='random_state', columns='rank', values='fms_synthetic')
            
            # Create heatmap
            sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='RdYlGn', 
                       center=0.7, vmin=0, vmax=1.0,
                       cbar_kws={'label': 'Synthetic FMS'},
                       linewidths=0.5, linecolor='gray',
                       ax=ax)
            
            ax.set_xlabel('Rank (Number of Components)', fontsize=12, fontweight='bold')
            ax.set_ylabel('Random State', fontsize=12, fontweight='bold')
            ax.set_title('Synthetic FMS Heatmap: Rank vs Random State\n' +
                        'Darker green = better factor recovery',
                        fontsize=13, fontweight='bold')
            
            # Mark optimal ranks with stars
            for rs_idx, rs in enumerate(pivot_data.index):
                if rs in optimal_ranks:
                    opt_rank = optimal_ranks[rs]
                    rank_idx = list(pivot_data.columns).index(opt_rank)
                    ax.text(rank_idx + 0.5, rs_idx + 0.5, '★',
                           ha='center', va='center', fontsize=20,
                           color='darkblue', weight='bold')
            
            plt.tight_layout()
            heatmap_file = os.path.join(comparison_dir, 'synthetic_fms_heatmap.png')
            plt.savefig(heatmap_file, dpi=300, bbox_inches='tight')
            print(f"✓ Saved FMS heatmap to: {heatmap_file}")
            plt.show()
            plt.close()
            
            # ===========================================
            # FIGURE 3: BAR PLOT - OPTIMAL RANK FREQUENCY
            # ===========================================
            fig, ax = plt.subplots(1, 1, figsize=(10, 6))
            
            # Count optimal ranks
            optimal_rank_values = list(optimal_ranks.values())
            unique_optimal_ranks = sorted(set(optimal_rank_values))
            rank_counts = {rank: optimal_rank_values.count(rank) for rank in unique_optimal_ranks}
            
            # Create bar plot
            bars = ax.bar(rank_counts.keys(), rank_counts.values(), 
                         color='steelblue', edgecolor='black', linewidth=1.5, alpha=0.8)
            
            # Add count labels on bars
            for bar in bars:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{int(height)}/{len(random_states)}',
                       ha='center', va='bottom', fontsize=11, fontweight='bold')
            
            ax.set_xlabel('Optimal Rank', fontsize=12, fontweight='bold')
            ax.set_ylabel('Frequency (Number of Random States)', fontsize=12, fontweight='bold')
            ax.set_title('Optimal Rank Selection Frequency Across Random States\n' +
                        f'Total Random States: {len(random_states)}',
                        fontsize=13, fontweight='bold')
            ax.grid(True, alpha=0.3, axis='y')
            ax.set_ylim([0, len(random_states) + 0.5])
            ax.set_xticks(unique_optimal_ranks)
            
            # Add consistency indicator
            if len(unique_optimal_ranks) == 1:
                consistency_text = f"✓ CONSISTENT: All states select rank {unique_optimal_ranks[0]}"
                text_color = 'green'
            else:
                most_common = max(rank_counts, key=rank_counts.get)
                consistency_text = f"⚠ VARIABLE: Most common is rank {most_common} ({rank_counts[most_common]}/{len(random_states)} states)"
                text_color = 'orange'
            
            ax.text(0.5, 0.95, consistency_text,
                   transform=ax.transAxes, ha='center', va='top',
                   fontsize=12, fontweight='bold', color=text_color,
                   bbox=dict(boxstyle='round,pad=0.5', facecolor='white', 
                            edgecolor=text_color, linewidth=2, alpha=0.9))
            
            plt.tight_layout()
            bar_plot_file = os.path.join(comparison_dir, 'optimal_rank_frequency.png')
            plt.savefig(bar_plot_file, dpi=300, bbox_inches='tight')
            print(f"✓ Saved optimal rank frequency plot to: {bar_plot_file}")
            plt.show()
            plt.close()
            
            # ===========================================
            # SUMMARY STATISTICS
            # ===========================================
            print(f"\n{'='*60}")
            print("SUMMARY: OPTIMAL RANK STABILITY")
            print(f"{'='*60}")
            
            print(f"\nOptimal Ranks by Random State:")
            for rs in sorted(optimal_ranks.keys()):
                print(f"  Random State {rs}: Rank {optimal_ranks[rs]}")
            
            print(f"\nOptimal Rank Frequency:")
            for rank in sorted(rank_counts.keys()):
                count = rank_counts[rank]
                pct = count / len(random_states) * 100
                print(f"  Rank {rank}: {count}/{len(random_states)} ({pct:.1f}%)")
            
            if len(unique_optimal_ranks) == 1:
                print(f"\n✓ CONCLUSION: Rank selection is STABLE")
                print(f"  → All random states consistently identify rank {unique_optimal_ranks[0]} as optimal")
                print(f"  → The FMS elbow is REAL and not dependent on random initialization")
                print(f"  → High confidence in rank {unique_optimal_ranks[0]} as the optimal choice")
            else:
                most_common_rank = max(rank_counts, key=rank_counts.get)
                most_common_count = rank_counts[most_common_rank]
                most_common_pct = most_common_count / len(random_states) * 100
                
                print(f"\n⚠ CONCLUSION: Rank selection shows VARIABILITY")
                print(f"  → Different random states identify different optimal ranks")
                print(f"  → Most common optimal rank: {most_common_rank} ({most_common_pct:.1f}% of states)")
                print(f"  → The FMS elbow may be influenced by random initialization")
                
                if most_common_pct >= 60:
                    print(f"  → Moderate confidence: Rank {most_common_rank} is optimal in majority of cases")
                    print(f"  → Consider examining decompositions for ranks: {sorted(unique_optimal_ranks)}")
                else:
                    print(f"  → Low confidence: No clear consensus on optimal rank")
                    print(f"  → Recommend examining multiple ranks: {sorted(unique_optimal_ranks)}")
            
            print(f"\n{'='*60}")
            
            # ===========================================
            # FIGURE 4: ELBOW PLOTS - FMS BY RANK FOR EACH RANDOM SEED
            # ===========================================
            print(f"\n{'='*60}")
            print("CREATING ELBOW PLOTS FOR RANK SELECTION")
            print(f"{'='*60}\n")
            
            # Create subplots - one for each random state
            n_states = len(random_states)
            n_cols = min(3, n_states)
            n_rows = int(np.ceil(n_states / n_cols))
            
            fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows))
            if n_states == 1:
                axes = np.array([axes])
            axes = axes.flatten() if n_states > 1 else axes
            
            print(f"Creating individual elbow plots for {n_states} random states...")
            
            for idx, rs in enumerate(random_states):
                ax = axes[idx]
                rs_data = plot_data[plot_data['random_state'] == rs].sort_values('rank')
                
                # Plot FMS vs Rank
                ax.plot(rs_data['rank'], rs_data['fms_synthetic'], 
                       'o-', linewidth=2.5, markersize=10, 
                       color=colors[idx], alpha=0.8, label='Synthetic FMS')
                
                # Mark the optimal rank
                max_idx = rs_data['fms_synthetic'].idxmax()
                optimal_rank = rs_data.loc[max_idx, 'rank']
                optimal_fms = rs_data.loc[max_idx, 'fms_synthetic']
                
                ax.plot(optimal_rank, optimal_fms, '*', 
                       markersize=20, color='red', 
                       markeredgecolor='darkred', markeredgewidth=2, 
                       zorder=10, label=f'Optimal: Rank {int(optimal_rank)}')
                
                # Calculate and annotate marginal gains
                ranks = rs_data['rank'].values
                fms_values = rs_data['fms_synthetic'].values
                
                for i in range(len(ranks)-1):
                    marginal_gain = fms_values[i+1] - fms_values[i]
                    mid_x = (ranks[i] + ranks[i+1]) / 2
                    mid_y = (fms_values[i] + fms_values[i+1]) / 2
                    
                    # Annotate with marginal gain
                    ax.annotate(f'+{marginal_gain:.3f}', 
                               xy=(mid_x, mid_y),
                               xytext=(0, 10), textcoords='offset points',
                               ha='center', fontsize=8, 
                               color='darkblue', alpha=0.7,
                               bbox=dict(boxstyle='round,pad=0.3', 
                                       facecolor='lightyellow', 
                                       alpha=0.6, edgecolor='none'))
                
                # Add quality threshold lines
                ax.axhline(y=0.9, color='green', linestyle=':', alpha=0.4, linewidth=1.5)
                ax.axhline(y=0.7, color='orange', linestyle=':', alpha=0.4, linewidth=1.5)
                ax.axhline(y=0.5, color='red', linestyle=':', alpha=0.4, linewidth=1.5)
                
                # Formatting
                ax.set_xlabel('Rank (Number of Components)', fontsize=11, fontweight='bold')
                ax.set_ylabel('Synthetic FMS', fontsize=11, fontweight='bold')
                ax.set_title(f'Random State {rs}\nElbow Plot for Rank Selection',
                            fontsize=12, fontweight='bold')
                ax.grid(True, alpha=0.3, linestyle='--')
                ax.legend(loc='best', fontsize=9, framealpha=0.9)
                ax.set_ylim([0, 1.05])
                
                # Add quality labels
                ax.text(0.98, 0.91, 'Excellent', transform=ax.transAxes,
                       ha='right', va='bottom', fontsize=8, color='green', alpha=0.6)
                ax.text(0.98, 0.71, 'Good', transform=ax.transAxes,
                       ha='right', va='bottom', fontsize=8, color='orange', alpha=0.6)
                ax.text(0.98, 0.51, 'Fair', transform=ax.transAxes,
                       ha='right', va='bottom', fontsize=8, color='red', alpha=0.6)
            
            # Hide extra subplots if any
            for idx in range(n_states, len(axes)):
                axes[idx].axis('off')
            
            plt.tight_layout()
            elbow_individual_file = os.path.join(comparison_dir, 'elbow_plots_by_random_state.png')
            plt.savefig(elbow_individual_file, dpi=300, bbox_inches='tight')
            print(f"✓ Saved individual elbow plots to: {elbow_individual_file}")
            plt.show()
            plt.close()
            
            # ===========================================
            # FIGURE 5: COMBINED ELBOW PLOT - ALL RANDOM SEEDS
            # ===========================================
            print(f"\nCreating combined elbow plot with all random states...")
            
            fig, ax = plt.subplots(1, 1, figsize=(14, 8))
            
            # Plot all random states together
            for i, rs in enumerate(random_states):
                rs_data = plot_data[plot_data['random_state'] == rs].sort_values('rank')
                
                # Main line
                ax.plot(rs_data['rank'], rs_data['fms_synthetic'], 
                       'o-', linewidth=2.5, markersize=8, 
                       label=f'Random State {rs}', 
                       color=colors[i], alpha=0.7)
                
                # Mark optimal
                max_idx = rs_data['fms_synthetic'].idxmax()
                optimal_rank = rs_data.loc[max_idx, 'rank']
                optimal_fms = rs_data.loc[max_idx, 'fms_synthetic']
                
                ax.plot(optimal_rank, optimal_fms, '*', 
                       markersize=15, color=colors[i], 
                       markeredgecolor='black', markeredgewidth=1.5, zorder=10)
            
            # Calculate mean FMS across random states for each rank
            mean_fms_by_rank = plot_data.groupby('rank')['fms_synthetic'].mean().reset_index()
            mean_fms_by_rank = mean_fms_by_rank.sort_values('rank')
            
            # Plot mean curve with thicker line
            ax.plot(mean_fms_by_rank['rank'], mean_fms_by_rank['fms_synthetic'],
                   'k--', linewidth=3.5, markersize=12, marker='D',
                   label='Mean across all states', zorder=5, alpha=0.9)
            
            # Find and mark mean optimal rank
            mean_optimal_idx = mean_fms_by_rank['fms_synthetic'].idxmax()
            mean_optimal_rank = mean_fms_by_rank.loc[mean_optimal_idx, 'rank']
            mean_optimal_fms = mean_fms_by_rank.loc[mean_optimal_idx, 'fms_synthetic']
            
            ax.plot(mean_optimal_rank, mean_optimal_fms, 'D',
                   markersize=18, color='darkred', 
                   markeredgecolor='black', markeredgewidth=2, zorder=15,
                   label=f'Mean Optimal: Rank {int(mean_optimal_rank)}')
            
            # Add marginal gains for mean curve
            ranks = mean_fms_by_rank['rank'].values
            fms_values = mean_fms_by_rank['fms_synthetic'].values
            
            for i in range(len(ranks)-1):
                marginal_gain = fms_values[i+1] - fms_values[i]
                mid_x = (ranks[i] + ranks[i+1]) / 2
                mid_y = (fms_values[i] + fms_values[i+1]) / 2
                
                ax.annotate(f'+{marginal_gain:.3f}', 
                           xy=(mid_x, mid_y),
                           xytext=(0, 15), textcoords='offset points',
                           ha='center', fontsize=9, fontweight='bold',
                           color='darkred', alpha=0.8,
                           bbox=dict(boxstyle='round,pad=0.4', 
                                   facecolor='yellow', 
                                   alpha=0.7, edgecolor='darkred', linewidth=1.5))
            
            # Quality thresholds
            ax.axhline(y=0.9, color='green', linestyle=':', alpha=0.3, linewidth=2)
            ax.text(ax.get_xlim()[1]*0.98, 0.91, 'Excellent', 
                   ha='right', va='bottom', fontsize=10, color='green', alpha=0.7, 
                   bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))
            ax.axhline(y=0.7, color='orange', linestyle=':', alpha=0.3, linewidth=2)
            ax.text(ax.get_xlim()[1]*0.98, 0.71, 'Good', 
                   ha='right', va='bottom', fontsize=10, color='orange', alpha=0.7,
                   bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))
            ax.axhline(y=0.5, color='red', linestyle=':', alpha=0.3, linewidth=2)
            ax.text(ax.get_xlim()[1]*0.98, 0.51, 'Fair', 
                   ha='right', va='bottom', fontsize=10, color='red', alpha=0.7,
                   bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))
            
            # Formatting
            ax.set_xlabel('Rank (Number of Components)', fontsize=14, fontweight='bold')
            ax.set_ylabel('Synthetic Factor Match Score', fontsize=14, fontweight='bold')
            ax.set_title('Combined Elbow Plot: Synthetic FMS Across All Random States\n' +
                        'Look for the elbow where marginal gains diminish',
                        fontsize=15, fontweight='bold', pad=20)
            ax.grid(True, alpha=0.3, linestyle='--')
            ax.legend(loc='best', fontsize=10, framealpha=0.95, 
                     ncol=2 if len(random_states) > 5 else 1)
            ax.set_ylim([0, 1.05])
            
            # Add consensus indicator
            if len(unique_optimal_ranks) == 1:
                consensus_text = f"✓ CONSENSUS: All states agree on Rank {unique_optimal_ranks[0]}"
                text_color = 'green'
                text_bg = 'lightgreen'
            else:
                most_common = max(rank_counts, key=rank_counts.get)
                pct = rank_counts[most_common] / len(random_states) * 100
                consensus_text = f"⚠ VARIABLE: Most common Rank {most_common} ({pct:.0f}% agreement)"
                text_color = 'orange'
                text_bg = 'lightyellow'
            
            ax.text(0.5, 0.02, consensus_text,
                   transform=ax.transAxes, ha='center', va='bottom',
                   fontsize=12, fontweight='bold', color=text_color,
                   bbox=dict(boxstyle='round,pad=0.7', facecolor=text_bg, 
                            edgecolor=text_color, linewidth=2.5, alpha=0.95))
            
            plt.tight_layout()
            elbow_combined_file = os.path.join(comparison_dir, 'elbow_plot_combined_all_states.png')
            plt.savefig(elbow_combined_file, dpi=300, bbox_inches='tight')
            print(f"✓ Saved combined elbow plot to: {elbow_combined_file}")
            plt.show()
            plt.close()
            
            print(f"\n{'='*60}")
            print("ELBOW PLOT GENERATION COMPLETE")
            print(f"{'='*60}")
            print(f"Generated plots:")
            print(f"  1. Individual elbow plots: {elbow_individual_file}")
            print(f"  2. Combined elbow plot: {elbow_combined_file}")
            print(f"\nUse these plots to identify the optimal rank where:")
            print(f"  • Marginal gains in FMS become small")
            print(f"  • The curve flattens (diminishing returns)")
            print(f"  • Trade-off between model complexity and performance")
            print(f"{'='*60}\n")

print(f"\nSYNTHETIC FMS PLOTTING COMPLETE")
print(f"{'='*60}")
```

### 1. Synthetic FMS Plots for Rank Selection

Located in `barnacle_factors/rank_comparison/`:

**Elbow Plots (NEW - for rank selection):**
- **`elbow_plot_combined_all_states.png`** - **MOST IMPORTANT FOR RANK SELECTION**
  - Shows all random states together with mean curve
  - Marginal gains annotated on mean curve
  - Look for where the curve flattens (diminishing returns)
  - Quality thresholds marked (Excellent > 0.9, Good > 0.7, Fair > 0.5)
  
- **`elbow_plots_by_random_state.png`** - Individual elbow plots
  - Separate subplot for each random initialization
  - Check consistency of elbow position across random states
  - Marginal gains annotated for each state

**FMS Comparison Plots:**
- **`synthetic_fms_by_random_state.png`** - Line plot of FMS curves
- **`synthetic_fms_heatmap.png`** - Heatmap showing rank vs random state
- **`optimal_rank_frequency.png`** - Bar plot of how often each rank is optimal

**Legacy Variance Plots:**
- **`elbow_plot.png`** - Shows variance explained vs rank (from original comparison)
- **`rank_comparison_summary.png`** - 6-panel overview
- **`loss_trajectories.png`** - Convergence speed comparison

### 2. Metrics Table

File: `barnacle_factors/rank_comparison/rank_comparison_metrics.csv`

Contains all quantitative metrics for comparison.

### 3. FMS Results Table

File: `barnacle_factors/rank_comparison/synthetic_fms_all_random_states.csv`

Contains synthetic FMS scores for all (rank, random_state) combinations.

### 4. Individual Rank Results

Each rank gets its own directory (`rank_05/`, `rank_10/`, etc.) with full factor matrices.

```{python plot-rank-comparison, eval=TRUE}
# ===========================================
# This chunk reads from rank_comparison_log.json
# and does NOT require variables from previous chunks
# ===========================================
import matplotlib.pyplot as plt
import seaborn as sns
import json
import glob
import os
import pandas as pd
import numpy as np

print(f"\n{'='*60}")
print("Creating rank comparison visualizations")
print(f"{'='*60}")

# Check if output_dir exists, if not get it from R
if 'output_dir' not in globals():
    print("Getting output_dir from R environment...")
    output_dir = r.output_dir
    print(f"  output_dir = {output_dir}")

comparison_dir = os.path.join(output_dir, 'rank_comparison')
os.makedirs(comparison_dir, exist_ok=True)

# ===========================================
# LOAD RESULTS FROM LOG FILE
# ===========================================
log_file = os.path.join(output_dir, 'rank_comparison_log.json')

if os.path.exists(log_file):
    print(f"Loading results from log file: {log_file}")
    with open(log_file, 'r') as f:
        log_data = json.load(f)
    
    print(f"  Log timestamp: {log_data['timestamp']}")
    print(f"  Total runs: {log_data['total_runs']}")
    print(f"  Successful: {log_data['successful_runs']}")
    print(f"  Failed: {log_data['failed_runs']}")
    
    # Reconstruct all_rank_results from log
    all_rank_results = []
    for result_info in log_data['results']:
        if result_info['success']:
            all_rank_results.append({
                'rank': result_info['rank'],
                'metrics': result_info['metrics'],
                'success': True
            })
        else:
            all_rank_results.append({
                'rank': result_info['rank'],
                'success': False,
                'error': result_info.get('error', 'Unknown error')
            })
    
    print(f"  Loaded {len(all_rank_results)} results from log file")
    
elif 'all_rank_results' not in globals():
    # Fallback: Load from individual metadata files
    print("Log file not found. Loading from individual rank directories...")
    all_rank_results = []
    
    # Find all rank_* directories
    rank_dirs = sorted(glob.glob(os.path.join(output_dir, 'rank_*')))
    print(f"  Found {len(rank_dirs)} rank directories")
    
    for rank_dir in rank_dirs:
        metadata_file = os.path.join(rank_dir, 'metadata.json')
        if os.path.exists(metadata_file):
            with open(metadata_file, 'r') as f:
                metadata = json.load(f)
                all_rank_results.append({
                    'rank': metadata['rank'],
                    'metrics': metadata['metrics'],
                    'success': True
                })
    
    print(f"  Loaded {len(all_rank_results)} results from disk")
else:
    print("Using all_rank_results from memory")

# Extract successful results
successful = [r for r in all_rank_results if r['success']]
if len(successful) == 0:
    print("No successful decompositions to compare!")
else:
    ranks = [r['rank'] for r in successful]
    
    # Create comparison dataframe
    comparison_df = pd.DataFrame([
        {
            'Rank': r['rank'],
            **r['metrics']
        }
        for r in successful
    ])
    
    # Save comparison table
    comparison_df.to_csv(os.path.join(comparison_dir, 'rank_comparison_metrics.csv'), index=False)
    print(f"  Saved metrics table")
    
    # Create multi-panel comparison plot
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    fig.suptitle('Rank Comparison: Key Metrics', fontsize=16, fontweight='bold')
    
    # 1. Variance Explained
    ax = axes[0, 0]
    ax.plot(ranks, comparison_df['variance_explained'], 'o-', linewidth=2, markersize=8, color='steelblue')
    ax.axhline(y=0.8, color='green', linestyle='--', alpha=0.5, label='80% threshold')
    ax.axhline(y=0.6, color='orange', linestyle='--', alpha=0.5, label='60% threshold')
    ax.set_xlabel('Rank (Number of Components)', fontsize=11)
    ax.set_ylabel('Variance Explained', fontsize=11)
    ax.set_title('Variance Explained vs Rank', fontweight='bold')
    ax.grid(True, alpha=0.3)
    ax.legend()
    ax.set_ylim([0, 1.05])
    
    # 2. Relative Error
    ax = axes[0, 1]
    ax.plot(ranks, comparison_df['relative_error'], 'o-', linewidth=2, markersize=8, color='coral')
    ax.set_xlabel('Rank (Number of Components)', fontsize=11)
    ax.set_ylabel('Relative Reconstruction Error', fontsize=11)
    ax.set_title('Reconstruction Error vs Rank', fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # 3. Final Loss
    ax = axes[0, 2]
    ax.plot(ranks, comparison_df['final_loss'], 'o-', linewidth=2, markersize=8, color='purple')
    ax.set_xlabel('Rank (Number of Components)', fontsize=11)
    ax.set_ylabel('Final Loss', fontsize=11)
    ax.set_title('Final Loss vs Rank', fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # 4. Gene Sparsity
    ax = axes[1, 0]
    ax.plot(ranks, comparison_df['gene_sparsity'], 'o-', linewidth=2, markersize=8, color='green')
    ax.axhline(y=0.9, color='red', linestyle='--', alpha=0.5, label='90% sparse')
    ax.set_xlabel('Rank (Number of Components)', fontsize=11)
    ax.set_ylabel('Gene Factor Sparsity', fontsize=11)
    ax.set_title('Gene Sparsity vs Rank', fontweight='bold')
    ax.grid(True, alpha=0.3)
    ax.legend()
    ax.set_ylim([0, 1.05])
    
    # 5. Weight Distribution (CV)
    ax = axes[1, 1]
    ax.plot(ranks, comparison_df['weight_cv'], 'o-', linewidth=2, markersize=8, color='darkorange')
    ax.set_xlabel('Rank (Number of Components)', fontsize=11)
    ax.set_ylabel('Weight Coefficient of Variation', fontsize=11)
    ax.set_title('Component Weight Variability vs Rank', fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    # 6. Iterations to Convergence
    ax = axes[1, 2]
    colors = ['green' if c else 'red' for c in comparison_df['converged']]
    ax.bar(ranks, comparison_df['n_iterations'], color=colors, alpha=0.7)
    ax.set_xlabel('Rank (Number of Components)', fontsize=11)
    ax.set_ylabel('Iterations to Convergence', fontsize=11)
    ax.set_title('Convergence Speed vs Rank\n(Green=Converged, Red=Max Iterations)', fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    output_path = os.path.join(comparison_dir, 'rank_comparison_summary.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"  Saved: rank_comparison_summary.png")
    plt.close()
    
    # Create elbow plot
    fig, ax = plt.subplots(figsize=(10, 6))
    
    var_exp = comparison_df['variance_explained'].values
    ax.plot(ranks, var_exp, 'o-', linewidth=3, markersize=10, color='steelblue', label='Variance Explained')
    
    # Calculate marginal gains
    if len(ranks) > 1:
        marginal_gains = np.diff(var_exp)
        for i in range(len(marginal_gains)):
            ax.annotate(f'+{marginal_gains[i]*100:.1f}%',
                       xy=(ranks[i+1], var_exp[i+1]),
                       xytext=(10, -10),
                       textcoords='offset points',
                       fontsize=9,
                       color='darkgreen',
                       bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.3))
    
    ax.axhline(y=0.8, color='green', linestyle='--', alpha=0.5, linewidth=2, label='80% threshold (excellent)')
    ax.axhline(y=0.6, color='orange', linestyle='--', alpha=0.5, linewidth=2, label='60% threshold (good)')
    ax.set_xlabel('Rank (Number of Components)', fontsize=13, fontweight='bold')
    ax.set_ylabel('Variance Explained', fontsize=13, fontweight='bold')
    ax.set_title('Elbow Plot: Finding Optimal Rank\n(Look for diminishing returns)', 
                fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)
    ax.legend(fontsize=11)
    ax.set_ylim([0, 1.05])
    ax.set_xticks(ranks)
    
    plt.tight_layout()
    output_path = os.path.join(comparison_dir, 'elbow_plot.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"  Saved: elbow_plot.png")
    plt.close()
    
    # Loss trajectories
    fig, ax = plt.subplots(figsize=(10, 6))
    
    loss_plotted = False
    for r in successful:
        # Try to get loss from model object first
        if 'model' in r and hasattr(r['model'], 'loss_') and r['model'].loss_ is not None:
            loss = r['model'].loss_
            ax.plot(range(1, len(loss)+1), loss, linewidth=2, label=f'Rank {r["rank"]}', alpha=0.8)
            loss_plotted = True
        else:
            # Load from disk
            rank_dir = os.path.join(output_dir, f'rank_{r["rank"]:02d}')
            loss_file = os.path.join(rank_dir, 'loss_history.csv')
            if os.path.exists(loss_file):
                loss_df = pd.read_csv(loss_file)
                ax.plot(loss_df['Iteration'], loss_df['Loss'], linewidth=2, label=f'Rank {r["rank"]}', alpha=0.8)
                loss_plotted = True
    
    if loss_plotted:
        ax.set_xlabel('Iteration', fontsize=12)
        ax.set_ylabel('Loss', fontsize=12)
        ax.set_title('Loss Convergence Trajectories by Rank', fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3)
        ax.legend(fontsize=10)
        ax.set_yscale('log')
        
        plt.tight_layout()
        output_path = os.path.join(comparison_dir, 'loss_trajectories.png')
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        print(f"  Saved: loss_trajectories.png")
    else:
        print(f"  Skipped loss_trajectories.png (no loss history available)")
    
    plt.close()
    
    print(f"\nAll comparison plots saved to: {comparison_dir}")
```

### Plot Comprehensive Methods Comparison

This chunk creates a comprehensive side-by-side comparison of all three rank selection methods:
1. Dissertation-validated Cross-Validation (PRIMARY - validated on 100 simulations)
2. Synthetic FMS (SUPPLEMENTARY - alternative validation)
3. Variance Explained (SUPPLEMENTARY - heuristic inspection)

```{python plot-methods-comparison, eval=FALSE}
# ===========================================
# COMPREHENSIVE RANK SELECTION METHODS COMPARISON
# Compares all three approaches side-by-side
# ===========================================

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
import json
import os

print("\n" + "="*60)
print("COMPREHENSIVE RANK SELECTION METHODS COMPARISON")
print("="*60)

# Initialize figure
fig = plt.figure(figsize=(18, 12))
gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

fig.suptitle('Comprehensive Rank Selection: Comparing Three Methods', 
             fontsize=18, fontweight='bold', y=0.98)

# ===========================================
# METHOD 1: DISSERTATION CV-SSE (TOP ROW)
# ===========================================
if 'cv_rank_results' in locals() and cv_rank_results is not None:
    cv_data = cv_rank_results[cv_rank_results['n_successful_folds'] == len(replicate_groups)].copy()
    
    if len(cv_data) > 0:
        # Plot 1A: CV-SSE
        ax1a = fig.add_subplot(gs[0, 0])
        ax1a.errorbar(cv_data['rank'], cv_data['mean_cv_sse'],
                     yerr=cv_data['std_cv_sse'],
                     marker='o', markersize=8, linewidth=2.5,
                     capsize=5, color='steelblue', ecolor='lightblue')
        
        if 'optimal_rank_cv' in locals() and optimal_rank_cv is not None:
            opt_data = cv_data[cv_data['rank'] == optimal_rank_cv]
            if len(opt_data) > 0:
                ax1a.plot(optimal_rank_cv, opt_data['mean_cv_sse'].values[0],
                         '*', markersize=22, color='red', 
                         markeredgecolor='darkred', markeredgewidth=2, zorder=10)
                ax1a.axvline(x=optimal_rank_cv, color='red', 
                            linestyle='--', alpha=0.5, linewidth=2)
        
        ax1a.set_xlabel('Rank', fontsize=11, fontweight='bold')
        ax1a.set_ylabel('CV-SSE', fontsize=11, fontweight='bold')
        ax1a.set_title('Method 1A: Cross-Validated SSE\n(Dissertation-Validated)', 
                      fontsize=11, fontweight='bold', color='darkgreen')
        ax1a.grid(True, alpha=0.3)
        ax1a.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))
        
        # Plot 1B: Normalized CV-SSE
        ax1b = fig.add_subplot(gs[0, 1])
        min_sse = cv_data['mean_cv_sse'].min()
        cv_data['norm_sse'] = cv_data['mean_cv_sse'] / min_sse
        
        ax1b.plot(cv_data['rank'], cv_data['norm_sse'],
                 'o-', linewidth=2.5, markersize=8, color='darkgreen')
        ax1b.axhline(y=1.0, color='red', linestyle='--', alpha=0.5, linewidth=2)
        
        if 'optimal_rank_cv' in locals() and optimal_rank_cv is not None:
            opt_data = cv_data[cv_data['rank'] == optimal_rank_cv]
            if len(opt_data) > 0:
                ax1b.plot(optimal_rank_cv, opt_data['norm_sse'].values[0],
                         '*', markersize=22, color='red',
                         markeredgecolor='darkred', markeredgewidth=2, zorder=10)
                ax1b.axvline(x=optimal_rank_cv, color='red',
                            linestyle='--', alpha=0.5, linewidth=2)
        
        ax1b.set_xlabel('Rank', fontsize=11, fontweight='bold')
        ax1b.set_ylabel('Normalized CV-SSE', fontsize=11, fontweight='bold')
        ax1b.set_title('Method 1B: Relative CV-SSE\n(Min = 1.0)', 
                      fontsize=11, fontweight='bold', color='darkgreen')
        ax1b.grid(True, alpha=0.3)
        
        # Plot 1C: Method 1 Summary
        ax1c = fig.add_subplot(gs[0, 2])
        ax1c.axis('off')
        
        summary1 = "METHOD 1: DISSERTATION CV\n"
        summary1 += "="*30 + "\n\n"
        summary1 += "Status: PRIMARY METHOD\n"
        summary1 += "Validation: ✓ Tested\n\n"
        
        if 'optimal_rank_cv' in locals() and optimal_rank_cv is not None:
            summary1 += f"Optimal Rank: {optimal_rank_cv}\n"
            opt_data = cv_data[cv_data['rank'] == optimal_rank_cv]
            if len(opt_data) > 0:
                sse = opt_data['mean_cv_sse'].values[0]
                summary1 += f"CV-SSE: {sse:.2e}\n\n"
        
        summary1 += "Criterion: Min CV-SSE\n"
        summary1 += f"CV Folds: {len(replicate_groups)}\n"
        summary1 += "Reference: Blaskowski\n"
        summary1 += "           (2024)\n\n"
        summary1 += "Validated:\n"
        summary1 += "  • 86% accuracy\n"
        summary1 += "  • 100 simulations\n"
        summary1 += "  • 10:1 noise ratio\n\n"
        summary1 += "★ RECOMMENDED ★"
        
        ax1c.text(0.05, 0.95, summary1,
                 transform=ax1c.transAxes,
                 verticalalignment='top',
                 fontsize=10,
                 family='monospace',
                 bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.4))
else:
    # No CV results
    for i in range(3):
        ax = fig.add_subplot(gs[0, i])
        ax.text(0.5, 0.5, 'CV Results Not Available\nRun run-cv-rank-selection',
               ha='center', va='center', fontsize=12, color='gray')
        ax.axis('off')

# ===========================================
# METHOD 2: SYNTHETIC FMS (MIDDLE ROW)
# ===========================================
# Check if FMS results exist
fms_comparison_file = os.path.join(output_dir, 'rank_comparison', 'fms_comparison_results.json')
if os.path.exists(fms_comparison_file):
    with open(fms_comparison_file, 'r') as f:
        fms_results = json.load(f)
    
    # Plot 2A: Mean FMS
    ax2a = fig.add_subplot(gs[1, 0])
    ranks_fms = [int(r) for r in fms_results.keys()]
    mean_fms = [fms_results[str(r)]['mean_fms'] for r in ranks_fms]
    std_fms = [fms_results[str(r)]['std_fms'] for r in ranks_fms]
    
    ax2a.errorbar(ranks_fms, mean_fms, yerr=std_fms,
                 marker='o', markersize=8, linewidth=2.5,
                 capsize=5, color='purple', ecolor='lavender')
    
    if 'optimal_rank_fms' in locals() and optimal_rank_fms is not None:
        if str(optimal_rank_fms) in fms_results:
            opt_fms = fms_results[str(optimal_rank_fms)]['mean_fms']
            ax2a.plot(optimal_rank_fms, opt_fms,
                     '*', markersize=22, color='red',
                     markeredgecolor='darkred', markeredgewidth=2, zorder=10)
            ax2a.axvline(x=optimal_rank_fms, color='red',
                        linestyle='--', alpha=0.5, linewidth=2)
    
    ax2a.set_xlabel('Rank', fontsize=11, fontweight='bold')
    ax2a.set_ylabel('Mean FMS', fontsize=11, fontweight='bold')
    ax2a.set_title('Method 2A: Synthetic FMS\n(Alternative Validation)', 
                  fontsize=11, fontweight='bold', color='purple')
    ax2a.grid(True, alpha=0.3)
    ax2a.set_ylim([0, 1.05])
    
    # Plot 2B: FMS Stability
    ax2b = fig.add_subplot(gs[1, 1])
    cv_fms = [fms_results[str(r)]['std_fms'] / (fms_results[str(r)]['mean_fms'] + 1e-10) 
              for r in ranks_fms]
    
    ax2b.bar(ranks_fms, cv_fms, color='purple', alpha=0.6)
    
    if 'optimal_rank_fms' in locals() and optimal_rank_fms is not None:
        ax2b.axvline(x=optimal_rank_fms, color='red',
                    linestyle='--', linewidth=2, alpha=0.7)
    
    ax2b.set_xlabel('Rank', fontsize=11, fontweight='bold')
    ax2b.set_ylabel('CV (Std/Mean)', fontsize=11, fontweight='bold')
    ax2b.set_title('Method 2B: FMS Stability\n(Lower = more stable)', 
                  fontsize=11, fontweight='bold', color='purple')
    ax2b.grid(True, alpha=0.3, axis='y')
    
    # Plot 2C: Method 2 Summary
    ax2c = fig.add_subplot(gs[1, 2])
    ax2c.axis('off')
    
    summary2 = "METHOD 2: SYNTHETIC FMS\n"
    summary2 += "="*30 + "\n\n"
    summary2 += "Status: SUPPLEMENTARY\n"
    summary2 += "Validation: Alternative\n\n"
    
    if 'optimal_rank_fms' in locals() and optimal_rank_fms is not None:
        summary2 += f"Optimal Rank: {optimal_rank_fms}\n"
        if str(optimal_rank_fms) in fms_results:
            fms = fms_results[str(optimal_rank_fms)]['mean_fms']
            summary2 += f"Mean FMS: {fms:.3f}\n\n"
    
    summary2 += "Criterion: Max FMS\n"
    summary2 += f"Random States: {fms_results[str(ranks_fms[0])]['n_states']}\n"
    summary2 += "Approach: Synthetic\n"
    summary2 += "          data test\n\n"
    summary2 += "Note: Alternative\n"
    summary2 += "validation method,\n"
    summary2 += "not used in\n"
    summary2 += "dissertation."
    
    ax2c.text(0.05, 0.95, summary2,
             transform=ax2c.transAxes,
             verticalalignment='top',
             fontsize=10,
             family='monospace',
             bbox=dict(boxstyle='round', facecolor='lavender', alpha=0.4))
else:
    # No FMS results
    for i in range(3):
        ax = fig.add_subplot(gs[1, i])
        ax.text(0.5, 0.5, 'FMS Results Not Available',
               ha='center', va='center', fontsize=12, color='gray')
        ax.axis('off')

# ===========================================
# METHOD 3: VARIANCE EXPLAINED (BOTTOM ROW)
# ===========================================
comparison_metrics_file = os.path.join(output_dir, 'rank_comparison', 'rank_comparison_metrics.csv')
if os.path.exists(comparison_metrics_file):
    var_exp_data = pd.read_csv(comparison_metrics_file)
    
    # Plot 3A: Variance Explained
    ax3a = fig.add_subplot(gs[2, 0])
    ax3a.plot(var_exp_data['Rank'], var_exp_data['variance_explained'],
             'o-', linewidth=2.5, markersize=8, color='coral')
    
    ax3a.axhline(y=0.8, color='green', linestyle='--', alpha=0.5, 
                linewidth=2, label='80% (excellent)')
    ax3a.axhline(y=0.6, color='orange', linestyle='--', alpha=0.5,
                linewidth=2, label='60% (good)')
    
    if 'optimal_rank_variance' in locals() and optimal_rank_variance is not None:
        opt_var = var_exp_data[var_exp_data['Rank'] == optimal_rank_variance]
        if len(opt_var) > 0:
            ax3a.plot(optimal_rank_variance, opt_var['variance_explained'].values[0],
                     '*', markersize=22, color='red',
                     markeredgecolor='darkred', markeredgewidth=2, zorder=10)
            ax3a.axvline(x=optimal_rank_variance, color='red',
                        linestyle='--', alpha=0.5, linewidth=2)
    
    ax3a.set_xlabel('Rank', fontsize=11, fontweight='bold')
    ax3a.set_ylabel('Variance Explained', fontsize=11, fontweight='bold')
    ax3a.set_title('Method 3A: Variance Explained\n(Heuristic Elbow)', 
                  fontsize=11, fontweight='bold', color='coral')
    ax3a.grid(True, alpha=0.3)
    ax3a.legend(fontsize=9)
    ax3a.set_ylim([0, 1.05])
    
    # Plot 3B: Marginal Gains
    ax3b = fig.add_subplot(gs[2, 1])
    
    if len(var_exp_data) > 1:
        ranks_var = var_exp_data['Rank'].values
        var_exp_vals = var_exp_data['variance_explained'].values
        marginal_gains = np.diff(var_exp_vals)
        
        colors = ['green' if g > 0.05 else 'orange' if g > 0.02 else 'red' 
                 for g in marginal_gains]
        
        ax3b.bar(ranks_var[1:], marginal_gains, color=colors, alpha=0.7)
        ax3b.axhline(y=0.02, color='orange', linestyle='--', alpha=0.5, 
                    linewidth=2, label='2% threshold')
        
        if 'optimal_rank_variance' in locals() and optimal_rank_variance is not None:
            ax3b.axvline(x=optimal_rank_variance, color='red',
                        linestyle='--', linewidth=2, alpha=0.7)
        
        ax3b.set_xlabel('Rank', fontsize=11, fontweight='bold')
        ax3b.set_ylabel('Marginal Gain', fontsize=11, fontweight='bold')
        ax3b.set_title('Method 3B: Diminishing Returns\n(Δ Variance per rank)', 
                      fontsize=11, fontweight='bold', color='coral')
        ax3b.grid(True, alpha=0.3, axis='y')
        ax3b.legend(fontsize=9)
    
    # Plot 3C: Method 3 Summary
    ax3c = fig.add_subplot(gs[2, 2])
    ax3c.axis('off')
    
    summary3 = "METHOD 3: VARIANCE\n"
    summary3 += "="*30 + "\n\n"
    summary3 += "Status: SUPPLEMENTARY\n"
    summary3 += "Validation: None\n\n"
    
    if 'optimal_rank_variance' in locals() and optimal_rank_variance is not None:
        summary3 += f"Suggested Rank: {optimal_rank_variance}\n"
        opt_var = var_exp_data[var_exp_data['Rank'] == optimal_rank_variance]
        if len(opt_var) > 0:
            var = opt_var['variance_explained'].values[0]
            summary3 += f"Var Explained: {var:.1%}\n\n"
    
    summary3 += "Criterion: Elbow\n"
    summary3 += "Approach: Visual\n"
    summary3 += "          inspection\n\n"
    summary3 += "Note: Heuristic\n"
    summary3 += "method, subjective\n"
    summary3 += "interpretation.\n\n"
    summary3 += "Not independently\n"
    summary3 += "validated."
    
    ax3c.text(0.05, 0.95, summary3,
             transform=ax3c.transAxes,
             verticalalignment='top',
             fontsize=10,
             family='monospace',
             bbox=dict(boxstyle='round', facecolor='mistyrose', alpha=0.4))
else:
    # No variance data
    for i in range(3):
        ax = fig.add_subplot(gs[2, i])
        ax.text(0.5, 0.5, 'Variance Data Not Available',
               ha='center', va='center', fontsize=12, color='gray')
        ax.axis('off')

# Save figure
plt.tight_layout()
comparison_plot_file = os.path.join(output_dir, 'rank_comparison', 
                                   'comprehensive_methods_comparison.png')
plt.savefig(comparison_plot_file, dpi=300, bbox_inches='tight')
print(f"\n✓ Comprehensive comparison plot saved to:")
print(f"  {comparison_plot_file}")

plt.show()
plt.close()

print("="*60)
```



# INDIVIDUAL RANK VISUALIZATIONS

## Detailed plots for Rank 10

This section creates comprehensive visualizations for rank 10 (the optimal rank determined from previous analysis).

```{python individual-rank-visualizations, eval=TRUE}
# ===========================================
# CREATE DETAILED VISUALIZATIONS FOR RANK 10
# ===========================================
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

print(f"\n{'='*60}")
print("CREATING RANK 10 VISUALIZATIONS")
print(f"{'='*60}")

# Get output directory
if 'output_dir' not in globals():
    output_dir = r.output_dir

# Set rank to analyze
SELECTED_RANK = 10
TOP_GENES_HEATMAP = 50  # Number of top genes to show in heatmap

# Process only rank 10
rank = SELECTED_RANK
print(f"\n--- Rank {rank} ---")
rank_dir = os.path.join(output_dir, f'rank_{rank:02d}')

# Load factor matrices
gene_factors_file = os.path.join(rank_dir, 'gene_factors.csv')
time_factors_file = os.path.join(rank_dir, 'time_factors.csv')
weights_file = os.path.join(rank_dir, 'component_weights.csv')

if not all([os.path.exists(f) for f in [gene_factors_file, time_factors_file, weights_file]]):
    print(f"ERROR: Missing factor files for rank {rank}")
    print(f"Please ensure barnacle decomposition has been run for rank {rank}")
else:
    gene_factors = pd.read_csv(gene_factors_file, index_col=0)
    time_factors = pd.read_csv(time_factors_file, index_col=0)
    weights = pd.read_csv(weights_file)
    
    # ===========================================
    # PLOT 1: GENE HEATMAPS (Multiple selection criteria)
    # ===========================================
    print(f"  Creating gene-component heatmaps with different selection criteria...")
    
    # ===========================================
    # 1A: Sum of absolute loadings (broadly active genes)
    # ===========================================
    print(f"    1A: Sum of absolute loadings (broadly active genes)...")
    gene_importance_sum = gene_factors.abs().sum(axis=1)
    top_genes_sum = gene_importance_sum.nlargest(TOP_GENES_HEATMAP).index
    top_gene_factors_sum = gene_factors.loc[top_genes_sum]
    
    # Save data to CSV
    csv_path = os.path.join(rank_dir, f'gene_heatmap_1A_sum_top{TOP_GENES_HEATMAP}_data.csv')
    top_gene_factors_sum.to_csv(csv_path)
    print(f"      Saved data: gene_heatmap_1A_sum_top{TOP_GENES_HEATMAP}_data.csv")
    
    fig, ax = plt.subplots(figsize=(max(12, rank*0.5), 10))
    sns.heatmap(
        top_gene_factors_sum,
        cmap='RdBu_r',
        center=0,
        cbar_kws={'label': 'Component Loading'},
        xticklabels=True,
        yticklabels=True,
        ax=ax
    )
    ax.set_xlabel('Components', fontsize=12, fontweight='bold')
    ax.set_ylabel('Gene ID', fontsize=12, fontweight='bold')
    ax.set_title(f'Rank {rank}: Top {TOP_GENES_HEATMAP} Genes by Sum of Absolute Loadings\n' + 
                f'(Genes that participate across multiple components)',
                fontsize=13, fontweight='bold', pad=15)
    plt.tight_layout()
    output_path = os.path.join(rank_dir, f'gene_heatmap_1A_sum_top{TOP_GENES_HEATMAP}.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"      Saved plot: gene_heatmap_1A_sum_top{TOP_GENES_HEATMAP}.png")
    
    # ===========================================
    # 1B: Maximum absolute loading (component-specific genes)
    # ===========================================
    print(f"    1B: Maximum absolute loading (component-specific genes)...")
    gene_importance_max = gene_factors.abs().max(axis=1)
    top_genes_max = gene_importance_max.nlargest(TOP_GENES_HEATMAP).index
    top_gene_factors_max = gene_factors.loc[top_genes_max]
    
    # Save data to CSV
    csv_path = os.path.join(rank_dir, f'gene_heatmap_1B_max_top{TOP_GENES_HEATMAP}_data.csv')
    top_gene_factors_max.to_csv(csv_path)
    print(f"      Saved data: gene_heatmap_1B_max_top{TOP_GENES_HEATMAP}_data.csv")
    
    fig, ax = plt.subplots(figsize=(max(12, rank*0.5), 10))
    sns.heatmap(
        top_gene_factors_max,
        cmap='RdBu_r',
        center=0,
        cbar_kws={'label': 'Component Loading'},
        xticklabels=True,
        yticklabels=True,
        ax=ax
    )
    ax.set_xlabel('Components', fontsize=12, fontweight='bold')
    ax.set_ylabel('Gene ID', fontsize=12, fontweight='bold')
    ax.set_title(f'Rank {rank}: Top {TOP_GENES_HEATMAP} Genes by Maximum Loading\n' + 
                f'(Genes highly specific to individual components)',
                fontsize=13, fontweight='bold', pad=15)
    plt.tight_layout()
    output_path = os.path.join(rank_dir, f'gene_heatmap_1B_max_top{TOP_GENES_HEATMAP}.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"      Saved plot: gene_heatmap_1B_max_top{TOP_GENES_HEATMAP}.png")
    
    # ===========================================
    # 1C: Mean absolute loading (consistently active genes)
    # ===========================================
    print(f"    1C: Mean absolute loading (consistently active genes)...")
    gene_importance_mean = gene_factors.abs().mean(axis=1)
    top_genes_mean = gene_importance_mean.nlargest(TOP_GENES_HEATMAP).index
    top_gene_factors_mean = gene_factors.loc[top_genes_mean]
    
    # Save data to CSV
    csv_path = os.path.join(rank_dir, f'gene_heatmap_1C_mean_top{TOP_GENES_HEATMAP}_data.csv')
    top_gene_factors_mean.to_csv(csv_path)
    print(f"      Saved data: gene_heatmap_1C_mean_top{TOP_GENES_HEATMAP}_data.csv")
    
    fig, ax = plt.subplots(figsize=(max(12, rank*0.5), 10))
    sns.heatmap(
        top_gene_factors_mean,
        cmap='RdBu_r',
        center=0,
        cbar_kws={'label': 'Component Loading'},
        xticklabels=True,
        yticklabels=True,
        ax=ax
    )
    ax.set_xlabel('Components', fontsize=12, fontweight='bold')
    ax.set_ylabel('Gene ID', fontsize=12, fontweight='bold')
    ax.set_title(f'Rank {rank}: Top {TOP_GENES_HEATMAP} Genes by Mean Absolute Loading\n' + 
                f'(Genes with consistent moderate activity across components)',
                fontsize=13, fontweight='bold', pad=15)
    plt.tight_layout()
    output_path = os.path.join(rank_dir, f'gene_heatmap_1C_mean_top{TOP_GENES_HEATMAP}.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"      Saved plot: gene_heatmap_1C_mean_top{TOP_GENES_HEATMAP}.png")
    
    # ===========================================
    # 1D: Variance of loadings (genes with high specificity)
    # ===========================================
    print(f"    1D: Variance of loadings (component-specific markers)...")
    gene_variance = gene_factors.var(axis=1)
    top_genes_var = gene_variance.nlargest(TOP_GENES_HEATMAP).index
    top_gene_factors_var = gene_factors.loc[top_genes_var]
    
    # Save data to CSV
    csv_path = os.path.join(rank_dir, f'gene_heatmap_1D_variance_top{TOP_GENES_HEATMAP}_data.csv')
    top_gene_factors_var.to_csv(csv_path)
    print(f"      Saved data: gene_heatmap_1D_variance_top{TOP_GENES_HEATMAP}_data.csv")
    
    fig, ax = plt.subplots(figsize=(max(12, rank*0.5), 10))
    sns.heatmap(
        top_gene_factors_var,
        cmap='RdBu_r',
        center=0,
        cbar_kws={'label': 'Component Loading'},
        xticklabels=True,
        yticklabels=True,
        ax=ax
    )
    ax.set_xlabel('Components', fontsize=12, fontweight='bold')
    ax.set_ylabel('Gene ID', fontsize=12, fontweight='bold')
    ax.set_title(f'Rank {rank}: Top {TOP_GENES_HEATMAP} Genes by Loading Variance\n' + 
                f'(Genes with high specificity - strong in some components, weak in others)',
                fontsize=13, fontweight='bold', pad=15)
    plt.tight_layout()
    output_path = os.path.join(rank_dir, f'gene_heatmap_1D_variance_top{TOP_GENES_HEATMAP}.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"      Saved plot: gene_heatmap_1D_variance_top{TOP_GENES_HEATMAP}.png")
    
    # ===========================================
    # Summary comparison
    # ===========================================
    print(f"\n    Gene selection summary:")
    print(f"      Sum method:      Favors genes active across many components")
    print(f"      Max method:      Favors genes strongly specific to one component")
    print(f"      Mean method:     Favors genes with consistent moderate activity")
    print(f"      Variance method: Favors genes with high component specificity")
    
    # Check overlap between methods
    overlap_sum_max = len(set(top_genes_sum) & set(top_genes_max))
    overlap_sum_mean = len(set(top_genes_sum) & set(top_genes_mean))
    overlap_max_var = len(set(top_genes_max) & set(top_genes_var))
    
    print(f"\n    Gene overlap between methods:")
    print(f"      Sum & Max:      {overlap_sum_max}/{TOP_GENES_HEATMAP} genes in common")
    print(f"      Sum & Mean:     {overlap_sum_mean}/{TOP_GENES_HEATMAP} genes in common")
    print(f"      Max & Variance: {overlap_max_var}/{TOP_GENES_HEATMAP} genes in common")
    
    # ===========================================
    # EXPORT TOP 500 GENES PER COMPONENT
    # ===========================================
    print(f"\n    Exporting top 500 genes per component...")
    TOP_GENES_PER_COMPONENT = 500
    
    for component in gene_factors.columns:
        # Get top genes by absolute loading for this component
        component_loadings = gene_factors[component].abs().sort_values(ascending=False)
        top_genes_for_component = component_loadings.head(TOP_GENES_PER_COMPONENT)
        
        # Create dataframe with gene ID and loading value (both absolute and signed)
        top_genes_df = pd.DataFrame({
            'gene_id': top_genes_for_component.index,
            'absolute_loading': top_genes_for_component.values,
            'signed_loading': gene_factors.loc[top_genes_for_component.index, component].values
        })
        
        # Save to CSV
        csv_filename = f'top_{TOP_GENES_PER_COMPONENT}_genes_{component}.csv'
        csv_path = os.path.join(rank_dir, csv_filename)
        top_genes_df.to_csv(csv_path, index=False)
        
    print(f"      Saved top {TOP_GENES_PER_COMPONENT} genes for each of {len(gene_factors.columns)} components")
    print(f"      Files: top_{TOP_GENES_PER_COMPONENT}_genes_Component_X.csv")
    
    # ===========================================
    # PLOT 2: TIME COMPONENT LINE PLOT
    # ===========================================
    print(f"  Creating time-component line plot...")
    
    fig, ax = plt.subplots(figsize=(12, 8))
    
    # Define different marker shapes for each component
    markers = ['o', 's', '^', 'D', 'v', '<', '>', 'p', '*', 'h', '+', 'x', '8', 'H', 'd', '|', '_']
    
    # Plot each component
    for i, col in enumerate(time_factors.columns):
        marker_style = markers[i % len(markers)]  # Cycle through markers if more components than markers
        ax.plot(time_factors.index, time_factors[col], 
               marker=marker_style, linewidth=2, markersize=8,
               label=f'{col}', alpha=0.8)
    
    ax.set_xlabel('Timepoint', fontsize=12, fontweight='bold')
    ax.set_ylabel('Component Loading', fontsize=12, fontweight='bold')
    ax.set_title(f'Rank {rank}: Component Loadings Across Timepoints',
                fontsize=13, fontweight='bold', pad=15)
    ax.grid(True, alpha=0.3)
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)
    
    plt.tight_layout()
    output_path = os.path.join(rank_dir, 'time_component_lineplot.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"    Saved: time_component_lineplot.png")
    
    # ===========================================
    # PLOT 3: SAMPLE HEATMAP (Samples x Components)
    # ===========================================
    print(f"  Creating sample-component heatmap...")
    
    # Load sample factors with metadata
    sample_factors_file = os.path.join(rank_dir, 'sample_factors.csv')
    sample_factors = pd.read_csv(sample_factors_file, index_col=0)
    
    # Extract only the component columns
    component_cols = [col for col in sample_factors.columns if col.startswith('Component_')]
    sample_component_data = sample_factors[component_cols]
    
    # Create labels that include species and sample info
    sample_labels = [f"{row['Species']}_{row['Sample_ID']}" for idx, row in sample_factors.iterrows()]
    
    # Add sample labels as index for clustering
    sample_component_data.index = sample_labels
    
    # Create clustered heatmap using seaborn's clustermap
    # This performs hierarchical clustering on samples (rows) based on their component loadings
    print(f"    Performing hierarchical clustering on samples...")
    
    g = sns.clustermap(
        sample_component_data,
        cmap='RdBu_r',
        center=0,
        figsize=(max(12, rank*0.5), max(10, len(sample_labels)*0.3)),
        cbar_kws={'label': 'Component Loading'},
        row_cluster=True,   # Cluster samples (rows)
        col_cluster=False,  # Don't cluster components (columns) - keep original order
        xticklabels=True,
        yticklabels=True,
        dendrogram_ratio=0.15,
        cbar_pos=(0.02, 0.8, 0.03, 0.15),
        linewidths=0,
        method='average',  # Linkage method for hierarchical clustering
        metric='euclidean'  # Distance metric
    )
    
    # Adjust labels and title
    g.ax_heatmap.set_xlabel('Components', fontsize=12, fontweight='bold')
    g.ax_heatmap.set_ylabel('Sample (Species_SampleID)', fontsize=12, fontweight='bold')
    g.fig.suptitle(f'Rank {rank}: Sample Loadings Across Components\n' + 
                   f'(Hierarchically clustered by component loading similarity)',
                   fontsize=13, fontweight='bold', y=0.98)
    
    plt.tight_layout()
    output_path = os.path.join(rank_dir, 'sample_component_heatmap.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"    Saved: sample_component_heatmap.png")
    
    # ===========================================
    # PLOT 4: COMPONENT CORRELATION HEATMAP
    # ===========================================
    print(f"  Creating component correlation heatmap...")
    
    # Calculate correlation between components based on gene loadings
    component_corr = gene_factors.corr()
    
    fig, ax = plt.subplots(figsize=(max(10, rank*0.4), max(8, rank*0.35)))
    
    sns.heatmap(
        component_corr,
        annot=True if rank <= 20 else False,
        fmt='.2f' if rank <= 20 else '.1f',
        cmap='coolwarm',
        center=0,
        square=True,
        cbar_kws={'label': 'Correlation'},
        ax=ax
    )
    
    ax.set_title(f'Rank {rank}: Component Correlation Matrix\n' +
                '(Based on gene loadings - low correlation = good separation)',
                fontsize=13, fontweight='bold', pad=15)
    
    plt.tight_layout()
    output_path = os.path.join(rank_dir, 'component_correlation_heatmap.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"    Saved: component_correlation_heatmap.png")
    
    print(f"\n{'='*60}")
    print("RANK 10 VISUALIZATIONS COMPLETE")
    print(f"{'='*60}")
    print(f"\nThe following plots were created in rank_{rank:02d}/ directory:")
    print(f"  1A. gene_heatmap_1A_sum_top{TOP_GENES_HEATMAP}.png - Broadly active genes (sum)")
    print(f"  1B. gene_heatmap_1B_max_top{TOP_GENES_HEATMAP}.png - Component-specific genes (max)")
    print(f"  1C. gene_heatmap_1C_mean_top{TOP_GENES_HEATMAP}.png - Consistently active genes (mean)")
    print(f"  1D. gene_heatmap_1D_variance_top{TOP_GENES_HEATMAP}.png - High specificity genes (variance)")
    print(f"  2. time_component_lineplot.png - Component patterns over time")
    print(f"  3. sample_component_heatmap.png - Individual samples across components")
    print(f"  4. component_correlation_heatmap.png - Component independence check")
```





## Close log file

```{python close-log, eval=TRUE}
from datetime import datetime

print()
print("="*60)
print(f"ANALYSIS COMPLETE")
print(f"Finished: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"All output saved to: {output_dir}")
print(f"Log file: {log_file}")
print("="*60)

# Restore stdout and close log file
if hasattr(sys.stdout, 'log'):
    sys.stdout.log.close()
    sys.stdout = sys.stdout.terminal
```

# GOSLIM ANNOTATION ANALYSIS

This section annotates the top 500 genes for components 5, 6, 7, and 9 with GOslim Biological Process terms from the rank_10 directory.

**Note:** Only Biological Process (BP) GOslim terms are included in this analysis. Cellular Component and Molecular Function GOslim terms are excluded.

## Setup and load annotations

```{r goslim-setup, eval=TRUE}
library(tidyverse)
library(ggplot2)

# Define paths
rank_10_dir <- file.path(output_dir, "rank_10")
goslim_output_dir <- file.path(output_dir, "rank_10", "goslim_analysis")
dir.create(goslim_output_dir, showWarnings = FALSE, recursive = TRUE)

# Path to goslim_generic.obo file
goslim_obo_file <- "../output/12-ortho-annot/run_20250831_172744/goslim_generic.obo"

# Load the ortholog groups annotation file
ortho_annot <- read.csv(ortholog_groups_file, stringsAsFactors = FALSE)

cat(strrep("=", 60), "\n")
cat("GOSLIM ANNOTATION SETUP\n")
cat(strrep("=", 60), "\n\n")
cat("Rank 10 directory:", rank_10_dir, "\n")
cat("GOslim output directory:", goslim_output_dir, "\n")
cat("Loaded ortholog annotations:", nrow(ortho_annot), "ortholog groups\n\n")
```

## Extract Biological Process GOslim IDs

```{r extract-bp-goslims, eval=TRUE}
# Function to parse OBO file and extract Biological Process GOslim IDs
extract_bp_goslim_ids <- function(obo_file) {
  if (!file.exists(obo_file)) {
    cat("WARNING: GOslim OBO file not found:", obo_file, "\n")
    cat("Proceeding without BP filtering (all GOslims will be included)\n")
    return(NULL)
  }
  
  # Read OBO file
  obo_lines <- readLines(obo_file)
  
  # Parse OBO file
  bp_goslim_ids <- c()
  current_term <- list(id = NULL, namespace = NULL, is_goslim = FALSE)
  
  for (line in obo_lines) {
    line <- trimws(line)
    
    if (line == "[Term]") {
      # Save previous term if it's a BP GOslim
      if (!is.null(current_term$id) && 
          current_term$namespace == "biological_process" && 
          current_term$is_goslim) {
        bp_goslim_ids <- c(bp_goslim_ids, current_term$id)
      }
      # Reset for new term
      current_term <- list(id = NULL, namespace = NULL, is_goslim = FALSE)
      
    } else if (grepl("^id:", line)) {
      current_term$id <- sub("^id:\\s*", "", line)
      
    } else if (grepl("^namespace:", line)) {
      current_term$namespace <- sub("^namespace:\\s*", "", line)
      
    } else if (grepl("^subset:\\s*goslim_generic", line)) {
      current_term$is_goslim <- TRUE
    }
  }
  
  # Don't forget the last term
  if (!is.null(current_term$id) && 
      current_term$namespace == "biological_process" && 
      current_term$is_goslim) {
    bp_goslim_ids <- c(bp_goslim_ids, current_term$id)
  }
  
  return(unique(bp_goslim_ids))
}

# Extract BP GOslim IDs
bp_goslim_ids <- extract_bp_goslim_ids(goslim_obo_file)

if (!is.null(bp_goslim_ids)) {
  cat("Extracted Biological Process GOslim terms:", length(bp_goslim_ids), "\n")
  cat("First 10 BP GOslim IDs:", paste(head(bp_goslim_ids, 10), collapse = ", "), "\n\n")
} else {
  cat("NOTE: BP filtering disabled - all GOslim terms will be used\n\n")
}
```

## Load top 500 genes for components 5, 6, 7, and 9

```{r load-top-genes, eval=TRUE}
# Components to analyze
components_to_analyze <- c(5, 6, 7, 9)

# Load top 500 genes for each component
top_genes_list <- list()

cat(strrep("=", 60), "\n")
cat("LOADING TOP 500 GENES\n")
cat(strrep("=", 60), "\n\n")

for (comp_num in components_to_analyze) {
  file_path <- file.path(rank_10_dir, 
                         paste0("top_500_genes_Component_", comp_num, ".csv"))
  
  if (!file.exists(file_path)) {
    cat("WARNING: File not found:", file_path, "\n")
    next
  }
  
  # Load the top genes file
  top_genes_df <- read.csv(file_path, stringsAsFactors = FALSE)
  
  # Rename gene_id column to group_id for consistency with annotation file
  if ("gene_id" %in% colnames(top_genes_df)) {
    top_genes_df <- top_genes_df %>%
      rename(group_id = gene_id)
  }
  
  top_genes_list[[paste0("Component_", comp_num)]] <- top_genes_df
  
  cat("Component", comp_num, ":\n")
  cat("  Loaded", nrow(top_genes_df), "genes\n")
  cat("  Loading range:", 
      round(min(top_genes_df$absolute_loading), 4), "to", 
      round(max(top_genes_df$absolute_loading), 4), "\n")
  cat("  Top 3 genes:", paste(head(top_genes_df$group_id, 3), collapse = ", "), "\n\n")
}

cat("Total components loaded:", length(top_genes_list), "\n\n")
```

## Annotate with GOslim terms

```{r annotate-goslim, eval=TRUE}
# Function to parse GOslim IDs and names, filtering for BP terms only
parse_goslim <- function(goslim_ids_str, goslim_names_str, bp_filter_ids = NULL) {
  if (is.na(goslim_ids_str) || goslim_ids_str == "" || 
      is.na(goslim_names_str) || goslim_names_str == "") {
    return(NULL)
  }
  
  # Split GOslim IDs and names
  ids <- strsplit(goslim_ids_str, ";\\s*")[[1]]
  names <- strsplit(goslim_names_str, ";\\s*")[[1]]
  
  # Clean up whitespace
  ids <- trimws(ids)
  names <- trimws(names)
  
  # Return data frame if lengths match
  if (length(ids) == length(names) && length(ids) > 0) {
    result_df <- data.frame(
      goslim_id = ids,
      goslim_name = names,
      stringsAsFactors = FALSE
    )
    
    # Filter for Biological Process GOslims if bp_filter_ids is provided
    if (!is.null(bp_filter_ids)) {
      result_df <- result_df %>%
        filter(goslim_id %in% bp_filter_ids)
      
      # Return NULL if no BP terms remain after filtering
      if (nrow(result_df) == 0) {
        return(NULL)
      }
    }
    
    return(result_df)
  }
  
  return(NULL)
}

# Annotate each component's top genes
annotated_components <- list()

cat(strrep("=", 60), "\n")
cat("ANNOTATING WITH GOSLIM TERMS (Biological Process only)\n")
cat(strrep("=", 60), "\n\n")

for (comp_num in components_to_analyze) {
  comp_col <- paste0("Component_", comp_num)
  
  if (!comp_col %in% names(top_genes_list)) {
    cat("Component", comp_num, ": Skipping (not loaded)\n\n")
    next
  }
  
  top_genes_df <- top_genes_list[[comp_col]]
  
  cat("Component", comp_num, ":\n")
  
  # Merge with annotation data
  annotated_df <- top_genes_df %>%
    left_join(ortho_annot %>% 
                select(group_id, goslim_ids, goslim_names),
              by = "group_id")
  
  # Expand GOslim terms (one row per GOslim term)
  # Filter for Biological Process terms only
  goslim_expanded <- list()
  
  for (i in seq_len(nrow(annotated_df))) {
    row <- annotated_df[i, ]
    # Pass bp_goslim_ids to filter for BP terms only
    goslim_data <- parse_goslim(row$goslim_ids, row$goslim_names, bp_goslim_ids)
    
    if (!is.null(goslim_data)) {
      goslim_data$group_id <- row$group_id
      goslim_data$loading <- row$absolute_loading
      goslim_expanded[[i]] <- goslim_data
    }
  }
  
  # Combine all GOslim annotations
  if (length(goslim_expanded) > 0) {
    goslim_all <- bind_rows(goslim_expanded)
    
    # Count occurrences of each GOslim term
    goslim_counts <- goslim_all %>%
      group_by(goslim_id, goslim_name) %>%
      summarize(
        count = n(),
        mean_loading = mean(loading),
        .groups = "drop"
      ) %>%
      arrange(desc(count))
    
    annotated_components[[comp_col]] <- list(
      top_genes = annotated_df,
      goslim_counts = goslim_counts
    )
    
    cat("  Genes with BP GOslim:", 
        length(unique(goslim_all$group_id)), "/", nrow(top_genes_df), "\n")
    cat("  Unique BP GOslim terms:", nrow(goslim_counts), "\n")
    cat("  Top 5 BP GOslim terms:\n")
    top5 <- head(goslim_counts, 5)
    for (j in seq_len(nrow(top5))) {
      cat("    ", j, ". ", top5$goslim_name[j], " (", top5$goslim_id[j], ", n=", top5$count[j], ")\n", sep = "")
    }
  } else {
    cat("  WARNING: No BP GOslim annotations found!\n")
    annotated_components[[comp_col]] <- list(
      top_genes = annotated_df,
      goslim_counts = data.frame()
    )
  }
  cat("\n")
}
```

## Save GOslim annotation tables

```{r save-goslim-tables, eval=TRUE}
cat(strrep("=", 60), "\n")
cat("SAVING GOSLIM TABLES (Biological Process only)\n")
cat(strrep("=", 60), "\n\n")

for (comp_num in components_to_analyze) {
  comp_col <- paste0("Component_", comp_num)
  
  if (!comp_col %in% names(annotated_components)) {
    next
  }
  
  # Save top genes with annotations
  top_genes_file <- file.path(goslim_output_dir, 
                               paste0("component_", comp_num, "_top500_genes_goslim_BP_annotated.csv"))
  write.csv(annotated_components[[comp_col]]$top_genes, 
            top_genes_file, 
            row.names = FALSE)
  cat("Saved:", basename(top_genes_file), "\n")
  
  # Save GOslim counts
  if (nrow(annotated_components[[comp_col]]$goslim_counts) > 0) {
    goslim_counts_file <- file.path(goslim_output_dir, 
                                    paste0("component_", comp_num, "_goslim_BP_counts.csv"))
    write.csv(annotated_components[[comp_col]]$goslim_counts, 
              goslim_counts_file, 
              row.names = FALSE)
    cat("Saved:", basename(goslim_counts_file), "\n")
  }
}

cat("\nAll tables saved to:", goslim_output_dir, "\n\n")
```

## Create GOslim bar charts

```{r plot-goslim-barcharts, eval=TRUE, fig.width=10, fig.height=8}
cat(strrep("=", 60), "\n")
cat("CREATING GOSLIM BAR CHARTS (Biological Process only)\n")
cat(strrep("=", 60), "\n\n")

# Color palette for components
component_colors <- c(
  "5" = "#E64B35",
  "6" = "#4DBBD5", 
  "7" = "#00A087",
  "9" = "#F39B7F"
)

for (comp_num in components_to_analyze) {
  comp_col <- paste0("Component_", comp_num)
  
  if (!comp_col %in% names(annotated_components)) {
    next
  }
  
  goslim_counts <- annotated_components[[comp_col]]$goslim_counts
  
  if (nrow(goslim_counts) == 0) {
    cat("Component", comp_num, ": No BP GOslim data to plot\n")
    next
  }
  
  # Take top 20 GOslim terms for visualization
  top_goslim <- goslim_counts %>%
    head(20) %>%
    mutate(goslim_name = reorder(goslim_name, count))
  
  # Get total number of genes for subtitle
  top_genes <- annotated_components[[comp_col]]$top_genes
  total_genes <- nrow(top_genes)
  
  # Create bar chart
  p <- ggplot(top_goslim, aes(x = goslim_name, y = count)) +
    geom_bar(stat = "identity", fill = component_colors[as.character(comp_num)], 
             alpha = 0.8) +
    geom_text(aes(label = count), hjust = -0.2, size = 3.5) +
    coord_flip() +
    labs(
      title = paste0("Component ", comp_num, ": Top 20 GOslim Biological Process Terms"),
      subtitle = paste0("Top 500 genes (genes can have multiple BP GOslim annotations)"),
      x = "GOslim Biological Process Term",
      y = "Number of Ortholog Groups"
    ) +
    theme_bw() +
    theme(
      plot.title = element_text(size = 14, face = "bold"),
      plot.subtitle = element_text(size = 11, color = "gray30"),
      axis.text.y = element_text(size = 10),
      axis.text.x = element_text(size = 10),
      axis.title = element_text(size = 11, face = "bold"),
      panel.grid.major.y = element_blank(),
      panel.grid.minor = element_blank()
    ) +
    scale_y_continuous(expand = expansion(mult = c(0, 0.15)))
  
  # Save plot
  plot_file <- file.path(goslim_output_dir, 
                         paste0("component_", comp_num, "_goslim_BP_barchart.png"))
  ggsave(plot_file, p, width = 10, height = 8, dpi = 300)
  cat("Saved:", basename(plot_file), "\n")
}

cat("\nAll bar charts saved to:", goslim_output_dir, "\n\n")
```

## Create GOslim percentage bar charts

```{r plot-goslim-percentage-barcharts, eval=TRUE, fig.width=10, fig.height=8}
cat(strrep("=", 60), "\n")
cat("CREATING GOSLIM PERCENTAGE BAR CHARTS (Biological Process only)\n")
cat(strrep("=", 60), "\n\n")

# Color palette for components
component_colors <- c(
  "5" = "#E64B35",
  "6" = "#4DBBD5", 
  "7" = "#00A087",
  "9" = "#F39B7F"
)

for (comp_num in components_to_analyze) {
  comp_col <- paste0("Component_", comp_num)
  
  if (!comp_col %in% names(annotated_components)) {
    next
  }
  
  goslim_counts <- annotated_components[[comp_col]]$goslim_counts
  
  if (nrow(goslim_counts) == 0) {
    cat("Component", comp_num, ": No BP GOslim data to plot\n")
    next
  }
  
  # Calculate total genes with BP GOslim annotations
  # Note: We're using the full top 500 genes as the denominator
  top_genes <- annotated_components[[comp_col]]$top_genes
  total_genes <- nrow(top_genes)
  
  # Calculate percentage for each GOslim term
  # (genes can have multiple GOslim terms, so percentages may sum to >100%)
  goslim_percentages <- goslim_counts %>%
    mutate(percentage = (count / total_genes) * 100) %>%
    head(20) %>%
    mutate(goslim_name = reorder(goslim_name, percentage))
  
  # Create percentage bar chart
  p <- ggplot(goslim_percentages, aes(x = goslim_name, y = percentage)) +
    geom_bar(stat = "identity", fill = component_colors[as.character(comp_num)], 
             alpha = 0.8) +
    geom_text(aes(label = sprintf("%.1f%%", percentage)), hjust = -0.2, size = 3.5) +
    coord_flip() +
    labs(
      title = paste0("Component ", comp_num, ": Top 20 GOslim Biological Process Terms (%)"),
      subtitle = paste0("Percentage of 500 genes with each BP GOslim (genes can have multiple annotations)"),
      x = "GOslim Biological Process Term",
      y = "Percentage of Ortholog Groups (%)"
    ) +
    theme_bw() +
    theme(
      plot.title = element_text(size = 14, face = "bold"),
      plot.subtitle = element_text(size = 11, color = "gray30"),
      axis.text.y = element_text(size = 10),
      axis.text.x = element_text(size = 10),
      axis.title = element_text(size = 11, face = "bold"),
      panel.grid.major.y = element_blank(),
      panel.grid.minor = element_blank()
    ) +
    scale_y_continuous(expand = expansion(mult = c(0, 0.15)))
  
  # Save plot
  plot_file <- file.path(goslim_output_dir, 
                         paste0("component_", comp_num, "_goslim_BP_barchart_percentage.png"))
  ggsave(plot_file, p, width = 10, height = 8, dpi = 300)
  cat("Saved:", basename(plot_file), "\n")
  
  # Also save the percentage data
  percentage_file <- file.path(goslim_output_dir, 
                               paste0("component_", comp_num, "_goslim_BP_percentages.csv"))
  goslim_percentages_full <- goslim_counts %>%
    mutate(percentage = (count / total_genes) * 100) %>%
    arrange(desc(percentage))
  write.csv(goslim_percentages_full, percentage_file, row.names = FALSE)
  cat("Saved:", basename(percentage_file), "\n\n")
}

cat("All percentage bar charts saved to:", goslim_output_dir, "\n\n")
```

## Create comprehensive GOslim percentage bar charts (all 72 BP terms)

```{r plot-goslim-all-percentage-barcharts, eval=TRUE, fig.width=12, fig.height=22}
cat(strrep("=", 60), "\n")
cat("CREATING COMPREHENSIVE GOSLIM PERCENTAGE BAR CHARTS (All 72 BP GOslim terms)\n")
cat(strrep("=", 60), "\n\n")

# Color palette for components
component_colors <- c(
  "5" = "#E64B35",
  "6" = "#4DBBD5", 
  "7" = "#00A087",
  "9" = "#F39B7F"
)

# Create a reference dataframe with ALL 72 BP GOslim terms
# Use the bp_goslim_ids that were already extracted, and get names from OBO file
extract_bp_goslim_names <- function(obo_file, bp_ids) {
  if (!file.exists(obo_file)) {
    return(data.frame(goslim_id = bp_ids, goslim_name = bp_ids, stringsAsFactors = FALSE))
  }
  
  obo_lines <- readLines(obo_file)
  id_to_name <- list()
  current_id <- NULL
  current_name <- NULL
  
  for (line in obo_lines) {
    line <- trimws(line)
    
    if (line == "[Term]") {
      # Save previous term
      if (!is.null(current_id) && !is.null(current_name)) {
        id_to_name[[current_id]] <- current_name
      }
      current_id <- NULL
      current_name <- NULL
      
    } else if (grepl("^id:", line)) {
      current_id <- sub("^id:\\s*", "", line)
      
    } else if (grepl("^name:", line)) {
      current_name <- sub("^name:\\s*", "", line)
    }
  }
  
  # Don't forget the last term
  if (!is.null(current_id) && !is.null(current_name)) {
    id_to_name[[current_id]] <- current_name
  }
  
  # Create dataframe with all BP GOslim IDs and their names
  data.frame(
    goslim_id = bp_ids,
    goslim_name = sapply(bp_ids, function(id) {
      if (id %in% names(id_to_name)) {
        id_to_name[[id]]
      } else {
        id  # Use ID as name if name not found
      }
    }, USE.NAMES = FALSE),
    stringsAsFactors = FALSE
  )
}

# Get names for all 72 BP GOslim IDs
all_bp_goslim_ref <- extract_bp_goslim_names(goslim_obo_file, bp_goslim_ids)

cat("BP GOslim reference table created with", nrow(all_bp_goslim_ref), "terms\n\n")

for (comp_num in components_to_analyze) {
  comp_col <- paste0("Component_", comp_num)
  
  if (!comp_col %in% names(annotated_components)) {
    next
  }
  
  goslim_counts <- annotated_components[[comp_col]]$goslim_counts
  
  # Get total number of genes
  top_genes <- annotated_components[[comp_col]]$top_genes
  total_genes <- nrow(top_genes)
  
  # Create complete dataframe with ALL 72 BP GOslim terms
  # Join with actual counts, filling missing terms with 0
  goslim_all_percentages <- all_bp_goslim_ref %>%
    left_join(goslim_counts, by = c("goslim_id", "goslim_name")) %>%
    mutate(
      count = replace_na(count, 0),
      percentage = (count / total_genes) * 100
    ) %>%
    arrange(desc(percentage)) %>%
    mutate(goslim_name = reorder(goslim_name, percentage))
  
  # Count how many terms have representation
  terms_with_data <- sum(goslim_all_percentages$count > 0)
  
  # Create comprehensive percentage bar chart with all 72 terms
  p <- ggplot(goslim_all_percentages, aes(x = goslim_name, y = percentage)) +
    geom_bar(stat = "identity", fill = component_colors[as.character(comp_num)], 
             alpha = 0.8) +
    geom_text(data = subset(goslim_all_percentages, percentage > 0),
              aes(label = sprintf("%.1f%%", percentage)), hjust = -0.2, size = 2.5) +
    coord_flip() +
    labs(
      title = paste0("Component ", comp_num, ": All 72 GOslim Biological Process Terms (%)"),
      subtitle = paste0(terms_with_data, " of 72 BP GOslim terms found in top 500 genes (genes can have multiple annotations)"),
      x = "GOslim Biological Process Term",
      y = "Percentage of Ortholog Groups (%)"
    ) +
    theme_bw() +
    theme(
      plot.title = element_text(size = 14, face = "bold"),
      plot.subtitle = element_text(size = 11, color = "gray30"),
      axis.text.y = element_text(size = 8),
      axis.text.x = element_text(size = 10),
      axis.title = element_text(size = 11, face = "bold"),
      panel.grid.major.y = element_blank(),
      panel.grid.minor = element_blank()
    ) +
    scale_y_continuous(expand = expansion(mult = c(0, 0.15)))
  
  # Save plot with fixed height for 72 terms
  plot_file <- file.path(goslim_output_dir, 
                         paste0("component_", comp_num, "_goslim_BP_barchart_all_percentage.png"))
  ggsave(plot_file, p, width = 12, height = 22, dpi = 300)
  cat("Saved:", basename(plot_file), "(", terms_with_data, "of 72 BP GOslim terms represented)\n")
}

cat("\nAll comprehensive percentage bar charts saved to:", goslim_output_dir, "\n\n")
```

## Summary statistics

```{r goslim-summary, eval=TRUE}
cat(strrep("=", 60), "\n")
cat("GOSLIM ANNOTATION SUMMARY (Biological Process only)\n")
cat(strrep("=", 60), "\n\n")

summary_data <- list()

for (comp_num in components_to_analyze) {
  comp_col <- paste0("Component_", comp_num)
  
  if (!comp_col %in% names(annotated_components)) {
    next
  }
  
  top_genes <- annotated_components[[comp_col]]$top_genes
  goslim_counts <- annotated_components[[comp_col]]$goslim_counts
  
  genes_with_goslim <- sum(!is.na(top_genes$goslim_ids) & top_genes$goslim_ids != "")
  
  if (nrow(goslim_counts) > 0) {
    top_term <- goslim_counts$goslim_name[1]
    top_term_id <- goslim_counts$goslim_id[1]
    top_count <- goslim_counts$count[1]
  } else {
    top_term <- NA
    top_term_id <- NA
    top_count <- NA
  }
  
  summary_data[[comp_col]] <- data.frame(
    Component = comp_num,
    Total_Genes = nrow(top_genes),
    Genes_With_BP_GOslim = length(unique(top_genes$group_id[!is.na(top_genes$goslim_ids) & top_genes$goslim_ids != ""])),
    Percent_Annotated = round(genes_with_goslim / nrow(top_genes) * 100, 1),
    Unique_BP_GOslim_Terms = nrow(goslim_counts),
    Top_BP_GOslim_Term = top_term,
    Top_BP_GOslim_ID = top_term_id,
    Top_BP_GOslim_Count = top_count,
    stringsAsFactors = FALSE
  )
}

summary_df <- bind_rows(summary_data)

print(summary_df)

# Save summary table
summary_file <- file.path(goslim_output_dir, "goslim_BP_annotation_summary.csv")
write.csv(summary_df, summary_file, row.names = FALSE)
cat("\nSaved summary table:", basename(summary_file), "\n")

cat("\n", strrep("=", 60), "\n")
cat("GOSLIM ANNOTATION ANALYSIS COMPLETE\n")
cat("Output directory:", goslim_output_dir, "\n")
cat(strrep("=", 60), "\n\n")

cat("Output files created:\n")
cat("  - component_[5,6,7,9]_top500_genes_goslim_BP_annotated.csv\n")
cat("  - component_[5,6,7,9]_goslim_BP_counts.csv\n")
cat("  - component_[5,6,7,9]_goslim_BP_percentages.csv\n")
cat("  - component_[5,6,7,9]_goslim_BP_barchart.png (counts)\n")
cat("  - component_[5,6,7,9]_goslim_BP_barchart_percentage.png (percentages)\n")
cat("  - goslim_BP_annotation_summary.csv\n")
cat("\nNote: Only Biological Process GOslim terms are included in this analysis.\n\n")
```

# SYSTEM INFO

```{r system-info, eval=TRUE}
sessionInfo()
```

# REFERENCES
